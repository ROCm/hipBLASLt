---
include: hipblaslt_common.yaml
include: known_bugs.yaml
include: matmul_common.yaml

Definitions:
  - &alpha_beta_range
    - { alpha:  5, beta:  0 }
    - { alpha:  0, beta:  3 }
    - { alpha:  1, beta:  3 }
    - { alpha:  1, beta:  1 }

  - &alpha_beta_range_small
    - { alpha: 2, alphai: 2, beta: -1.0, betai: 2.0 }

  - &transA_transB_range
    - { transA: N, transB: N }
    - { transA: N, transB: T }
    - { transA: T, transB: N }
    - { transA: T, transB: T }

  - &deepbench_alpha_beta_range
    - { alpha: 1, beta: 0 }
    - { alpha: 1, beta: 1 }

  - &deepbench_transA_transB_range
    - { transA: N, transB: N }
    - { transA: N, transB: T }
    - { transA: T, transB: N }

  - &ldd_transA_transB_range
    - { transA: N, transB: N }
    - { transA: N, transB: T }
    - { transA: T, transB: N }
    - { transA: T, transB: T }


Tests:
- name: matmul_bad_arg
  category: pre_checkin
  function:
    - matmul_bad_arg: *real_precisions
  transA: N
  transB: N
  fortran: [ false, true ]

# Tests confirm no NaN propagation when alpha = 0, 2 and beta = 0. Value .NaN is converted into zero
- {name: alpha_beta_zero_NaN, category: pre_checkin, precision: *real_precisions,
   function: matmul, transA: N, transB: N, M: 256, N: 128, K:  64, alpha: [ .NaN, 2 ], beta: [ .NaN, 2 ] }

- name: matmul_one
  category: quick
  function:
    matmul: *real_precisions
  matrix_size: *one_matrix_size_range
  transA_transB: *transA_transB_range
  alpha: [ 0.0, 1.0 ]
  beta: [ 0.0, 2.0 ]

- name: matmul_small
  category: quick
  function:
    matmul: *real_precisions
  matrix_size: *small_matrix_size_range
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range

- name: matmul_medium
  category: pre_checkin
  function:
    matmul: *real_precisions
  matrix_size: *medium_matrix_size_range
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range

#TODO: enable it after enable fp16altimpl
#- name: matmul_medium_alt
#  category: pre_checkin
#  function:
#    matmul: *hpa_half_precision
#  matrix_size: *medium_matrix_size_range
#  transA_transB: *transA_transB_range
#  alpha: 1
#  beta: 0
#  initialization: special

- name: matmul_medium_HMM
  category: HMM
  function:
    matmul: *real_precisions
  matrix_size: *medium_matrix_size_range
  transA: [ N ]
  transB: [ N ]
  alpha: 1
  beta: 1
  HMM: true

- name: matmul_chunk
  category: pre_checkin
  function:
    matmul: *real_precisions
  matrix_size: *chunk_matrix_size_range
  transA_transB: *transA_transB_range
  alpha: 2
  beta: 3

#- name: matmul_deepbench
#  category: nightly
#  function:
#    matmul: *real_precisions
#  matrix_size: *deepbench_sizes
#  alpha_beta: *deepbench_alpha_beta_range
#  transA_transB: *deepbench_transA_transB_range

#TODO: enable it after enable fp16altimpl
#- name: matmul_deepbench_alt
#  category: nightly
#  function:
#    matmul: *hpa_half_precision
#  matrix_size: *deepbench_sizes
#  alpha: 1
#  beta: 0
#  flags: 4
#  initialization: special
#  transA_transB: *deepbench_transA_transB_range

- name: resnet50_fwd
  category: nightly
  function:
    matmul: *real_precisions
  transA: N
  transB: N
  matrix_size: *resnet50_fwd_sizes
  alpha: 1
  beta: 0

#TODO: enable it after enable fp16altimpl
#- name: resnet50_fwd_alt
#  category: nightly
#  function:
#    matmul: *hpa_half_precision
#  transA: N
#  transB: N
#  matrix_size: *resnet50_fwd_sizes
#  alpha: 1
#  beta: 0
#  initialization: special

- name: resnet50_bwdwrw
  category: nightly
  function:
    matmul: *real_precisions
  transA: T
  transB: N
  matrix_size: *resnet50_bwdwrw_sizes
  alpha: 1
  beta: 1

- name: resnet50_bwddata
  category: nightly
  function:
    matmul: *real_precisions
  transA: N
  transB: T
  matrix_size: *resnet50_bwddata_sizes
  alpha: 1
  beta: 0

- name: inception4_fwd
  category: nightly
  function:
    matmul: *real_precisions
  transA: N
  transB: N
  matrix_size: *inception4_fwd_sizes
  alpha: 1
  beta: 0

- name: inception4_bwdwrw
  category: nightly
  function:
    matmul: *real_precisions
  transA: T
  transB: N
  matrix_size: *inception4_bwdwrw_sizes
  alpha: 1
  beta: 1

- name: inception4_bwddata
  category: nightly
  function:
    matmul: *real_precisions
  transA: N
  transB: T
  matrix_size: *inception4_bwddata_sizes
  alpha: 1
  beta: 0

- name: ctest_bwdwrw
  category: nightly
  function:
    matmul: *real_precisions
  transA: T
  transB: N
  alpha: 1
  beta: 1
  matrix_size: *ctest_bwdwrw_sizes

- name: ctest_fwd
  category: nightly
  function:
    matmul: *real_precisions
  transA: N
  transB: N
  alpha: 1
  beta: 0
  matrix_size: *ctest_fwd_sizes

- name: matmul_8
  category: quick
  function:
    matmul: *real_precisions
  M: 8
  N: 8
  K: 8
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_16
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 16
  K: 16
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_24
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 24
  N: 24
  K: 24
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_algo
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 32
  N: 32
  K: 32
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  matmul_algo: [ 0, 1 ]

- name: matmul_32_8_128
  category: nightly
  function:
    matmul: *real_precisions
  M: [8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128]
  N: 32
  K: 32
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_48_8_128
  category: nightly
  function:
    matmul: *real_precisions
  M: 48
  N: [8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128]
  K: 48
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_64_8_128
  category: nightly
  function:
    matmul: *real_precisions
  M: 64
  N: 64
  K: [16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128]
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_64_8
  category: quick
  function:
    matmul: *real_precisions
  M: 64
  N: 8
  K: 8
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_8_64
  category: quick
  function:
    matmul: *real_precisions
  M: 8
  N: 8
  K: 64
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range


- name: matmul_96
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 96
  N: 96
  K: 96
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_128
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 128
  N: 128
  K: 128
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_256
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 256
  N: 256
  K: 256
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_256_8_16
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 256
  N: 8
  K: 16
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_16_256_8
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 16
  N: 256
  K: 8
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_8_16_256
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 8
  N: 16
  K: 256
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_512
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 512
  N: 512
  K: 512
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_1024
  category: nightly
  function:
    matmul: *real_precisions
  M: 1024
  N: 1024
  K: 1024
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_bf16
  category: quick
  function:
    matmul: *hpa_bf16_precision
  transA: T
  transB: N
  alpha: 1
  beta: 0
  matrix_size:
    - { M:  512, N:  512, K:  512 }
    - { M:  960, N: 1024, K: 1024 }
    - { M: 1024, N: 1024, K: 1024 }

- name: matmul_small2
  category: pre_checkin
  transA: N
  transB: N
  function:
    matmul: *real_precisions
  matrix_size:
    - { M:  512, N:  512, K:  512 }
    - { M: 960,  N: 1024, K: 1024 }
    - { M: 1024, N: 1024, K: 1024 }
  alpha: [ 0.0, 0.5, 1.0 ]
  beta:  [ 0.0, 0.5, 1.0 ]

- name: matmul_bias_relu
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  activation_type: relu
  bias_vector: [0, 1]
  unit_check: 1

- name: matmul_bias_gelu
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  activation_type: gelu
  bias_vector: [0, 1]
  unit_check: 0
  norm_check: 1

- name: matmul_bias_only
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  bias_vector: 1
  unit_check: 1

- name: matmul_bias_type
  category: pre_checkin
  function:
    matmul: *real_precisions_2b
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  bias_vector: 1
  bias_type: [default, f32_r]
  unit_check: 1

- name: matmul_fallback_equality
  category: pre_checkin
  function:
    matmul: *hpa_half_precision
  matrix_size:
    - { M:  1104, N:  1   , K:  4608 }
    - { M:  1104, N:  16  , K:  4608 }
    - { M:  1104, N:  1335, K:  4608 }
    - { M:  1104, N:  1408, K:  4608 }
    - { M:  4608, N:  1   , K:  320 }
    - { M:  4608, N:  16  , K:  320 }
    - { M:  4608, N:  1335, K:  320 }
    - { M:  4608, N:  1408, K:  320 }
    - { M:  16  , N:  1   , K:  4608 }
    - { M:  16  , N:  16  , K:  4608 }
    - { M:  16  , N:  1335, K:  4608 }
    - { M:  16  , N:  1408, K:  4608 }
    - { M:  768 , N:  1   , K:  4608 }
    - { M:  768 , N:  16  , K:  4608 }
    - { M:  768 , N:  1335, K:  4608 }
    - { M:  768 , N:  1408, K:  4608 }
    - { M:  4608, N:  1   , K:  768 }
    - { M:  4608, N:  16  , K:  768 }
    - { M:  4608, N:  1335, K:  768 }
    - { M:  4608, N:  1408, K:  768 }
  transA: N
  transB: N
  alpha: 1
  beta: [ 0.0, 2.0 ]

- name: matmul_scaleD
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  scaleD_vector: 1
  unit_check: 1

...
