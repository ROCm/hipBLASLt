---
include: hipblaslt_common.yaml
include: known_bugs.yaml
include: matmul_common.yaml

Definitions:
  - &alpha_beta_range
    - { alpha:  5, beta:  0 }
    - { alpha:  0, beta:  3 }
    - { alpha:  1, beta:  3 }
    - { alpha:  1, beta:  1 }

  - &alpha_beta_range_small
    - { alpha: 2, alphai: 2, beta: -1.0, betai: 2.0 }

  - &transA_transB_range
    - { transA: N, transB: N }
    - { transA: N, transB: T }
    - { transA: T, transB: N }
    - { transA: T, transB: T }

  - &deepbench_alpha_beta_range
    - { alpha: 1, beta: 0 }
    - { alpha: 1, beta: 1 }

  - &deepbench_transA_transB_range
    - { transA: N, transB: N }
    - { transA: N, transB: T }
    - { transA: T, transB: N }

  - &ldd_transA_transB_range
    - { transA: N, transB: N }
    - { transA: N, transB: T }
    - { transA: T, transB: N }
    - { transA: T, transB: T }


Tests:
- name: matmul_bad_arg
  category: pre_checkin
  function:
    - matmul_bad_arg: *real_precisions
  transA: N
  transB: N
  fortran: [ false, true ]

# Tests confirm no NaN propagation when alpha = 0, 2 and beta = 0. Value .NaN is converted into zero
- {name: alpha_beta_zero_NaN, category: pre_checkin, precision: *real_precisions,
   function: matmul, transA: N, transB: N, M: 256, N: 128, K:  64, alpha: [ .NaN, 2 ], beta: [ .NaN, 2 ] }

- name: matmul_one
  category: quick
  function:
    matmul: *real_precisions
  matrix_size: *one_matrix_size_range
  transA_transB: *transA_transB_range
  alpha: [ 0.0, 1.0 ]
  beta: [ 0.0, 2.0 ]

- name: matmul_small
  category: quick
  function:
    matmul: *real_precisions
  matrix_size: *small_matrix_size_range
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range

- name: matmul_medium
  category: pre_checkin
  function:
    matmul: *real_precisions
  matrix_size: *medium_matrix_size_range
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range

- name: matmul_batch_medium
  category: pre_checkin
  function:
    matmul: *real_precisions
  matrix_size: *medium_matrix_size_range
  transA_transB: *transA_transB_range
  alpha_beta: *alpha_beta_range
  batch_count: 10

#TODO: enable it after enable fp16altimpl
#- name: matmul_medium_alt
#  category: pre_checkin
#  function:
#    matmul: *hpa_half_precision
#  matrix_size: *medium_matrix_size_range
#  transA_transB: *transA_transB_range
#  alpha: 1
#  beta: 0
#  initialization: special

- name: matmul_medium_HMM
  category: HMM
  function:
    matmul: *real_precisions
  matrix_size: *medium_matrix_size_range
  transA: [ N ]
  transB: [ N ]
  alpha: 1
  beta: 1
  HMM: true

- name: matmul_chunk
  category: pre_checkin
  function:
    matmul: *real_precisions
  matrix_size: *chunk_matrix_size_range
  transA_transB: *transA_transB_range
  alpha: 2
  beta: 3

- name: matmul_deepbench
  category: nightly
  function:
    matmul: *real_precisions
  matrix_size: *deepbench_sizes
  alpha_beta: *deepbench_alpha_beta_range
  transA_transB: *deepbench_transA_transB_range

#TODO: enable it after enable fp16altimpl
#- name: matmul_deepbench_alt
#  category: nightly
#  function:
#    matmul: *hpa_half_precision
#  matrix_size: *deepbench_sizes
#  alpha: 1
#  beta: 0
#  flags: 4
#  initialization: special
#  transA_transB: *deepbench_transA_transB_range

- name: resnet50_fwd
  category: nightly
  function:
    matmul: *real_precisions
  transA: N
  transB: N
  matrix_size: *resnet50_fwd_sizes
  alpha: 1
  beta: 0

#TODO: enable it after enable fp16altimpl
#- name: resnet50_fwd_alt
#  category: nightly
#  function:
#    matmul: *hpa_half_precision
#  transA: N
#  transB: N
#  matrix_size: *resnet50_fwd_sizes
#  alpha: 1
#  beta: 0
#  initialization: special

- name: resnet50_bwdwrw
  category: nightly
  function:
    matmul: *real_precisions
  transA: T
  transB: N
  matrix_size: *resnet50_bwdwrw_sizes
  alpha: 1
  beta: 1

- name: resnet50_bwddata
  category: nightly
  function:
    matmul: *real_precisions
  transA: N
  transB: T
  matrix_size: *resnet50_bwddata_sizes
  alpha: 1
  beta: 0

- name: inception4_fwd
  category: nightly
  function:
    matmul: *real_precisions
  transA: N
  transB: N
  matrix_size: *inception4_fwd_sizes
  alpha: 1
  beta: 0

- name: inception4_bwdwrw
  category: nightly
  function:
    matmul: *real_precisions
  transA: T
  transB: N
  matrix_size: *inception4_bwdwrw_sizes
  alpha: 1
  beta: 1

- name: inception4_bwddata
  category: nightly
  function:
    matmul: *real_precisions
  transA: N
  transB: T
  matrix_size: *inception4_bwddata_sizes
  alpha: 1
  beta: 0

- name: ctest_bwdwrw
  category: nightly
  function:
    matmul: *real_precisions
  transA: T
  transB: N
  alpha: 1
  beta: 1
  matrix_size: *ctest_bwdwrw_sizes

- name: ctest_fwd
  category: nightly
  function:
    matmul: *real_precisions
  transA: N
  transB: N
  alpha: 1
  beta: 0
  matrix_size: *ctest_fwd_sizes

- name: matmul_8
  category: quick
  function:
    matmul: *real_precisions
  M: 8
  N: 8
  K: 8
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_16
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 16
  K: 16
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_24
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 24
  N: 24
  K: 24
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_algo
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 32
  N: 32
  K: 32
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range
  matmul_algo: [ 0, 1 ]

- name: matmul_32_8_128
  category: nightly
  function:
    matmul: *real_precisions
  M: [8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128]
  N: 32
  K: 32
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_48_8_128
  category: nightly
  function:
    matmul: *real_precisions
  M: 48
  N: [8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128]
  K: 48
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_64_8_128
  category: nightly
  function:
    matmul: *real_precisions
  M: 64
  N: 64
  K: [16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128]
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_64_8
  category: quick
  function:
    matmul: *real_precisions
  M: 64
  N: 8
  K: 8
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_8_64
  category: quick
  function:
    matmul: *real_precisions
  M: 8
  N: 8
  K: 64
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range


- name: matmul_96
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 96
  N: 96
  K: 96
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_128
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 128
  N: 128
  K: 128
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_256
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 256
  N: 256
  K: 256
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_256_8_16
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 256
  N: 8
  K: 16
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_16_256_8
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 16
  N: 256
  K: 8
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_8_16_256
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 8
  N: 16
  K: 256
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_512
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: 512
  N: 512
  K: 512
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_1024
  category: nightly
  function:
    matmul: *real_precisions
  M: 1024
  N: 1024
  K: 1024
  alpha_beta: *alpha_beta_range
  transA_transB: *transA_transB_range

- name: matmul_bf16
  category: quick
  function:
    matmul: *hpa_bf16_precision
  transA: T
  transB: N
  alpha: 1
  beta: 0
  matrix_size:
    - { M:  512, N:  512, K:  512 }
    - { M:  960, N: 1024, K: 1024 }
    - { M: 1024, N: 1024, K: 1024 }

- name: matmul_small2
  category: pre_checkin
  transA: N
  transB: N
  function:
    matmul: *real_precisions
  matrix_size:
    - { M:  512, N:  512, K:  512 }
    - { M: 960,  N: 1024, K: 1024 }
    - { M: 1024, N: 1024, K: 1024 }
  alpha: [ 0.0, 0.5, 1.0 ]
  beta:  [ 0.0, 0.5, 1.0 ]

- name: matmul_bias_relu
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  activation_type: relu
  bias_vector: [0, 1]
  unit_check: 1

- name: matmul_bias_gelu
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  activation_type: gelu
  bias_vector: [0, 1]
  unit_check: 0
  norm_check: 1

- name: matmul_bias_only
  category: pre_checkin
  function:
    matmul: *real_precisions
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  bias_vector: 1
  unit_check: 1

- name: matmul_bias_type
  category: pre_checkin
  function:
    matmul: *real_precisions_2b
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  bias_vector: 1
  bias_type: [default, f32_r]
  unit_check: 1

- name: matmul_fallback_equality_NN
  category: pre_checkin
  function:
    matmul: *hpa_half_precision
  matrix_size:
    - { M:  1104 , N:  1   , K:  4608 }
    - { M:  1104 , N:  16  , K:  4608 }
    - { M:  1104 , N:  1335, K:  4608 }
    - { M:  1104 , N:  1408, K:  4608 }
    - { M:  4608 , N:  1   , K:  320 }
    - { M:  4608 , N:  16  , K:  320 }
    - { M:  4608 , N:  1335, K:  320 }
    - { M:  4608 , N:  1408, K:  320 }
    - { M:  16   , N:  1   , K:  4608 }
    - { M:  16   , N:  16  , K:  4608 }
    - { M:  16   , N:  1335, K:  4608 }
    - { M:  16   , N:  1408, K:  4608 }
    - { M:  768  , N:  1   , K:  4608 }
    - { M:  768  , N:  16  , K:  4608 }
    - { M:  768  , N:  1335, K:  4608 }
    - { M:  768  , N:  1408, K:  4608 }
    - { M:  4608 , N:  1   , K:  768 }
    - { M:  4608 , N:  16  , K:  768 }
    - { M:  4608 , N:  1335, K:  768 }
    - { M:  4608 , N:  1408, K:  768 }
    - { M:  320  , N:  8192, K:  320 }
    - { M:  640  , N:  2048, K:  640 }
    - { M:  320  , N:  8192, K:  1280 }
    - { M:  1280 , N:  512 , K:  1280 }
    - { M:  10240, N:  512 , K:  1280 }
    - { M:  2560 , N:  8192, K:  320 }
    - { M:  5120 , N:  2048, K:  640 }
    - { M:  1280 , N:  512 , K:  5120 }
    - { M:  640  , N:  2048, K:  2560 }
    - { M:  320  , N:  154 , K:  768 }
    - { M:  1280 , N:  154 , K:  768 }
  transA: N
  transB: N
  alpha: 1
  beta: [ 0.0, 2.0 ]

- name: matmul_equality_NN_bias_vector_dst_fp16_32
  category: pre_checkin
  function:
    matmul: *hpa_half_precisions_fp_16_32_dst
  matrix_size:
    - { M:  1104 , N:  1   , K:  4608 }
    - { M:  1104 , N:  16  , K:  4608 }
    - { M:  1104 , N:  1335, K:  4608 }
    - { M:  1104 , N:  1408, K:  4608 }
    - { M:  4608 , N:  1   , K:  320 }
    - { M:  4608 , N:  16  , K:  320 }
    - { M:  4608 , N:  1335, K:  320 }
    - { M:  4608 , N:  1408, K:  320 }
    - { M:  16   , N:  1   , K:  4608 }
    - { M:  16   , N:  16  , K:  4608 }
    - { M:  16   , N:  1335, K:  4608 }
    - { M:  16   , N:  1408, K:  4608 }
    - { M:  768  , N:  1   , K:  4608 }
    - { M:  768  , N:  16  , K:  4608 }
    - { M:  768  , N:  1335, K:  4608 }
    - { M:  768  , N:  1408, K:  4608 }
    - { M:  4608 , N:  1   , K:  768 }
    - { M:  4608 , N:  16  , K:  768 }
    - { M:  4608 , N:  1335, K:  768 }
    - { M:  4608 , N:  1408, K:  768 }
  transA: N
  transB: N
  alpha: [ 1.0, 2.0 ]
  beta: [ 0.0, 2.0 ]
  bias_vector: [0, 1]
  scaleAlpha_vector: [0, 1]

- name: matmul_f8_bf8_dst_fp32
  category: pre_checkin
  function:
    matmul: *real_precisions_1b_dst_f32
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA: T
  transB: N
  alpha: 1
  beta: [ 0.0, 2.0 ]
  scaleA: [ 0, 1]
  scaleB: [ 0, 1]
  bias_vector: [0, 1]
  bias_type: f32_r
  unit_check: 1
  gpu_arch: '94?'

- name: matmul_f8_bf8_dst_fp16
  category: pre_checkin
  function:
    matmul: *real_precisions_1b_dst_f16
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA: T
  transB: N
  alpha: 1
  beta: [ 0.0, 2.0 ]
  scaleA: [ 0, 1]
  scaleB: [ 0, 1]
  bias_vector: [0, 1]
  bias_type: f16_r
  unit_check: 1
  gpu_arch: '94?'


- name: matmul_fallback_equality_NN_batch16
  category: pre_checkin
  function:
    matmul: *hpa_half_precision
  matrix_size:
    - { M:  40   , N:  4096, K:  4096 }
    - { M:  80   , N:  1024, K:  1024 }
    - { M:  40   , N:  4096, K:  77 }
    - { M:  80   , N:  1024, K:  77 }
    - { M:  160  , N:  256 , K:  256 }
  transA: N
  transB: N
  alpha: 1
  beta: [ 0.0, 2.0 ]
  batch_count: 16

- name: matmul_fallback_equality_TN_batch16
  category: pre_checkin
  function:
    matmul: *hpa_half_precision
  matrix_size:
    - { M:  4096, N:  4096, K:  40 }
    - { M:  1024, N:  1024, K:  80 }
    - { M:  77  , N:  1024, K:  80 }
    - { M:  77  , N:  4096, K:  40 }
    - { M:  77  , N:  256 , K:  160 }
  transA: T
  transB: N
  alpha: 1
  beta: [ 0.0, 2.0 ]
  batch_count: 16

- name: matmul_groupedgemm_specific_sizes
  category: pre_checkin
  function:
    matmul: *hpa_half_precisions_fp_16_32_dst
  matrix_size:
    - { M:  4096, N:  4096, K:  40 }
    - { M:  1024, N:  1024, K:  80 }
    - { M:  77  , N:  1024, K:  80 }
    - { M:  77  , N:  4096, K:  40 }
    - { M:  77  , N:  256 , K:  160 }
  transA_transB: *transA_transB_range
  grouped_gemm: [1, 5]
  alpha: 1
  beta: [ 0.0, 2.0 ]
  unit_check: 1

- name: matmul_groupedgemm
  category: pre_checkin
  function:
    matmul: *hpa_half_precisions_fp_16_32_dst
  M: [127, 129]
  N: [127, 129]
  K: [127, 129]
  transA_transB: *transA_transB_range
  grouped_gemm: [1, 7]
  alpha: 1
  beta: [ 0.0, 2.0 ]
  unit_check: 1

- name: matmul_groupedgemm_zero_n
  category: pre_checkin
  function:
    matmul: *hpa_half_precision
  M: [127, 129]
  N: [127, 0]
  K: [127, 129]
  transA_transB: *transA_transB_range
  grouped_gemm: [1, 7]
  alpha: 1
  beta: [ 0.0, 2.0 ]
  unit_check: 1

- name: matmul_bias_gelu_aux_fp16
  category: pre_checkin
  function:
    matmul: *hpa_half_precision
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  use_e: 1
  activation_type: gelu
  bias_vector: [0, 1]
  unit_check: 0
  norm_check: 1

- name: matmul_dgelu_fp16
  category: pre_checkin
  function:
    matmul: *hpa_half_precision
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  use_e: 1
  gradient: 1
  activation_type: gelu
  unit_check: 1

- name: matmul_dgelu_bias
  category: pre_checkin
  function:
    matmul: *hpa_half_precision
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  use_e: 1
  gradient: 1
  activation_type: gelu
  bias_vector: 1
  bias_type: f32_r
  unit_check: 0
  norm_check: 1

- name: matmul_bgradb_f8_bf8_dst_fp32
  category: pre_checkin
  function:
    matmul: *real_precisions_1b_dst_f32
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA: T
  transB: N
  alpha: 1
  beta: [ 0.0, 2.0 ]
  gradient: 1
  bias_vector: 1
  bias_type: f32_r
  bias_source: b
  scaleA: [ 0, 1]
  scaleB: [ 0, 1]
  unit_check: 1
  gpu_arch: '94?'

- name: matmul_bgrada_f16
  category: pre_checkin
  function:
    matmul: *hpa_half_precision
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  gradient: 1
  bias_vector: 1
  bias_type: f32_r
  bias_source: a
  unit_check: 1
  # norm_check: 1

- name: matmul_bias_gelu_aux_fp32
  category: pre_checkin
  function:
    matmul: *single_precision
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  use_e: 1
  activation_type: gelu
  bias_vector: [0, 1]
  unit_check: 0
  norm_check: 1
  gpu_arch: '94?'

- name: matmul_dgelu_fp32
  category: pre_checkin
  function:
    matmul: *single_precision
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  use_e: 1
  gradient: 1
  activation_type: gelu
  unit_check: 0
  norm_check: 1
  gpu_arch: '94?'

- name: matmul_bgradb_fp32
  category: pre_checkin
  function:
    matmul: *single_precision
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  gradient: 1
  bias_vector: 1
  bias_type: f32_r
  bias_source: b
  unit_check: 1
  gpu_arch: '94?'

- name: matmul_bgradb_f16
  category: pre_checkin
  function:
    matmul: *hpa_half_precision
  M: [128, 129]
  N: [128, 129]
  K: [128, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  gradient: 1
  bias_vector: 1
  bias_type: [f16_r, f32_r]
  bias_source: b
  unit_check: 1

- name: matmul_extapi_algo_method_gemm
  category: pre_checkin
  function:
    matmul: *hpa_half_precision
  M: [127, 129]
  N: [127, 129]
  K: [127, 129]
  transA_transB: *transA_transB_range
  algo_method: [1, 2]
  alpha: 1
  beta: [ 0.0, 2.0 ]
  unit_check: 1

- name: matmul_extapi_gemm
  category: pre_checkin
  function:
    matmul: *hpa_half_precision
  M: [127, 129]
  N: [127, 129]
  K: [127, 129]
  transA_transB: *transA_transB_range
  use_ext: [1]
  use_ext_setproblem: [0, 1]
  algo_method: [0, 1, 2]
  alpha: 1
  beta: [ 0.0, 2.0 ]
  unit_check: 1

- name: matmul_extapi_groupedgemm
  category: pre_checkin
  function:
    matmul: *hpa_half_precision
  M: [127, 129]
  N: [127, 129]
  K: [127, 129]
  transA_transB: *transA_transB_range
  grouped_gemm: [2]
  use_ext_setproblem: [0, 1]
  algo_method: [0, 1, 2]
  alpha: 1
  beta: [ 0.0, 2.0 ]
  unit_check: 1

- name: matmul_gemm_xf32
  category: pre_checkin
  function:
    matmul: *xf32_precision
  matrix_size:
    - { M:  128,  N:  128,  K:  128  }
    - { M:  131,  N:  131,  K:  131  }
    - { M:  1024, N:  1024, K:  1024 }
    - { M:  1031, N:  1031, K:  1031 }
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  unit_check: 1
  gpu_arch: '94?'

- name: matmul_extapi_groupedgemm
  category: pre_checkin
  function:
    matmul: *hpa_half_precision
  M: [127, 129]
  N: [127, 129]
  K: [127, 129]
  transA_transB: *transA_transB_range
  grouped_gemm: [2]
  use_ext_setproblem: [1]
  algo_method: [0, 1, 2]
  use_user_args: 1
  alpha: 1
  beta: [ 0.0, 2.0 ]
  unit_check: 1

- name: matmul_gemm_double
  category: pre_checkin
  function:
    matmul: *double_precision
  matrix_size:
    - { M:  128,  N:  128,  K:  1024 }
    - { M:  4608, N:  1335, K:  640  }
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  unit_check: 1

- name: matmul_gemm_i8
  category: pre_checkin
  function:
    matmul: *integer8_precisions
  matrix_size:
    - { M:  128,  N:  128,  K:  128  }
    - { M:  131,  N:  131,  K:  131  }
    - { M:  1024, N:  1024, K:  1024 }
    - { M:  1031, N:  1031, K:  1031 }
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0, 2 ]
  unit_check: 1
  gpu_arch: '90a'

- name: matmul_gemm_i8_94x
  category: pre_checkin
  function:
    matmul: *integer8_precisions
  matrix_size:
    - { M:  128,  N:  128,  K:  128  }
    - { M:  131,  N:  131,  K:  131  }
    - { M:  1024, N:  1024, K:  1024 }
    - { M:  1031, N:  1031, K:  1031 }
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0, 2 ]
  scaleAlpha_vector: [0, 1]
  activation_type: relu
  unit_check: 1
  gpu_arch: '94?'

- name: matmul_gemm_mix_precisions
  category: pre_checkin
  function:
    matmul: *real_mix_precisions
  matrix_size:
  M: [127, 129]
  N: [127, 129]
  K: [127, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0, 2 ]
  use_ext: [1]
  use_ext_setproblem: [1]
  bias_vector: 1
  bias_type: [f32_r]
  unit_check: 1
  gpu_arch: '94?'

- name: matmul_gemm_mix_precisions_fp8
  category: pre_checkin
  function:
    matmul: *real_mix_precisions_fp8
  M: [127, 129]
  N: [127, 129]
  K: [127, 129]
  transA_transB: *transA_transB_range
  alpha: 1
  beta: [ 0.0, 2.0 ]
  scaleA: [0,1]
  scaleB: [0,1]
  scaleC: [0,1]
  scaleD: [0,1]
  use_ext: [1]
  use_ext_setproblem: [1]
  bias_vector: 1
  bias_type: [f32_r]
  unit_check: 1
  gpu_arch: '94?'
...
