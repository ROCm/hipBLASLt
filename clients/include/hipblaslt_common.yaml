# Data types are defined as either aliases to Python-recognized ctypes,
# or enums defined with c_int base clases and attributes.
Datatypes:
  - hipDataType:
      bases: [ c_int ]
      attr:
        default: 0
        f16_r: 2
        f32_r: 0
        f64_r: 1
        i8_r: 3
        i32_r: 10
        bf16_r: 14
        f8_r: 1000
        bf8_r: 1001
  - hipblasComputeType_t:
      bases: [ c_int ]
      attr:
        c_f32_r: 2
        c_xf32_r: 6
        c_f64_r: 7
        c_i32_r: 9
        c_f32_fast_f16_r: 4
  - { half: f16_r }
  - hipblaslt_initialization:
      bases: [ c_int ]
      attr:
        rand_int: 111
        trig_float: 222
        hpl: 333
        special: 444
  - hipblaslt_activation_type:
      bases: [ c_int ]
      attr:
        none: 1
        relu: 2
        gelu: 3
  - hipblaslt_bias_source:
      bases: [ c_int ]
      attr:
        a: 1
        b: 2
        d: 3



Common threads and streams: &common_threads_streams
  - { threads: 0,  streams: 0}
  # - { threads: 0, streams: 1 }
  # - { threads: 1, streams: 0 }
  # - { threads: 3, streams: 3 }
  # - { threads: 5, streams: 5 }

Real precisions: &real_precisions
  - &hpa_half_precision
    { a_type: f16_r, b_type: f16_r, c_type: f16_r, d_type: f16_r, compute_type: c_f32_r, scale_type: f32_r}
  - &hpa_bf16_precision
    { a_type: bf16_r, b_type: bf16_r, c_type: bf16_r, d_type: bf16_r, compute_type: c_f32_r, scale_type: f32_r}
  - &single_precision
    { a_type: f32_r, b_type: f32_r, c_type: f32_r, d_type: f32_r, compute_type: c_f32_r , scale_type: f32_r}

Real precisions 1 bytes: &real_precisions_1b
  - &f8_precision_dst_fp32
    { a_type: f8_r, b_type: f8_r, c_type: f32_r, d_type: f32_r, compute_type: c_f32_r, scale_type: f32_r}
  - &bf8f8_precision_dst_fp32
    { a_type: bf8_r, b_type: f8_r, c_type: f32_r, d_type: f32_r, compute_type: c_f32_r, scale_type: f32_r}
  - &f8bf8_precision_dst_fp32
    { a_type: f8_r, b_type: bf8_r, c_type: f32_r, d_type: f32_r, compute_type: c_f32_r, scale_type: f32_r}
  - &f8_precision_dst_fp16
    { a_type: f8_r, b_type: f8_r, c_type: f16_r, d_type: f16_r, compute_type: c_f32_r, scale_type: f32_r}
  - &bf8f8_precision_dst_fp16
    { a_type: bf8_r, b_type: f8_r, c_type: f16_r, d_type: f16_r, compute_type: c_f32_r, scale_type: f32_r}
  - &f8bf8_precision_dst_fp16
    { a_type: f8_r, b_type: bf8_r, c_type: f16_r, d_type: f16_r, compute_type: c_f32_r, scale_type: f32_r}

Real precisions 1 bytes dst fp32: &real_precisions_1b_dst_f32
  - *f8_precision_dst_fp32
  - *bf8f8_precision_dst_fp32
  - *f8bf8_precision_dst_fp32

Real precisions 1 bytes dst fp16: &real_precisions_1b_dst_f16
  - *f8_precision_dst_fp16
  - *bf8f8_precision_dst_fp16
  - *f8bf8_precision_dst_fp16

Real precisions 2 bytes: &real_precisions_2b
  - *hpa_half_precision
  - *hpa_bf16_precision

Real precisions xf32: &real_precisions_intermeddiate
  - &xf32_precision
    { a_type: f32_r, b_type: f32_r, c_type: f32_r, d_type: f32_r, compute_type: c_xf32_r , scale_type: f32_r}

Real precisions i8: &integer_precisions_i8
  - &integer8_precisions
    { a_type: i8_r, b_type: i8_r, c_type: i32_r, d_type: i32_r, compute_type: c_i32_r , scale_type: i32_r}

Real precisions dstf32: &hpa_half_precisions_fp_16_32_dst
  - &hpa_half_fp16dst_precision
    { a_type: f16_r, b_type: f16_r, c_type: f16_r, d_type: f16_r, compute_type: c_f32_r, scale_type: f32_r}
  - &hpa_half_fp32dst_precision
    { a_type: f16_r, b_type: f16_r, c_type: f32_r, d_type: f32_r, compute_type: c_f32_r, scale_type: f32_r}

Real precisions gemm only: &real_precisions_gemm_only
  - &double_precision
    { a_type: f64_r, b_type: f64_r, c_type: f64_r, d_type: f64_r, compute_type: c_f64_r , scale_type: f64_r}

Real mix precisions: &real_mix_precisions
  - &fp8fp16_precision_dst_fp16
    { a_type: f8_r, b_type: f16_r, c_type: f16_r, d_type: f16_r, compute_type: c_f32_fast_f16_r, scale_type: f32_r}
  - &fp16fp8_precision_dst_fp16
    { a_type: f16_r, b_type: f8_r, c_type: f16_r, d_type: f16_r, compute_type: c_f32_fast_f16_r, scale_type: f32_r}
  - &fp8fp16_precision_dst_fp32
    { a_type: f8_r, b_type: f16_r, c_type: f32_r, d_type: f32_r, compute_type: c_f32_fast_f16_r, scale_type: f32_r}
  - &fp16fp8_precision_dst_fp32
    { a_type: f16_r, b_type: f8_r, c_type: f32_r, d_type: f32_r, compute_type: c_f32_fast_f16_r, scale_type: f32_r}

Real mix precisions fp8: &real_mix_precisions_fp8
  - &fp8fp16_precision_dst_fp8
    { a_type: f8_r, b_type: f16_r, c_type: f8_r, d_type: f8_r, compute_type: c_f32_fast_f16_r, scale_type: f32_r}
  - &fp16fp8_precision_dst_fp8
    { a_type: f16_r, b_type: f8_r, c_type: f8_r, d_type: f8_r, compute_type: c_f32_fast_f16_r, scale_type: f32_r}

# The Arguments struct passed directly to C++. See hipblaslt_arguments.hpp.
# The order of the entries is significant, so it can't simply be a dictionary.
# The types on the RHS are eval'd for Python-recognized types including ctypes
# and datatypes defined in Datatypes above. T*n represents array of length n.
Arguments:
  - function: c_char*64
  - name: c_char*64
  - category: c_char*64
  - known_bug_platforms: c_char*64
  - alpha: c_float
  - beta: c_float
  - stride_a: c_int64*32
  - stride_b: c_int64*32
  - stride_c: c_int64*32
  - stride_d: c_int64*32
  - stride_e: c_int64*32
  - user_allocated_workspace: c_size_t
  - M: c_int64*32
  - N: c_int64*32
  - K: c_int64*32
  - lda: c_int64*32
  - ldb: c_int64*32
  - ldc: c_int64*32
  - ldd: c_int64*32
  - lde: c_int64*32
  - batch_count: c_int32
  - iters: c_int32
  - cold_iters: c_int32
  - algo: c_uint32
  - solution_index: c_int32
  - requested_solution_num: c_int32
  - a_type: hipDataType
  - b_type: hipDataType
  - c_type: hipDataType
  - d_type: hipDataType
  - compute_type: hipblasComputeType_t
  - scale_type: hipDataType
  - initialization: hipblaslt_initialization
  - gpu_arch: c_char*4
  - pad: c_uint32
  - grouped_gemm: c_int32
  - threads: c_uint16
  - streams: c_uint16
  - devices: c_uint8
  - norm_check: c_int8
  - unit_check: c_int8
  - timing: c_int8
  - transA: c_char
  - transB: c_char
  - activation_type: hipblaslt_activation_type
  - activation_arg1: c_float
  - activation_arg2: c_float
  - bias_type: hipDataType
  - bias_source: hipblaslt_bias_source
  - bias_vector: c_bool
  - scaleA: c_bool
  - scaleB: c_bool
  - scaleC: c_bool
  - scaleD: c_bool
  - scaleE: c_bool
  - scaleAlpha_vector: c_bool
  - c_noalias_d: c_bool
  - HMM: c_bool
  - use_e: c_bool
  - gradient: c_bool
  - norm_check_assert: c_bool
  - use_ext: c_bool
  - use_ext_setproblem: c_bool
  - algo_method: c_int32
  - use_user_args: c_bool
  - print_solution_found: c_bool

# These named dictionary lists [ {dict1}, {dict2}, etc. ] supply subsets of
# test arguments in a structured way. The dictionaries are applied to the test
# one at a time, to generate combinations.  If one of this table's entries is
# a dictionary of size one, it means that the argument named by its key takes
# on values paired with argument named by its value. For example:
#
# - function: precision
#
# when used with the code:
#
# function:
#   func1: prec1
#   func2: prec2
#   func3: prec3
#
# causes (function, precision) to take on the values (func1, prec1),
# (func2, prec2), (func3, prec3), etc.

Dictionary lists to expand:
  - arguments
  - transA_transB
  - alpha_beta
  - alphai_betai
  - incx_incy
  - matrix_size
  - precision
  - function: precision
  - threads_streams

# In case an array argument needs to be passed directly to C as an array,
# it needs to be listed here to avoid being expanded into multiple test
# cases with each of its elements.
Lists to not expand:
  - e.g., an array argument not to be expanded

# Defaults
Defaults:
  M: 128
  N: 128
  K: 128
  alpha: 1.0
  beta: 0.0
  transA: '*'
  transB: '*'
  batch_count: 1
  HMM: false
  pad: 4096
  threads: 0
  streams: 0
  devices: 0
  gpu_arch: ''
  norm_check: 0
  unit_check: 1
  timing: 0
  iters: 10
  cold_iters: 2
  algo: 0
  solution_index: 0
  requested_solution_num: 1
  workspace_size: 0
  initialization: rand_int
  category: nightly
  known_bug_platforms: ''
  name: hipblaslt-bench
  c_noalias_d: false
  user_allocated_workspace: 0
  activation_type: none
  activation_arg1: 0
  activation_arg2: 0
  bias_type: default
  bias_source: d
  use_e: false
  gradient: false
  bias_vector: false
  scaleA: false
  scaleB: false
  scaleC: false
  scaleD: false
  scaleE: false
  scaleAlpha_vector: false
  grouped_gemm: 0
  norm_check_assert: true
  use_ext: false
  use_ext_setproblem: false
  algo_method: 0
  use_user_args: false
  print_solution_found: false
