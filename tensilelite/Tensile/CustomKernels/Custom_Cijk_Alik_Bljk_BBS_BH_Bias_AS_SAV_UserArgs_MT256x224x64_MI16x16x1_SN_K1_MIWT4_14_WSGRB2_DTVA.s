
/******************************************/
/* Begin Kernel                           */
/******************************************/
.amdgcn_target "amdgcn-amd-amdhsa--gfx942"
.text
.protected Custom_Cijk_Alik_Bljk_BBS_BH_Bias_AS_SAV_UserArgs_MT256x224x64_MI16x16x1_SN_K1_MIWT4_14_WSGRB2_DTVA
.globl Custom_Cijk_Alik_Bljk_BBS_BH_Bias_AS_SAV_UserArgs_MT256x224x64_MI16x16x1_SN_K1_MIWT4_14_WSGRB2_DTVA
.p2align 8
.type Custom_Cijk_Alik_Bljk_BBS_BH_Bias_AS_SAV_UserArgs_MT256x224x64_MI16x16x1_SN_K1_MIWT4_14_WSGRB2_DTVA,@function
.section .rodata,#alloc
.p2align 6
.amdhsa_kernel Custom_Cijk_Alik_Bljk_BBS_BH_Bias_AS_SAV_UserArgs_MT256x224x64_MI16x16x1_SN_K1_MIWT4_14_WSGRB2_DTVA
  .amdhsa_user_sgpr_kernarg_segment_ptr 1
  .amdhsa_accum_offset 256 // accvgpr offset
  .amdhsa_next_free_vgpr 480 // vgprs
  .amdhsa_next_free_sgpr 84 // sgprs
  .amdhsa_group_segment_fixed_size 35840 // lds bytes
  .amdhsa_private_segment_fixed_size 0
  .amdhsa_system_sgpr_workgroup_id_x 1
  .amdhsa_system_sgpr_workgroup_id_y 1
  .amdhsa_system_sgpr_workgroup_id_z 1
  .amdhsa_system_vgpr_workitem_id 0
  .amdhsa_float_denorm_mode_32 3
  .amdhsa_float_denorm_mode_16_64 3
  .amdhsa_user_sgpr_count 13
  .amdhsa_user_sgpr_kernarg_preload_length 11
  .amdhsa_user_sgpr_kernarg_preload_offset 0
.end_amdhsa_kernel
.text
/* Num VGPR   =256 */
/* Num AccVGPR=224 */
/* Num SGPR   =84 */

/******************************************/
/* Optimizations and Config:              */
/******************************************/
/* ThreadTile= 16 x 14 */
/* SubGroup= 16 x 16 */
/* VectorWidthA=4 */
/* VectorWidthB=1 */
/* GlobalReadVectorWidthA=8, GlobalReadVectorWidthB=8 */
/* DirectToLdsA=False */
/* DirectToLdsB=False */
/* UseSgprForGRO=1 */
.amdgpu_metadata
---
custom.config:
   ProblemType:
      OperationType: GEMM
      DataType: b
      DestDataType: b
      ComputeDataType: s
      HighPrecisionAccumulate: True
      TransposeA: True
      TransposeB: False
      UseBias: 1
      Activation: True
      UseScaleAlphaVec: 1
      UseBeta: True
      Batched: True
      GroupedGemm: False
      SupportUserArgs: True
   MatrixInstruction: [16, 16, 16, 1, 1, 4, 14, 4, 1]
   1LDSBuffer: 1
   ScheduleIterAlg: 3
   DepthU: 64
   GlobalReadVectorWidthA: 8
   GlobalReadVectorWidthB: 8
   AssertFree0ElementMultiple: 1
   AssertFree1ElementMultiple: 1
   AssertSummationElementMultiple: 1
   NoReject: True
   InternalSupportParams:
      KernArgsVersion: 1
      SupportUserGSU: True
      SupportCustomWGM: True
      SupportCustomStaggerU: True
      UseUniversalArgs: True
amdhsa.version:
  - 1
  - 1
amdhsa.kernels:
  - .name: Custom_Cijk_Alik_Bljk_BBS_BH_Bias_AS_SAV_UserArgs_MT256x224x64_MI16x16x1_SN_K1_MIWT4_14_WSGRB2_DTVA
    .symbol: 'Custom_Cijk_Alik_Bljk_BBS_BH_Bias_AS_SAV_UserArgs_MT256x224x64_MI16x16x1_SN_K1_MIWT4_14_WSGRB2_DTVA.kd'
    .language:                   OpenCL C
    .language_version:
      - 2
      - 0
    .args:
      - .name:            Gemm info
        .size:            4
        .offset:          0
        .value_kind:      by_value
        .value_type:      u32
      - .name:            kernel info0
        .size:            4
        .offset:          4
        .value_kind:      by_value
        .value_type:      u32
      - .name:            kernel info1
        .size:            4
        .offset:          8
        .value_kind:      by_value
        .value_type:      u32
      - .name:            numWG
        .size:            4
        .offset:          12
        .value_kind:      by_value
        .value_type:      u32
      - .name:            SizesFree0
        .size:            4
        .offset:          16
        .value_kind:      by_value
        .value_type:      u32
      - .name:            SizesFree1
        .size:            4
        .offset:          20
        .value_kind:      by_value
        .value_type:      u32
      - .name:            SizesFree2
        .size:            4
        .offset:          24
        .value_kind:      by_value
        .value_type:      u32
      - .name:            SizesSum0
        .size:            4
        .offset:          28
        .value_kind:      by_value
        .value_type:      u32
      - .name:            D
        .size:            8
        .offset:          32
        .value_kind:      global_buffer
        .value_type:      bf16
        .address_space:   generic
      - .name:            C
        .size:            8
        .offset:          40
        .value_kind:      global_buffer
        .value_type:      bf16
        .address_space:   generic
      - .name:            A
        .size:            8
        .offset:          48
        .value_kind:      global_buffer
        .value_type:      bf16
        .address_space:   generic
      - .name:            B
        .size:            8
        .offset:          56
        .value_kind:      global_buffer
        .value_type:      bf16
        .address_space:   generic
      - .name:            strideD0
        .size:            4
        .offset:          64
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideD1
        .size:            4
        .offset:          68
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideC0
        .size:            4
        .offset:          72
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideC1
        .size:            4
        .offset:          76
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideA0
        .size:            4
        .offset:          80
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideA1
        .size:            4
        .offset:          84
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideB0
        .size:            4
        .offset:          88
        .value_kind:      by_value
        .value_type:      u32
      - .name:            strideB1
        .size:            4
        .offset:          92
        .value_kind:      by_value
        .value_type:      u32
      - .name:            alpha
        .size:            4
        .offset:          96
        .value_kind:      by_value
        .value_type:      f32
      - .name:            beta
        .size:            4
        .offset:          100
        .value_kind:      by_value
        .value_type:      f32
      - .name:            AddressScaleAlphaVec
        .size:            8
        .offset:          104
        .value_kind:      global_buffer
        .value_type:      f32
        .address_space:   generic
      - .name:            bias
        .size:            8
        .offset:          112
        .value_kind:      global_buffer
        .value_type:      void
        .address_space:   generic
      - .name:            biasType
        .size:            4
        .offset:          120
        .value_kind:      by_value
        .value_type:      u32
      - .name:            StrideBias
        .size:            4
        .offset:          124
        .value_kind:      by_value
        .value_type:      u32
      - .name:            activationAlpha
        .size:            4
        .offset:          128
        .value_kind:      by_value
        .value_type:      f32
      - .name:            activationBeta
        .size:            4
        .offset:          132
        .value_kind:      by_value
        .value_type:      f32
      - .name:            activationType
        .size:            4
        .offset:          136
        .value_kind:      by_value
        .value_type:      u32
    .group_segment_fixed_size:   35840
    .kernarg_segment_align:      8
    .kernarg_segment_size:       144
    .max_flat_workgroup_size:    256
    .private_segment_fixed_size: 0
    .sgpr_count:                 84
    .sgpr_spill_count:           0
    .vgpr_count:                 256
    .vgpr_spill_count:           0
    .wavefront_size:             64
...
.end_amdgpu_metadata
Custom_Cijk_Alik_Bljk_BBS_BH_Bias_AS_SAV_UserArgs_MT256x224x64_MI16x16x1_SN_K1_MIWT4_14_WSGRB2_DTVA:
label_ASM_Start:  /// Main body of the asm kernel

/* Magic div and mod functions */
.macro V_MAGIC_DIV dstIdx:req dividend:req magicNumber:req magicShift:req magicA:req
    v_mul_hi_u32 v[\dstIdx+1] \dividend \magicNumber
    v_mul_lo_u32 v[\dstIdx+0] \dividend \magicA
    v_add_u32 v[\dstIdx+0] v[\dstIdx+0] v[\dstIdx+1]
    v_lshrrev_b32 v[\dstIdx+0] \magicShift v[\dstIdx+0]
.endm

/******************************************/
/* VGPR Assignments                       */
/******************************************/
/* ValuC range: [0-0), serializedStore enabled */
.set vgprValuC, 0
/* ValuA/B   Xn=PLR buffer idx,  In=InnerUnroll idx */
.set vgprValuA_X0_I0_0, 0

.set vgprValuA_X2_I0_0, 16

.set vgprValuB_X0_I0, 32

.set vgprValuB_X2_I0, 88

.set vgprLocalWriteAddrA, 144
.set vgprLocalWriteAddrB, 145
.set vgprGlobalReadOffsetA, 146
.set vgprGlobalReadOffsetB, 147
.set vgprG2LB, 148
.set vgprValuA_X0_I0_1, 176
.set vgprValuA_X2_I0_1, 192
.set vgprLocalReadAddrA, 208
.set vgprLocalReadAddrB, 209
.set vgprSerial, 210

/******************************************/
/* SGPR Assignments                       */
/******************************************/
.set sgprKernArgAddress, 0
.set sgprWorkGroup0, 2
.set sgprWorkGroup1, 3
.set sgprWorkGroup2, 4
.set sgprArgType, 5
.set sgprGSUSumIdx, 6
.set sgprGSULog2BpeC, 8
.set sgprGSULog2BpeD, 9
.set sgprStaggerU, 10
.set sgprWGM, 11
.set sgprLoopCounterL, 12
.set sgprOrigLoopCounter, 13
.set sgprSrdD, 16
.set sgprSrdC, 20
.set sgprNumWorkGroups0, 14
.set sgprNumWorkGroups1, 15
.set sgprSizesFree, 24
.set sgprSizesSum, 27
.set sgprAddressD, 28
.set sgprAddressC, 30
.set sgprAddressA, 32
.set sgprAddressB, 34
.set sgprStridesD, 36
.set sgprStridesC, 38
.set sgprStridesA, 40
.set sgprStridesB, 42
.set sgprAlpha, 44
.set sgprBeta, 45
.set sgprGSU, 46

/* Size Assignments */
.set sgprSizeI, sgprSizesFree+0
.set sgprSizeJ, sgprSizesFree+1
.set sgprSizeK, sgprSizesFree+2
.set sgprSizeL, sgprSizesSum+0

/* Stride Assignments */
.set constStrideD0I, 1
.set sgprStrideD1J, sgprStridesD+0
.set sgprStrideDK, sgprStridesD+1
.set constStrideC0I, 1
.set sgprStrideC1J, sgprStridesC+0
.set sgprStrideCK, sgprStridesC+1
.set constStrideAL, 1
.set sgprStrideA0I, sgprStridesA+0
.set sgprStrideAK, sgprStridesA+1
.set constStrideBL, 1
.set sgprStrideB1J, sgprStridesB+0
.set sgprStrideBK, sgprStridesB+1

.set MT0, 256
.set MT1, 224
.set DepthU, 64
.set BpeA, 2
.set BpeALog2, 1
.set BpeB, 2
.set BpeBLog2, 1
.set BpeAGR, 2
.set BpeAGRLog2, 1
.set BpeBGR, 2
.set BpeBGRLog2, 1
/* Number of elements to shift-left SRD */
.set SrdShiftLeftA, 8
.set SrdShiftLeftB, 8
/* 2GB limit - set offsets to -1 to exceed this and clamp */
.set BufferLimit, 0xffffffff
.set BufferOOB, 0x80000000

/******************************************/
/* Bits 127:96 of SRD.                    */
/* hex: 0x00020000                        */
/* dst_sel_x (3b): 0                      */
/* dst_sel_y (3b): 0                      */
/* dst_sel_z (3b): 0                      */
/* dst_sel_w (3b): 0                      */
/* num_format (3b): 0                     */
/* data_format (4b): 4                    */
/* user_vm_enable (1b): 0                 */
/* user_vm_mode (1b): 0                   */
/* index_stride (2b): 0                   */
/* add_tid_enable (1b): 0                 */
/* _unusedA (3b): 0                       */
/* nv (1b): 0                             */
/* _unusedB (2b): 0                       */
/* type (2b): 0                           */
/******************************************/
.set Srd127_96, 0x00020000

/* Global Offset A */
.macro GLOBAL_OFFSET_A vgprAddr:req vgprTmp:req
    v_and_b32 v[\vgprTmp+0], 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
    v_and_b32 v[\vgprAddr+0], 15, v[\vgprTmp+0]                               // 1. M offset: mIdx = wtid % MI_M(16)
    v_mul_lo_u32 v[\vgprAddr+0], s[sgprStrideA0I], v[\vgprAddr+0]              // 1. M offset: mOffset = mIdx * mStride(k)
    v_lshlrev_b32 v[\vgprAddr+0], 0x2, v[\vgprAddr+0]                          // 4. apply VectorWidth: bnOffset = bnOffset * vw(4)
    v_and_b32 v[\vgprTmp+0], 63, v[vgprSerial]                    // 5. thread id in wave: wtid = tid % wavelength(64)
    v_lshrrev_b32 v[\vgprTmp+0], 4, v[\vgprTmp+0]                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
    v_lshlrev_b32 v[\vgprTmp+0], 0x3, v[\vgprTmp+0]                          // 5. K offset: lrKOffset = kIdx * mStride(8)
    v_add_u32 v[\vgprAddr+0], v[\vgprTmp+0], v[\vgprAddr+0]                               // 6. offset in wave: lrOffset = bnOffset + lrKOffset
    v_lshrrev_b32 v[\vgprTmp+0], 6, v[vgprSerial]                 // 7. wave offset in M dimen: wtid = tid / dividedForWaveId(64)
    v_and_b32 v[\vgprTmp+0], 3, v[\vgprTmp+0]                                // 7. wave offset in M dimen: wtid0 = wtid % num1DWaves(4)
    v_mul_lo_u32 v[\vgprTmp+0], s[sgprStrideA0I], v[\vgprTmp+0]                          // 7. wave offset in M dimen: wOffset = wtid0 * s[sgprStrideA0I](8192)
    v_lshlrev_b32 v[\vgprTmp+0], 0x6, v[\vgprTmp+0]                          // 7. wave offset in M dimen: wOffset = wOffset * 16 * vw(4)
    v_add_u32 v[\vgprAddr+0], v[\vgprTmp+0], v[\vgprAddr+0]                               // 7. final local read offset: flrOffset = lrOffset + WOffset
    v_add_u32 v[\vgprAddr+0] 0x8 v[\vgprAddr+0]      // add prepad for pointer shift
    v_lshlrev_b32 v[\vgprAddr+0] 0x1 v[\vgprAddr+0]  // offset *= bytes/element
.endm

/* Global Offset B */
.macro GLOBAL_OFFSET_B vgprAddr:req vgprOffsetL:req vgprOffset1J:req vgprTmp:req
    v_mul_lo_u32 v[\vgprTmp+0] s[sgprStrideB1J] v[\vgprOffset1J] // mul d1 lower
    v_add_co_u32 v[\vgprAddr+0] vcc v[\vgprOffsetL] v[\vgprTmp+0] // accumulate K lower
    v_add_u32 v[\vgprAddr+0] 0x8 v[\vgprAddr+0]      // add prepad for pointer shift
    v_lshlrev_b32 v[\vgprAddr+0] 0x1 v[\vgprAddr+0]  // offset *= bytes/element
.endm

/* Dynamic Scalar Divide: vQuotient=vDividend/vDivisor; vRemainder=vDividend%vDivisor; */
.macro DYNAMIC_VECTOR_DIVIDE vQuotient vRemainder vDividend vDivisor vTmp0 vTmp1 sTmp
    v_cvt_f32_u32 v[\vQuotient] v[\vDivisor]
    v_rcp_f32 v[\vQuotient] v[\vQuotient]
    v_mul_f32 v[\vQuotient] 0x4f800000 v[\vQuotient]
    v_cvt_u32_f32 v[\vQuotient] v[\vQuotient]
    v_mul_lo_u32 v[\vRemainder] v[\vDivisor] v[\vQuotient]
    v_mul_hi_u32 v[\vTmp0] v[\vDivisor] v[\vQuotient]
    v_sub_co_u32 v[\vTmp1] vcc 0x0 v[\vRemainder]
    v_cmp_ne_i32 s[\sTmp:\sTmp+1] 0x0 v[\vTmp0]
    v_cndmask_b32 v[\vRemainder] v[\vTmp1] v[\vRemainder] s[\sTmp:\sTmp+1]
    v_mul_hi_u32 v[\vRemainder] v[\vRemainder] v[\vQuotient]
    v_sub_co_u32 v[\vTmp0] vcc v[\vQuotient] v[\vRemainder]
    v_add_co_u32 v[\vQuotient] vcc v[\vQuotient] v[\vRemainder]
    v_cndmask_b32 v[\vQuotient] v[\vQuotient] v[\vTmp0] s[\sTmp:\sTmp+1]
    v_mul_hi_u32 v[\vQuotient] v[\vQuotient] v[\vDividend]
    v_mul_lo_u32 v[\vRemainder] v[\vQuotient] v[\vDivisor]
    v_sub_co_u32 v[\vTmp0] vcc v[\vDividend] v[\vRemainder]
    v_cmp_ge_u32 s[\sTmp:\sTmp+1] v[\vDividend] v[\vRemainder]
    v_add_co_u32 v[\vRemainder] vcc 0x1 v[\vQuotient]
    v_add_co_u32 v[\vTmp1] vcc -1 v[\vQuotient]
    v_cmp_le_u32 vcc v[\vDivisor] v[\vTmp0]
    s_and_b64 vcc s[\sTmp:\sTmp+1] vcc
    v_cndmask_b32 v[\vQuotient] v[\vQuotient] v[\vRemainder] vcc
    v_cndmask_b32 v[\vQuotient] v[\vTmp1] v[\vQuotient] s[\sTmp:\sTmp+1]
    v_cmp_ne_i32 vcc 0x0 v[\vDivisor]
    v_cndmask_b32 v[\vQuotient] -1 v[\vQuotient] vcc // final result
    v_mul_lo_u32 v[\vRemainder] v[\vQuotient] v[\vDivisor]
    v_sub_co_u32 v[\vRemainder] vcc v[\vDividend] v[\vRemainder] // final result
.endm

/******************************************/
/* Allocate Resources                     */
/******************************************/

/* Load num of Gemms */
s_load_dword s47, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x0

/* Load GSU data */
s_load_dword s[sgprGSU], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x4

/* Load WGM data */
s_load_dword s[sgprWGM], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x8
s_waitcnt lgkmcnt(0)
s_lshr_b32 s48, s47, 0x1e                          // Get arg type
s_and_b32 s47, 0x3fffffff, s47                     // Get nums of gemm
s_cmp_eq_u32 s48, 0                                // Is kernel args
s_cbranch_scc0 label_HBMArgs
s_add_u32 s[sgprKernArgAddress], s[sgprKernArgAddress], 0x10 // Shift common args
s_addc_u32 s[sgprKernArgAddress+1], s[sgprKernArgAddress+1], 0x0

/* Load Kernel Args */
s_load_dwordx16 s[24:39], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x0
s_load_dwordx4 s[40:43], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x40
s_load_dwordx2 s[44:45], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x50
s_waitcnt lgkmcnt(0)
s_branch label_LoadArgsEnd
label_HBMArgs:

/* Load address of kernel arguments */
s_load_dwordx2 s[sgprKernArgAddress:sgprKernArgAddress+1], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x10
s_waitcnt lgkmcnt(0)                               // wait for args to load
label_LoadArgsEnd:
s_branch label_common_kernel_entry

/* pad 39 snops to satisfy 0x100 code size for Preload Backward Compatibility Prologue */
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
s_nop 0
label_Preload_Offset_Start:
s_and_b32 s47, 0x3fffffff, s2                      // Get nums of gemm
s_lshr_b32 s48, s2, 0x1e                           // Get arg type
s_mov_b32 s[sgprGSU], s3                           // Preload internal args
s_cmp_eq_u32 s48, 0                                // Is kernel args
s_cbranch_scc0 label_Preload_HBMArgs
s_add_u32 s[sgprKernArgAddress], s[sgprKernArgAddress], 0x10 // Shift common args
s_addc_u32 s[sgprKernArgAddress+1], s[sgprKernArgAddress+1], 0x0

/* Load Kernel Args */
s_load_dword s31, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x1c
s_load_dwordx8 s[32:39], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x20
s_load_dwordx4 s[40:43], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x40
s_load_dwordx2 s[44:45], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x50
s_mov_b32 s24, s6                                  // move preload data to correct sgpr
s_mov_b32 s25, s7                                  // move preload data to correct sgpr
s_mov_b32 s26, s8                                  // move preload data to correct sgpr
s_mov_b32 s27, s9                                  // move preload data to correct sgpr
s_mov_b32 s28, s10                                 // move preload data to correct sgpr
s_mov_b32 s29, s11                                 // move preload data to correct sgpr
s_mov_b32 s30, s12                                 // move preload data to correct sgpr
s_branch label_Preload_LoadArgsEnd
label_Preload_HBMArgs:
s_mov_b64 s[sgprKernArgAddress:sgprKernArgAddress+1], s[6:7] // Load address of kernel arguments
label_Preload_LoadArgsEnd:
s_mov_b32 s[sgprWGM], s4                           // Preload internal args2
label_common_kernel_entry:  /// for both preload/non-preload common code
s_mov_b32 s[sgprWorkGroup0+0], s13                 // restore workgroup id
s_mov_b32 s[sgprWorkGroup0+1], s14                 // restore workgroup id
s_mov_b32 s[sgprWorkGroup0+2], s15                 // restore workgroup id
s_and_b32 s[sgprStaggerU], s[sgprGSU], 0xffff0000  // Restore StaggerU related vars
s_lshr_b32 s[sgprStaggerU], s[sgprStaggerU], 0x10
s_and_b32 s[sgprGSU], s[sgprGSU], 0xffff           // Restore GSU
s_mov_b32 s[sgprArgType], s48
s_mov_b32 m0, 0x8c00                               // LDS clamp at 35840 bytes
v_mov_b32 v[vgprSerial], v0                        // thread serial id
s_cmp_eq_u32 s48, 0
s_cbranch_scc0 label_MultiGemm
/* init: add vgpr [0...144) to pool */
/* init: add vgpr [0...0) to pool */
/* init: add agpr [0...224) to pool */

/******************************************/
/* Local Read Addresses                   */
/******************************************/

/* local read addresses: tile assignments a/b */
/* lr0I */
v_and_b32 v1, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_and_b32 v0, 15, v1                               // 1. N offset: nIdx = wtid % MI_N(16)
v_lshlrev_b32 v0, 0x6, v0                          // 1. N offset: nOffset = nIdx * nStride(64)
/* Skip. 2. block offset: bnOffset = 0 when num1DBlocks = 1 */
v_lshlrev_b32 v0, 0x2, v0                          // 4. apply VectorWidth: bnOffset = bnOffset * vw(4)
v_and_b32 v1, 63, v[vgprSerial]                    // 5. thread id in wave: wtid = tid % wavelength(64)
v_lshrrev_b32 v1, 4, v1                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
v_lshlrev_b32 v1, 0x3, v1                          // 5. K offset: lrKOffset = kIdx * mStride(8)
v_add_u32 v0, v1, v0                               // 6. offset in wave: lrOffset = bnOffset + lrKOffset
v_lshrrev_b32 v1, 6, v[vgprSerial]                 // 7. wave offset in N dimen: wtid = tid / dividedForWaveId(64)
v_and_b32 v1, 3, v1                                // 7. wave offset in M dimen: wtid0 = wtid / num1DWaves(4)
v_lshlrev_b32 v1, 0xc, v1                          // 7. wave offset in M dimen: wOffset = wtid0 * W0Stride(4096)
v_add_u32 v0, v1, v0                               // 7. final local read offset: flrOffset = lrOffset + WOffset
/* lr1J */
v_and_b32 v2, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_and_b32 v1, 15, v2                               // 1. N offset: nIdx = wtid % MI_N(16)
v_lshlrev_b32 v1, 0x6, v1                          // 1. N offset: nOffset = nIdx * nStride(64)
/* Skip. 2. block offset: bnOffset = 0 when num1DBlocks = 1 */
                                                   // 4. apply VectorWidth: bnOffset = bnOffset * vw(1) (multiplier is 1, do nothing)
v_and_b32 v2, 63, v[vgprSerial]                    // 5. thread id in wave: wtid = tid % wavelength(64)
v_lshrrev_b32 v2, 4, v2                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
v_lshlrev_b32 v2, 0x3, v2                          // 5. K offset: lrKOffset = kIdx * mStride(8)
v_add_u32 v1, v2, v1                               // 6. offset in wave: lrOffset = bnOffset + lrKOffset

/* local read addresses: final offsets a */
v_lshrrev_b32 v2, 6, v[vgprSerial]                 // v2 = v[vgprSerial] / 64
v_lshrrev_b32 v2, 2, v2                            // LSU offset: Get LSU wave_id
s_mov_b32 s49, 64                                  // LSU offset: stride = lsuStride(64) when umlds==True
v_mul_lo_u32 v2, s49, v2                           // LSU offset: lsuoffset = wave_id*lsuStride*(MT0+PAD)
v_add_lshl_u32 v[vgprLocalReadAddrA], v2, v0, 0x1  // Final Offset: offset = (lro0+lsuoffset)*bpeDS
v_lshrrev_b32 v3, 9, v[vgprLocalReadAddrA]         // Final Offset: padding 32 per block 512
v_lshlrev_b32 v3, 0x5, v3                          // Final Offset: padding 32 per block 512
v_add_u32 v[vgprLocalReadAddrA], v3, v[vgprLocalReadAddrA] // Final Offset: add padding 32 per block 512

/* local read addresses: final offsets b */
v_lshrrev_b32 v0, 6, v[vgprSerial]                 // v0 = v[vgprSerial] / 64
v_lshrrev_b32 v0, 2, v0                            // LSU offset: Get LSU wave_id
s_mov_b32 s49, 64                                  // LSU offset: stride = lsuStride(64) when umlds==True
v_mul_lo_u32 v0, s49, v0                           // LSU offset: lsuoffset = wave_id*lsuStride*(MT1+PAD)
v_add_lshl_u32 v[vgprLocalReadAddrB], v0, v1, 0x1  // Final Offset: offset = (lro1+lsuoffset)*bpeDS
v_lshrrev_b32 v2, 7, v[vgprLocalReadAddrB]         // Final Offset: padding 32 per block 128
v_lshlrev_b32 v2, 0x5, v2                          // Final Offset: padding 32 per block 128
v_add_u32 v[vgprLocalReadAddrB], v2, v[vgprLocalReadAddrB] // Final Offset: add padding 32 per block 128

/* local read addresses: declare addresses a */
/* N/A */

/* local read addresses: declare addresses b */

/******************************************/
/* Local Write Addresses                  */
/******************************************/
/* LVCA = 8 */
/* v1 = A-unroll = serial%LVCA */
v_lshrrev_b32 v0, 3, v[vgprSerial]                 // v0 = v[vgprSerial] / 8
v_and_b32 v1, 7, v[vgprSerial]                     // v1 = v[vgprSerial] % 8
/* unroll *= glvw */
v_lshlrev_b32 v1, 0x3, v1                          // v1 = v1 * 8
v_mov_b32 v4, v1                                   // copy for GlobalSplitU
/* LVCB = 8 */
/* v3 = B-unroll = serial%LVCB */
v_and_b32 v5, 63, v[vgprSerial]                    // v5 = v[vgprSerial] % 64
v_lshrrev_b32 v2, 3, v5                            // v2 = v5 / 8
v_and_b32 v3, 7, v5                                // v3 = v5 % 8
v_lshrrev_b32 v5, 0x6, v[vgprSerial]               // WaveID
v_mov_b32 v8, 28                                   // Global Read Wave: add back to cloumn index
v_mul_lo_u32 v2, v8, v2                            // Global Read Wave: add back to cloumn index
v_add_u32 v2, v5, v2                               // Global Read Wave: add back to cloumn index
/* unroll *= glvw */
v_lshlrev_b32 v3, 0x3, v3                          // v3 = v3 * 8
v_mov_b32 v5, v3                                   // copy for GlobalSplitU
/* lwaUnrollAssignmentA = v4 */
/* lwaUnrollAssignmentB = v5 */

/* local write addresses: first offset a */
v_mul_u32_u24 v[vgprLocalWriteAddrA], 0x40, v0     // lwAL**(DepthU_Compute + PAD)
v_add_lshl_u32 v[vgprLocalWriteAddrA], v4, v[vgprLocalWriteAddrA], 0x1 // lwFOA = (lwAA + lwAL*(DepthU+PAD))*bpeDS
v_lshrrev_b32 v6, 9, v[vgprLocalWriteAddrA]        // padding 32 per block 512
v_lshlrev_b32 v6, 0x5, v6                          // padding 32 per block 512
v_add_u32 v[vgprLocalWriteAddrA], v6, v[vgprLocalWriteAddrA] // add padding 32 per block 512

/* local write addresses: first offset b */
v_mul_u32_u24 v[vgprLocalWriteAddrB], 0x40, v2     // lwBL**(DepthU_Compute + PAD)
v_add_lshl_u32 v[vgprLocalWriteAddrB], v5, v[vgprLocalWriteAddrB], 0x1 // lwFOB = (lwBB + lwBL*(DepthU+PAD))*bpeDS
v_lshrrev_b32 v6, 7, v[vgprLocalWriteAddrB]        // padding 32 per block 128
v_lshlrev_b32 v6, 0x5, v6                          // padding 32 per block 128
v_add_u32 v[vgprLocalWriteAddrB], v6, v[vgprLocalWriteAddrB] // add padding 32 per block 128
v_mov_b32 v8, MT0                                  // set MT0 into sgpr
v_mov_b32 v7, s[sgprSizesFree+0]                   // set Free0 size
v_cvt_f32_u32 v6, v8                               // v6 = ceil(v7 / v8)
v_rcp_iflag_f32 v6, v6                             // v6 = ceil(v7 / v8)
v_cvt_f32_u32 v9, v7                               // v6 = ceil(v7 / v8)
v_mul_f32 v6, v6, v9                               // v6 = ceil(v7 / v8)
v_cvt_u32_f32 v6, v6                               // v6 = ceil(v7 / v8)
v_mul_u32_u24 v9, v6, v8                           // v6 = ceil(v7 / v8)
v_sub_u32 v9, v7, v9                               // v6 = ceil(v7 / v8)
v_cmp_ne_u32 vcc, v9, 0                            // v6 = ceil(v7 / v8)
v_addc_co_u32 v6, vcc, v6, 0, vcc                  // ceil
v_mov_b32 v8, MT1                                  // set MT1 into sgpr
v_mov_b32 v7, s[sgprSizesFree+1]                   // set Free1 size
v_readfirstlane_b32 s[sgprNumWorkGroups0], v6      // set back to numWorkGroup0
v_cvt_f32_u32 v6, v8                               // v6 = ceil(v7 / v8)
v_rcp_iflag_f32 v6, v6                             // v6 = ceil(v7 / v8)
v_cvt_f32_u32 v9, v7                               // v6 = ceil(v7 / v8)
v_mul_f32 v6, v6, v9                               // v6 = ceil(v7 / v8)
v_cvt_u32_f32 v6, v6                               // v6 = ceil(v7 / v8)
v_mul_u32_u24 v9, v6, v8                           // v6 = ceil(v7 / v8)
v_sub_u32 v9, v7, v9                               // v6 = ceil(v7 / v8)
v_cmp_ne_u32 vcc, v9, 0                            // v6 = ceil(v7 / v8)
v_addc_co_u32 v6, vcc, v6, 0, vcc                  // ceil
s_nop 0                                            // 1 wait states
v_readfirstlane_b32 s[sgprNumWorkGroups1], v6      // set back to numWorkGroup1
s_waitcnt lgkmcnt(0)                               // wait for 44/0 bytes of kern args

/* remap wg from 1D(idxWG012) to 3D(wg2,wg1,wg0) */
/* wg2 = idxWG012 * smallMagicNumber(1/(numWG0*numWG1)) */
s_mul_i32 s48, s[sgprNumWorkGroups0], s[sgprNumWorkGroups1]
s_mul_i32 s48, s48, s[sgprGSU]
v_cvt_f32_u32 v6, s48                              // s48 = s[sgprWorkGroup0] / s48
v_rcp_iflag_f32 v6, v6                             // s48 = s[sgprWorkGroup0] / s48
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // s48 = s[sgprWorkGroup0] / s48
v_mul_f32 v6, v6, v7                               // s48 = s[sgprWorkGroup0] / s48
v_cvt_u32_f32 v6, v6                               // s48 = s[sgprWorkGroup0] / s48
v_mul_u32_u24 v7, v6, s48                          // s48 = s[sgprWorkGroup0] / s48
v_sub_u32 v7, s[sgprWorkGroup0], v7                // s48 = s[sgprWorkGroup0] / s48
v_cmpx_eq_u32 exec, v7, s48                        // s48 = s[sgprWorkGroup0] / s48
v_add_u32 v6, 1, v6                                // s48 = s[sgprWorkGroup0] / s48
s_mov_b64 exec, -1                                 // s48 = s[sgprWorkGroup0] / s48
v_readfirstlane_b32 s48, v6                        // quotient
s_mov_b32 s[sgprWorkGroup2], s48
/* idxWG01 = idxWG012 - wg2 * numWG0 * numWG1 */
s_mul_i32 s48, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s48, s48, s[sgprWorkGroup2]
s_mul_i32 s48, s48, s[sgprGSU]
s_sub_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s48
/* wg1 = idxWG01 * smallMagicNumber(1/numWG0) */
v_cvt_f32_u32 v6, s[sgprNumWorkGroups0]            // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_rcp_iflag_f32 v6, v6                             // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_mul_f32 v6, v6, v7                               // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cvt_u32_f32 v6, v6                               // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_mul_u32_u24 v7, v6, s[sgprNumWorkGroups0]        // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_sub_u32 v7, s[sgprWorkGroup0], v7                // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cmpx_eq_u32 exec, v7, s[sgprNumWorkGroups0]      // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_add_u32 v6, 1, v6                                // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
s_mov_b64 exec, -1                                 // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_readfirstlane_b32 s48, v6                        // quotient
s_mov_b32 s[sgprWorkGroup1], s48
/* wg0 = idxWG01 - wg1 * numWG0 */
s_mul_i32 s48, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_sub_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s48
s_branch label_MultiGemmEnd
label_MultiGemm:

/* Check if custom structure pointer is null */
s_cmp_eq_u32 s[sgprArgType], 2                     // ArgType == 2 ?
s_cbranch_scc1 label_IsExternalValid               // branch if ArgType == 2
s_mov_b32 s15, 124
s_mul_i32 s54, s47, 4
s_mov_b64 s[48:49], s[sgprKernArgAddress:sgprKernArgAddress+1]
s_branch label_IsExternalValidEnd
label_IsExternalValid:
s_mov_b32 s15, 196
s_mov_b32 s54, 0x0
s_mov_b64 s[48:49], s[sgprKernArgAddress:sgprKernArgAddress+1]
label_IsExternalValidEnd:

/* Grouped Gemm:: prefetch 1 arg load */
s_mov_b32 s14, 1
s_mov_b32 s55, 0
s_load_dwordx4 s[24:27], s[48:49], s54
s_cmpk_eq_u32 s47, 1                               // if gemm_count is 1?
s_cbranch_scc1 label_wgTable_noLoadLoop

/* Grouped Gemm:: accumulate numTiles for each gemm */
/* Grouped Gemm:: loop start */
label_Loop_GemmCount:
s_waitcnt lgkmcnt(0)
s_lshr_b32 s52, s24, 8                             // s52 = s24 / 256
s_and_b32 s50, 255, s24                            // s50 = s24 % 256
s_addc_u32 s52, s52, 0x0
s_mov_b32 s51, 0x0                                 // STATIC_DIV: divisior=224
s_mul_i32 s50, 0x249, s25                          // tmp1 = dividend * magic hi
s_lshl_b64 s[50:51], s[50:51], 0x10                // left shift 16 bits
s_mul_i32 s53, s25, 0x2493                         // tmp0 = dividend * magic lo
s_add_u32 s50, s53, s50                            // add lo
s_addc_u32 s51, s51, 0x0                           // add hi
s_lshr_b64 s[50:51], s[50:51], 0x21                // tmp0 = quotient
s_mul_i32 s51, s50, 0xe0                           // tmp1 = quotient * divisor
s_cmp_lg_u32 s51, s25                              // if (quotient * divisor != dividend), result+=1
s_addc_u32 s53, s50, 0x0                           // if (quotient * divisor != dividend), result+=1
s_mul_i32 s52, s52, s53
s_mul_i32 s52, s52, s26
s_mul_i32 s52, s52, s[sgprGSU]
s_add_u32 s55, s55, s52
s_cmp_lt_u32 s[sgprWorkGroup0], s55
s_cbranch_scc1 label_FOUND
s_add_u32 s54, s54, s15
s_load_dwordx4 s[24:27], s[48:49], s54
s_add_u32 s14, s14, 1
s_cmp_lt_u32 s14, s47
s_cbranch_scc1 label_Loop_GemmCount

/* Grouped Gemm:: noLoadLoop */
label_wgTable_noLoadLoop:
s_waitcnt lgkmcnt(0)
s_lshr_b32 s52, s24, 8                             // s52 = s24 / 256
s_and_b32 s50, 255, s24                            // s50 = s24 % 256
s_addc_u32 s52, s52, 0x0
s_mov_b32 s51, 0x0                                 // STATIC_DIV: divisior=224
s_mul_i32 s50, 0x249, s25                          // tmp1 = dividend * magic hi
s_lshl_b64 s[50:51], s[50:51], 0x10                // left shift 16 bits
s_mul_i32 s53, s25, 0x2493                         // tmp0 = dividend * magic lo
s_add_u32 s50, s53, s50                            // add lo
s_addc_u32 s51, s51, 0x0                           // add hi
s_lshr_b64 s[50:51], s[50:51], 0x21                // tmp0 = quotient
s_mul_i32 s51, s50, 0xe0                           // tmp1 = quotient * divisor
s_cmp_lg_u32 s51, s25                              // if (quotient * divisor != dividend), result+=1
s_addc_u32 s53, s50, 0x0                           // if (quotient * divisor != dividend), result+=1
s_mul_i32 s52, s52, s53
s_mul_i32 s52, s52, s26
s_mul_i32 s52, s52, s[sgprGSU]
s_add_u32 s55, s55, s52

/* Grouped Gemm:: gemmIndex found */
label_FOUND:
s_sub_u32 s49, s14, 1
s_sub_u32 s48, s55, s52
s_sub_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s48
/* Check if custom structure pointer is null */
s_cmp_eq_u32 s[sgprArgType], 2                     // ArgType == 2 ?
s_cbranch_scc1 label_LoadExternalStruct            // branch if ArgType == 2

/* Grouped Gemm: offset argument address to gemm */
/* Grouped Gemm: offset address from wg_table_start to args_start */
s_lshl2_add_u32 s[sgprKernArgAddress], s47, s[sgprKernArgAddress]
s_addc_u32 s[sgprKernArgAddress+1], s[sgprKernArgAddress+1], 0x0
/* Grouped Gemm: offset address from args_start to gemm_start */
s_mul_i32 s49, s49, 124
s_add_u32 s[sgprKernArgAddress], s[sgprKernArgAddress], s49
s_addc_u32 s[sgprKernArgAddress+1], s[sgprKernArgAddress+1], 0x0

/* Load Kernel Args */
s_load_dwordx16 s[28:43], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x10
s_load_dwordx2 s[44:45], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x50
s_branch label_LoadExternalStructEnd
label_LoadExternalStruct:
/* Grouped Gemm: offset address from args_start to gemm_start */
s_mul_i32 s49, s49, 196
s_add_u32 s[sgprKernArgAddress], s[sgprKernArgAddress], s49
s_addc_u32 s[sgprKernArgAddress+1], s[sgprKernArgAddress+1], 0x0
s_load_dwordx16 s[28:43], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x10
s_load_dword s44, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x50
// Read Beta
s_load_dword s45, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x60
label_LoadExternalStructEnd:
/* init: add vgpr [0...144) to pool */
/* init: add vgpr [0...0) to pool */
/* init: add agpr [0...224) to pool */

/******************************************/
/* Local Read Addresses                   */
/******************************************/

/* local read addresses: tile assignments a/b */
/* lr0I */
v_and_b32 v1, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_and_b32 v0, 15, v1                               // 1. N offset: nIdx = wtid % MI_N(16)
v_lshlrev_b32 v0, 0x6, v0                          // 1. N offset: nOffset = nIdx * nStride(64)
/* Skip. 2. block offset: bnOffset = 0 when num1DBlocks = 1 */
v_lshlrev_b32 v0, 0x2, v0                          // 4. apply VectorWidth: bnOffset = bnOffset * vw(4)
v_and_b32 v1, 63, v[vgprSerial]                    // 5. thread id in wave: wtid = tid % wavelength(64)
v_lshrrev_b32 v1, 4, v1                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
v_lshlrev_b32 v1, 0x3, v1                          // 5. K offset: lrKOffset = kIdx * mStride(8)
v_add_u32 v0, v1, v0                               // 6. offset in wave: lrOffset = bnOffset + lrKOffset
v_lshrrev_b32 v1, 6, v[vgprSerial]                 // 7. wave offset in N dimen: wtid = tid / dividedForWaveId(64)
v_and_b32 v1, 3, v1                                // 7. wave offset in M dimen: wtid0 = wtid / num1DWaves(4)
v_lshlrev_b32 v1, 0xc, v1                          // 7. wave offset in M dimen: wOffset = wtid0 * W0Stride(4096)
v_add_u32 v0, v1, v0                               // 7. final local read offset: flrOffset = lrOffset + WOffset
/* lr1J */
v_and_b32 v2, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_and_b32 v1, 15, v2                               // 1. N offset: nIdx = wtid % MI_N(16)
v_lshlrev_b32 v1, 0x6, v1                          // 1. N offset: nOffset = nIdx * nStride(64)
/* Skip. 2. block offset: bnOffset = 0 when num1DBlocks = 1 */
                                                   // 4. apply VectorWidth: bnOffset = bnOffset * vw(1) (multiplier is 1, do nothing)
v_and_b32 v2, 63, v[vgprSerial]                    // 5. thread id in wave: wtid = tid % wavelength(64)
v_lshrrev_b32 v2, 4, v2                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
v_lshlrev_b32 v2, 0x3, v2                          // 5. K offset: lrKOffset = kIdx * mStride(8)
v_add_u32 v1, v2, v1                               // 6. offset in wave: lrOffset = bnOffset + lrKOffset

/* local read addresses: final offsets a */
v_lshrrev_b32 v2, 6, v[vgprSerial]                 // v2 = v[vgprSerial] / 64
v_lshrrev_b32 v2, 2, v2                            // LSU offset: Get LSU wave_id
s_mov_b32 s49, 64                                  // LSU offset: stride = lsuStride(64) when umlds==True
v_mul_lo_u32 v2, s49, v2                           // LSU offset: lsuoffset = wave_id*lsuStride*(MT0+PAD)
v_add_lshl_u32 v[vgprLocalReadAddrA], v2, v0, 0x1  // Final Offset: offset = (lro0+lsuoffset)*bpeDS
v_lshrrev_b32 v3, 9, v[vgprLocalReadAddrA]         // Final Offset: padding 32 per block 512
v_lshlrev_b32 v3, 0x5, v3                          // Final Offset: padding 32 per block 512
v_add_u32 v[vgprLocalReadAddrA], v3, v[vgprLocalReadAddrA] // Final Offset: add padding 32 per block 512

/* local read addresses: final offsets b */
v_lshrrev_b32 v0, 6, v[vgprSerial]                 // v0 = v[vgprSerial] / 64
v_lshrrev_b32 v0, 2, v0                            // LSU offset: Get LSU wave_id
s_mov_b32 s49, 64                                  // LSU offset: stride = lsuStride(64) when umlds==True
v_mul_lo_u32 v0, s49, v0                           // LSU offset: lsuoffset = wave_id*lsuStride*(MT1+PAD)
v_add_lshl_u32 v[vgprLocalReadAddrB], v0, v1, 0x1  // Final Offset: offset = (lro1+lsuoffset)*bpeDS
v_lshrrev_b32 v2, 7, v[vgprLocalReadAddrB]         // Final Offset: padding 32 per block 128
v_lshlrev_b32 v2, 0x5, v2                          // Final Offset: padding 32 per block 128
v_add_u32 v[vgprLocalReadAddrB], v2, v[vgprLocalReadAddrB] // Final Offset: add padding 32 per block 128

/* local read addresses: declare addresses a */
/* N/A */

/* local read addresses: declare addresses b */

/******************************************/
/* Local Write Addresses                  */
/******************************************/
/* LVCA = 8 */
/* v1 = A-unroll = serial%LVCA */
v_lshrrev_b32 v0, 3, v[vgprSerial]                 // v0 = v[vgprSerial] / 8
v_and_b32 v1, 7, v[vgprSerial]                     // v1 = v[vgprSerial] % 8
/* unroll *= glvw */
v_lshlrev_b32 v1, 0x3, v1                          // v1 = v1 * 8
v_mov_b32 v4, v1                                   // copy for GlobalSplitU
/* LVCB = 8 */
/* v3 = B-unroll = serial%LVCB */
v_and_b32 v5, 63, v[vgprSerial]                    // v5 = v[vgprSerial] % 64
v_lshrrev_b32 v2, 3, v5                            // v2 = v5 / 8
v_and_b32 v3, 7, v5                                // v3 = v5 % 8
v_lshrrev_b32 v5, 0x6, v[vgprSerial]               // WaveID
v_mov_b32 v8, 28                                   // Global Read Wave: add back to cloumn index
v_mul_lo_u32 v2, v8, v2                            // Global Read Wave: add back to cloumn index
v_add_u32 v2, v5, v2                               // Global Read Wave: add back to cloumn index
/* unroll *= glvw */
v_lshlrev_b32 v3, 0x3, v3                          // v3 = v3 * 8
v_mov_b32 v5, v3                                   // copy for GlobalSplitU
/* lwaUnrollAssignmentA = v4 */
/* lwaUnrollAssignmentB = v5 */

/* local write addresses: first offset a */
v_mul_u32_u24 v[vgprLocalWriteAddrA], 0x40, v0     // lwAL**(DepthU_Compute + PAD)
v_add_lshl_u32 v[vgprLocalWriteAddrA], v4, v[vgprLocalWriteAddrA], 0x1 // lwFOA = (lwAA + lwAL*(DepthU+PAD))*bpeDS
v_lshrrev_b32 v6, 9, v[vgprLocalWriteAddrA]        // padding 32 per block 512
v_lshlrev_b32 v6, 0x5, v6                          // padding 32 per block 512
v_add_u32 v[vgprLocalWriteAddrA], v6, v[vgprLocalWriteAddrA] // add padding 32 per block 512

/* local write addresses: first offset b */
v_mul_u32_u24 v[vgprLocalWriteAddrB], 0x40, v2     // lwBL**(DepthU_Compute + PAD)
v_add_lshl_u32 v[vgprLocalWriteAddrB], v5, v[vgprLocalWriteAddrB], 0x1 // lwFOB = (lwBB + lwBL*(DepthU+PAD))*bpeDS
v_lshrrev_b32 v6, 7, v[vgprLocalWriteAddrB]        // padding 32 per block 128
v_lshlrev_b32 v6, 0x5, v6                          // padding 32 per block 128
v_add_u32 v[vgprLocalWriteAddrB], v6, v[vgprLocalWriteAddrB] // add padding 32 per block 128
v_mov_b32 v8, MT0                                  // set MT0 into sgpr
v_mov_b32 v7, s[sgprSizesFree+0]                   // set Free0 size
v_cvt_f32_u32 v6, v8                               // v6 = ceil(v7 / v8)
v_rcp_iflag_f32 v6, v6                             // v6 = ceil(v7 / v8)
v_cvt_f32_u32 v9, v7                               // v6 = ceil(v7 / v8)
v_mul_f32 v6, v6, v9                               // v6 = ceil(v7 / v8)
v_cvt_u32_f32 v6, v6                               // v6 = ceil(v7 / v8)
v_mul_u32_u24 v9, v6, v8                           // v6 = ceil(v7 / v8)
v_sub_u32 v9, v7, v9                               // v6 = ceil(v7 / v8)
v_cmp_ne_u32 vcc, v9, 0                            // v6 = ceil(v7 / v8)
v_addc_co_u32 v6, vcc, v6, 0, vcc                  // ceil
v_mov_b32 v8, MT1                                  // set MT1 into sgpr
v_mov_b32 v7, s[sgprSizesFree+1]                   // set Free1 size
v_readfirstlane_b32 s[sgprNumWorkGroups0], v6      // set back to numWorkGroup0
v_cvt_f32_u32 v6, v8                               // v6 = ceil(v7 / v8)
v_rcp_iflag_f32 v6, v6                             // v6 = ceil(v7 / v8)
v_cvt_f32_u32 v9, v7                               // v6 = ceil(v7 / v8)
v_mul_f32 v6, v6, v9                               // v6 = ceil(v7 / v8)
v_cvt_u32_f32 v6, v6                               // v6 = ceil(v7 / v8)
v_mul_u32_u24 v9, v6, v8                           // v6 = ceil(v7 / v8)
v_sub_u32 v9, v7, v9                               // v6 = ceil(v7 / v8)
v_cmp_ne_u32 vcc, v9, 0                            // v6 = ceil(v7 / v8)
v_addc_co_u32 v6, vcc, v6, 0, vcc                  // ceil
s_nop 0                                            // 1 wait states
v_readfirstlane_b32 s[sgprNumWorkGroups1], v6      // set back to numWorkGroup1
s_waitcnt lgkmcnt(0)                               // wait for 44/0 bytes of kern args

/* Early stop if N(SizeFreeJ) == 0 */
s_cmp_eq_u32 s[sgprSizeJ], 0x0
s_cbranch_scc0 label_NoEarlyStop_N0
label_EarlyStop_if_N_is_0:
s_endpgm
label_NoEarlyStop_N0:

/* remap wg from 1D(idxWG012) to 3D(wg2,wg1,wg0) */
/* wg2 = idxWG012 * smallMagicNumber(1/(numWG0*numWG1)) */
s_mul_i32 s48, s[sgprNumWorkGroups0], s[sgprNumWorkGroups1]
s_mul_i32 s48, s48, s[sgprGSU]
v_cvt_f32_u32 v6, s48                              // s48 = s[sgprWorkGroup0] / s48
v_rcp_iflag_f32 v6, v6                             // s48 = s[sgprWorkGroup0] / s48
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // s48 = s[sgprWorkGroup0] / s48
v_mul_f32 v6, v6, v7                               // s48 = s[sgprWorkGroup0] / s48
v_cvt_u32_f32 v6, v6                               // s48 = s[sgprWorkGroup0] / s48
v_mul_u32_u24 v7, v6, s48                          // s48 = s[sgprWorkGroup0] / s48
v_sub_u32 v7, s[sgprWorkGroup0], v7                // s48 = s[sgprWorkGroup0] / s48
v_cmpx_eq_u32 exec, v7, s48                        // s48 = s[sgprWorkGroup0] / s48
v_add_u32 v6, 1, v6                                // s48 = s[sgprWorkGroup0] / s48
s_mov_b64 exec, -1                                 // s48 = s[sgprWorkGroup0] / s48
v_readfirstlane_b32 s48, v6                        // quotient
s_mov_b32 s[sgprWorkGroup2], s48
/* idxWG01 = idxWG012 - wg2 * numWG0 * numWG1 */
s_mul_i32 s48, s[sgprNumWorkGroups1], s[sgprNumWorkGroups0]
s_mul_i32 s48, s48, s[sgprWorkGroup2]
s_mul_i32 s48, s48, s[sgprGSU]
s_sub_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s48
/* wg1 = idxWG01 * smallMagicNumber(1/numWG0) */
v_cvt_f32_u32 v6, s[sgprNumWorkGroups0]            // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_rcp_iflag_f32 v6, v6                             // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_mul_f32 v6, v6, v7                               // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cvt_u32_f32 v6, v6                               // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_mul_u32_u24 v7, v6, s[sgprNumWorkGroups0]        // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_sub_u32 v7, s[sgprWorkGroup0], v7                // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_cmpx_eq_u32 exec, v7, s[sgprNumWorkGroups0]      // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_add_u32 v6, 1, v6                                // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
s_mov_b64 exec, -1                                 // s48 = s[sgprWorkGroup0] / s[sgprNumWorkGroups0]
v_readfirstlane_b32 s48, v6                        // quotient
s_mov_b32 s[sgprWorkGroup1], s48
/* wg0 = idxWG01 - wg1 * numWG0 */
s_mul_i32 s48, s[sgprWorkGroup1], s[sgprNumWorkGroups0]
s_sub_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s48

/* Early stop if wg exceed */
s_cmp_ge_u32 s[sgprWorkGroup2], s[sgprSizesFree+2]
s_cbranch_scc0 label_NoEarlyStop_wgExceed
label_EarlyStop_if_wg_exceed:
s_endpgm
label_NoEarlyStop_wgExceed:

label_MultiGemmEnd:
.set sgprSrdA, 48
.set sgprSrdB, 52
.set sgprShadowLimitA, 56
.set sgprShadowLimitB, 58
.set sgprStaggerUIter, 47
.set sgprWrapUA, 60
.set sgprWrapUB, 62
.set sgprGlobalReadIncsA, 64
.set sgprGlobalReadIncsB, 65
.set sgprScalarGlobalReadOffsetA, 66
.set sgprScalarGlobalReadOffsetB, 73
s_sub_u32 s[sgprAddressA+0], s[sgprAddressA+0], 16 // pre-pad to make room for possible pointer shift
s_subb_u32 s[sgprAddressA+1], s[sgprAddressA+1], 0 // pre-pad to make room for possible pointer shift
s_sub_u32 s[sgprAddressB+0], s[sgprAddressB+0], 16 // pre-pad to make room for possible pointer shift
s_subb_u32 s[sgprAddressB+1], s[sgprAddressB+1], 0 // pre-pad to make room for possible pointer shift

/* Short circuit condition if Alpha == 0, then sumDims=0 */
v_cmp_eq_f32 vcc, s[sgprAlpha], 0.0                // s[Alpha] == 0.0f ?
s_cbranch_vccz label_AlphaNonZero                  // branch if s[Alpha] != 0
s_mov_b32 s[sgprSizesSum+0], 0x0                   // Set summation dim=0 if Alpha == 0
label_AlphaNonZero:

/******************************************/
/* Begin setupNewTile                     */
/******************************************/

/* global read addresses: work-group */
/* graWorkGroup mapping */
s_cmp_eq_u32 s[sgprGSU], 1                         // GSU == 1 ?
s_cbranch_scc1 label_GSU                           // branch if GSU == 1
// GSU-not-WGMapRR :nwg1 = (size1J + MT1J - 1) / MT1J;
v_cvt_f32_u32 v6, s[sgprGSU]                       // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_rcp_iflag_f32 v6, v6                             // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_cvt_f32_u32 v7, s[sgprWorkGroup1]                // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_mul_f32 v6, v6, v7                               // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_cvt_u32_f32 v6, v6                               // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_mul_u32_u24 v7, v6, s[sgprGSU]                   // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_sub_u32 v7, s[sgprWorkGroup1], v7                // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_cmpx_eq_u32 exec, v7, s[sgprGSU]                 // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_add_u32 v6, 1, v6                                // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_mov_b32 v7, 0                                    // s[sgprGSUSumIdx] = s[sgprWorkGroup1] % s[sgprGSU]
s_mov_b64 exec, -1                                 // s[sgprWorkGroup1] = s[sgprWorkGroup1] / s[sgprGSU]
v_readfirstlane_b32 s[sgprWorkGroup1], v6          // quotient
v_readfirstlane_b32 s[sgprGSUSumIdx], v7           // remainder
s_mov_b32 s[sgprGSULog2BpeC], 1
s_mov_b32 s[sgprGSULog2BpeD], 2
s_branch label_GSU_End
label_GSU:
s_mov_b64 s[sgprGSUSumIdx:sgprGSUSumIdx+1], 0      // Set GSUSumIdx to 0
s_mov_b32 s[sgprGSULog2BpeC], 1
s_mov_b32 s[sgprGSULog2BpeD], 1
label_GSU_End:
s_cmp_gt_i32 s[sgprWGM], 1                         // WGM > 1 ?
s_cbranch_scc1 label_WGMPositive                   // branch if WGM > 1
s_abs_i32 s[sgprWGM], s[sgprWGM]                   // abs(WGM)
s_cmp_le_i32 s[sgprWGM], 1                         // WGM <= 1 ?
s_cbranch_scc1 label_WGM                           // branch if WGM <= 1
v_cvt_f32_u32 v6, s[sgprWGM]                       // WGM
v_rcp_iflag_f32 v6, v6                             // WGM
v_cvt_f32_u32 v7, s[sgprWorkGroup0]                // WGM
v_mul_f32 v6, v6, v7                               // WGM
v_cvt_u32_f32 v6, v6                               // WGM
v_mul_u32_u24 v7, v6, s[sgprWGM]                   // WGM
v_sub_u32 v7, s[sgprWorkGroup0], v7                // WGM
v_cmpx_eq_u32 exec, v7, s[sgprWGM]                 // WGM
v_add_u32 v6, 1, v6                                // WGM
s_mov_b64 exec, -1                                 // WGM
v_readfirstlane_b32 s79, v6                        // quotient
s_mul_i32 s82, s79, s[sgprWGM]                     // quotient * non-magic divisor
s_sub_u32 s82, s[sgprWorkGroup0], s82              // WorkGroup0=remainder
s_mul_i32 s82, s82, s[sgprNumWorkGroups1]          // (wg1 % WGM)*NumWorkGroups1
s_add_u32 s82, s82, s[sgprWorkGroup1]              // wgSerial = wg0 + (wg1 % WGM)*NumWorkGroups1
v_cvt_f32_u32 v6, s[sgprWGM]                       // WGM
v_rcp_iflag_f32 v6, v6                             // WGM
v_cvt_f32_u32 v7, s[sgprNumWorkGroups0]            // WGM
v_mul_f32 v6, v6, v7                               // WGM
v_cvt_u32_f32 v6, v6                               // WGM
v_mul_u32_u24 v7, v6, s[sgprWGM]                   // WGM
v_sub_u32 v7, s[sgprNumWorkGroups0], v7            // WGM
v_cmpx_eq_u32 exec, v7, s[sgprWGM]                 // WGM
v_add_u32 v6, 1, v6                                // WGM
s_mov_b64 exec, -1                                 // WGM
v_readfirstlane_b32 s80, v6                        // quotient
s_mul_i32 s81, s[sgprWGM], s80                     // quotient * non-magic divisor
s_sub_u32 s81, s[sgprNumWorkGroups0], s81          // NumWorkGroups0=remainder
s_cmp_eq_u32 s81, 0                                // remainder == 0 ?
s_cmov_b32 s81, s[sgprWGM]                         // remainder = WGM if remainder == 0
s_cmp_ge_u32 s79, s80                              // blockId >= numFullBlocks ?
s_cselect_b32 s80, s81, s[sgprWGM]
v_cvt_f32_u32 v6, s80                              // s[sgprWorkGroup1] = s82 / s80
v_rcp_iflag_f32 v6, v6                             // s[sgprWorkGroup1] = s82 / s80
v_cvt_f32_u32 v7, s82                              // s[sgprWorkGroup1] = s82 / s80
v_mul_f32 v6, v6, v7                               // s[sgprWorkGroup1] = s82 / s80
v_cvt_u32_f32 v6, v6                               // s[sgprWorkGroup1] = s82 / s80
v_mul_u32_u24 v7, v6, s80                          // s[sgprWorkGroup1] = s82 / s80
v_sub_u32 v7, s82, v7                              // s[sgprWorkGroup1] = s82 / s80
v_cmpx_eq_u32 exec, v7, s80                        // s[sgprWorkGroup1] = s82 / s80
v_add_u32 v6, 1, v6                                // s[sgprWorkGroup1] = s82 / s80
v_mov_b32 v7, 0                                    // s[sgprWorkGroup0] = s82 % s80
s_mov_b64 exec, -1                                 // s[sgprWorkGroup1] = s82 / s80
v_readfirstlane_b32 s[sgprWorkGroup1], v6          // quotient
v_readfirstlane_b32 s[sgprWorkGroup0], v7          // remainder
s_mul_i32 s[sgprWorkGroup0], s[sgprWorkGroup1], s80 // quotient * non-magic divisor
s_sub_u32 s[sgprWorkGroup0], s82, s[sgprWorkGroup0] // WorkGroup0=remainder
s_mul_i32 s79, s79, s[sgprWGM]                     // blockId * WGM
s_add_u32 s[sgprWorkGroup0], s[sgprWorkGroup0], s79 // wg1 += blockId * WGM
s_branch label_WGM
label_WGMPositive:
v_cvt_f32_u32 v6, s[sgprWGM]                       // WGM
v_rcp_iflag_f32 v6, v6                             // WGM
v_cvt_f32_u32 v7, s[sgprWorkGroup1]                // WGM
v_mul_f32 v6, v6, v7                               // WGM
v_cvt_u32_f32 v6, v6                               // WGM
v_mul_u32_u24 v7, v6, s[sgprWGM]                   // WGM
v_sub_u32 v7, s[sgprWorkGroup1], v7                // WGM
v_cmpx_eq_u32 exec, v7, s[sgprWGM]                 // WGM
v_add_u32 v6, 1, v6                                // WGM
s_mov_b64 exec, -1                                 // WGM
v_readfirstlane_b32 s79, v6                        // quotient
s_mul_i32 s82, s79, s[sgprWGM]                     // quotient * non-magic divisor
s_sub_u32 s82, s[sgprWorkGroup1], s82              // WorkGroup1=remainder
s_mul_i32 s82, s82, s[sgprNumWorkGroups0]          // (wg1 % WGM)*NumWorkGroups0
s_add_u32 s82, s82, s[sgprWorkGroup0]              // wgSerial = wg0 + (wg1 % WGM)*NumWorkGroups0
v_cvt_f32_u32 v6, s[sgprWGM]                       // WGM
v_rcp_iflag_f32 v6, v6                             // WGM
v_cvt_f32_u32 v7, s[sgprNumWorkGroups1]            // WGM
v_mul_f32 v6, v6, v7                               // WGM
v_cvt_u32_f32 v6, v6                               // WGM
v_mul_u32_u24 v7, v6, s[sgprWGM]                   // WGM
v_sub_u32 v7, s[sgprNumWorkGroups1], v7            // WGM
v_cmpx_eq_u32 exec, v7, s[sgprWGM]                 // WGM
v_add_u32 v6, 1, v6                                // WGM
s_mov_b64 exec, -1                                 // WGM
v_readfirstlane_b32 s80, v6                        // quotient
s_mul_i32 s81, s[sgprWGM], s80                     // quotient * non-magic divisor
s_sub_u32 s81, s[sgprNumWorkGroups1], s81          // NumWorkGroups1=remainder
s_cmp_eq_u32 s81, 0                                // remainder == 0 ?
s_cmov_b32 s81, s[sgprWGM]                         // remainder = WGM if remainder == 0
s_cmp_ge_u32 s79, s80                              // blockId >= numFullBlocks ?
s_cselect_b32 s80, s81, s[sgprWGM]
v_cvt_f32_u32 v6, s80                              // s[sgprWorkGroup0] = s82 / s80
v_rcp_iflag_f32 v6, v6                             // s[sgprWorkGroup0] = s82 / s80
v_cvt_f32_u32 v7, s82                              // s[sgprWorkGroup0] = s82 / s80
v_mul_f32 v6, v6, v7                               // s[sgprWorkGroup0] = s82 / s80
v_cvt_u32_f32 v6, v6                               // s[sgprWorkGroup0] = s82 / s80
v_mul_u32_u24 v7, v6, s80                          // s[sgprWorkGroup0] = s82 / s80
v_sub_u32 v7, s82, v7                              // s[sgprWorkGroup0] = s82 / s80
v_cmpx_eq_u32 exec, v7, s80                        // s[sgprWorkGroup0] = s82 / s80
v_add_u32 v6, 1, v6                                // s[sgprWorkGroup0] = s82 / s80
v_mov_b32 v7, 0                                    // s[sgprWorkGroup1] = s82 % s80
s_mov_b64 exec, -1                                 // s[sgprWorkGroup0] = s82 / s80
v_readfirstlane_b32 s[sgprWorkGroup0], v6          // quotient
v_readfirstlane_b32 s[sgprWorkGroup1], v7          // remainder
s_mul_i32 s[sgprWorkGroup1], s[sgprWorkGroup0], s80 // quotient * non-magic divisor
s_sub_u32 s[sgprWorkGroup1], s82, s[sgprWorkGroup1] // WorkGroup1=remainder
s_mul_i32 s79, s79, s[sgprWGM]                     // blockId * WGM
s_add_u32 s[sgprWorkGroup1], s[sgprWorkGroup1], s79 // wg1 += blockId * WGM
label_WGM:

/* global read addresses: tile offset assignment a */
/* graTileAssignmentA = v0 */

/* global read addresses: tile offset assignment b */
/* graTileAssignmentB = v2 */

/* global read addresses: unroll assignment a */
/* v1 */

/* global read addresses: unroll assignment b */
/* v3 */

/* global read addresses: other free assignments */
/* s[sgprWorkGroup2] */

/* global read addresses: tile offsets a */

/* global read addresses: tile offsets b */

/* global read addresses: unroll offsets a */

/* global read addresses: unroll offsets b */

/* global read addresses: final offsets a */
GLOBAL_OFFSET_A vgprGlobalReadOffsetA+0, 6
s_mul_i32 s[sgprScalarGlobalReadOffsetA+0], s[sgprStrideA0I], 1 // compute offset diff (scaled tileDim)
s_lshl_b32 s[sgprScalarGlobalReadOffsetA+0], s[sgprScalarGlobalReadOffsetA+0], 0x1 // scalar offset *= bytes/element
s_mul_i32 s[sgprScalarGlobalReadOffsetA+1], s[sgprStrideA0I], 2 // compute offset diff (scaled tileDim)
s_lshl_b32 s[sgprScalarGlobalReadOffsetA+1], s[sgprScalarGlobalReadOffsetA+1], 0x1 // scalar offset *= bytes/element
s_mul_i32 s[sgprScalarGlobalReadOffsetA+2], s[sgprStrideA0I], 3 // compute offset diff (scaled tileDim)
s_lshl_b32 s[sgprScalarGlobalReadOffsetA+2], s[sgprScalarGlobalReadOffsetA+2], 0x1 // scalar offset *= bytes/element
s_mul_i32 s[sgprScalarGlobalReadOffsetA+3], 1, 32 // compute offset diff (scaled tileDim)
s_lshl_b32 s[sgprScalarGlobalReadOffsetA+3], s[sgprScalarGlobalReadOffsetA+3], 0x1 // scalar offset *= bytes/element
s_add_u32 s[sgprScalarGlobalReadOffsetA+4], s[sgprScalarGlobalReadOffsetA+0], s[sgprScalarGlobalReadOffsetA+3]
s_add_u32 s[sgprScalarGlobalReadOffsetA+5], s[sgprScalarGlobalReadOffsetA+1], s[sgprScalarGlobalReadOffsetA+3]
s_add_u32 s[sgprScalarGlobalReadOffsetA+6], s[sgprScalarGlobalReadOffsetA+2], s[sgprScalarGlobalReadOffsetA+3]

/* global read addresses: final offsets b */
GLOBAL_OFFSET_B vgprGlobalReadOffsetB+0,  3,  2, 6 // gROB_0_0_0_0
s_mul_i32 s[sgprScalarGlobalReadOffsetB+0], s[sgprStrideB1J], 4 // compute offset diff (scaled tileDim)
s_lshl_b32 s[sgprScalarGlobalReadOffsetB+0], s[sgprScalarGlobalReadOffsetB+0], 0x1 // scalar offset *= bytes/element
s_mul_i32 s[sgprScalarGlobalReadOffsetB+1], s[sgprStrideB1J], 8 // compute offset diff (scaled tileDim)
s_lshl_b32 s[sgprScalarGlobalReadOffsetB+1], s[sgprScalarGlobalReadOffsetB+1], 0x1 // scalar offset *= bytes/element
s_mul_i32 s[sgprScalarGlobalReadOffsetB+2], s[sgprStrideB1J], 12 // compute offset diff (scaled tileDim)
s_lshl_b32 s[sgprScalarGlobalReadOffsetB+2], s[sgprScalarGlobalReadOffsetB+2], 0x1 // scalar offset *= bytes/element
s_mul_i32 s[sgprScalarGlobalReadOffsetB+3], s[sgprStrideB1J], 16 // compute offset diff (scaled tileDim)
s_lshl_b32 s[sgprScalarGlobalReadOffsetB+3], s[sgprScalarGlobalReadOffsetB+3], 0x1 // scalar offset *= bytes/element
s_mul_i32 s[sgprScalarGlobalReadOffsetB+4], s[sgprStrideB1J], 20 // compute offset diff (scaled tileDim)
s_lshl_b32 s[sgprScalarGlobalReadOffsetB+4], s[sgprScalarGlobalReadOffsetB+4], 0x1 // scalar offset *= bytes/element
s_mul_i32 s[sgprScalarGlobalReadOffsetB+5], s[sgprStrideB1J], 24 // compute offset diff (scaled tileDim)
s_lshl_b32 s[sgprScalarGlobalReadOffsetB+5], s[sgprScalarGlobalReadOffsetB+5], 0x1 // scalar offset *= bytes/element

/* global read addresses: addresses a */
/* max read offset = size[n] * stride[n-1] */
s_mul_hi_u32 s83, s[sgprWorkGroup0], 256           // WorkGroup[01] * MT
s_mul_i32 s82, s[sgprWorkGroup0], 256              // WorkGroup[01] * MT
s_mul_hi_u32 s83, s82, s[sgprStrideA0I]            // tlu=0, scaled tile-offset by stride
s_mul_i32 s82, s82, s[sgprStrideA0I]               // tlu=0, scaled tile-offset by stride
s_mul_hi_u32 s81, 64, s[sgprGSUSumIdx]             // gsuOffset = DepthU*GSUSumIdx
s_mul_i32 s80, 64, s[sgprGSUSumIdx]                // gsuOffset = DepthU*GSUSumIdx
s_add_u32 s82, s82, s80                            // accum GsuOffset term to tilestart
s_addc_u32 s83, s83, s81                           // accum GsuOffset term to tilestart
s_mov_b32 s[sgprShadowLimitA+0], 1                 // Init tensor size
s_mov_b32 s[sgprShadowLimitA+1], 0                 // init tensor size
s_sub_u32 s80, s[sgprSizeL], 1                     // (size-1)
s_mul_hi_u32 s81, constStrideAL, s80               // stride x (size-1)
s_mul_i32 s80, constStrideAL, s80                  // stride x (size-1)
s_add_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s80 // sum tensor size
s_addc_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s81 // sum tensor size
s_sub_u32 s80, s[sgprSizeI], 1                     // (size-1)
s_mul_hi_u32 s81, s[sgprStrideA0I], s80            // stride x (size-1)
s_mul_i32 s80, s[sgprStrideA0I], s80               // stride x (size-1)
s_add_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s80 // sum tensor size
s_addc_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s81 // sum tensor size
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s82 // sub tileStart
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s83 // sub tileStart
s_lshl_b64 s[sgprShadowLimitA:sgprShadowLimitA+1], s[sgprShadowLimitA:sgprShadowLimitA+1], 0x1 // Set limit to use bytes
s_add_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], 16 // extend limit for pre-pad
s_addc_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], 0 // extend limit for pre-pad
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
s_mul_hi_u32 s81, s[sgprStrideAK], s[sgprWorkGroup2] // Stride*WG
s_mul_i32 s80, s[sgprStrideAK], s[sgprWorkGroup2]  // Stride*WG
s_add_u32 s82, s82, s80                            // accum wg term to tilestart
s_addc_u32 s83, s83, s81                           // accum wg term to tilestart
s_lshl_b64 s[82:83], s[82:83], 0x1                 // tileStart *= BPE
s_add_u32 s[sgprSrdA+0], s[sgprAddressA+0], s82    // SRD base = Address+ tileStart0
s_addc_u32 s[sgprSrdA+1], s[sgprAddressA+1], s83   // SRD base = Address+ tileStart1
s_mov_b32 s[sgprSrdA+3], Srd127_96                 // Set bits 127_96 in SRD

/* global read addresses: addresses b */
/* max read offset = size[n] * stride[n-1] */
s_mul_hi_u32 s83, s[sgprWorkGroup1], 224           // WorkGroup[01] * MT
s_mul_i32 s82, s[sgprWorkGroup1], 224              // WorkGroup[01] * MT
s_mul_hi_u32 s83, s82, s[sgprStrideB1J]            // tlu=0, scaled tile-offset by stride
s_mul_i32 s82, s82, s[sgprStrideB1J]               // tlu=0, scaled tile-offset by stride
s_mul_hi_u32 s81, 64, s[sgprGSUSumIdx]             // gsuOffset = DepthU*GSUSumIdx
s_mul_i32 s80, 64, s[sgprGSUSumIdx]                // gsuOffset = DepthU*GSUSumIdx
s_add_u32 s82, s82, s80                            // accum GsuOffset term to tilestart
s_addc_u32 s83, s83, s81                           // accum GsuOffset term to tilestart
s_mov_b32 s[sgprShadowLimitB+0], 1                 // Init tensor size
s_mov_b32 s[sgprShadowLimitB+1], 0                 // init tensor size
s_sub_u32 s80, s[sgprSizeL], 1                     // (size-1)
s_mul_hi_u32 s81, constStrideBL, s80               // stride x (size-1)
s_mul_i32 s80, constStrideBL, s80                  // stride x (size-1)
s_add_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s80 // sum tensor size
s_addc_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s81 // sum tensor size
s_sub_u32 s80, s[sgprSizeJ], 1                     // (size-1)
s_mul_hi_u32 s81, s[sgprStrideB1J], s80            // stride x (size-1)
s_mul_i32 s80, s[sgprStrideB1J], s80               // stride x (size-1)
s_add_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s80 // sum tensor size
s_addc_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s81 // sum tensor size
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s82 // sub tileStart
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s83 // sub tileStart
s_lshl_b64 s[sgprShadowLimitB:sgprShadowLimitB+1], s[sgprShadowLimitB:sgprShadowLimitB+1], 0x1 // Set limit to use bytes
s_add_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], 16 // extend limit for pre-pad
s_addc_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], 0 // extend limit for pre-pad
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
s_mul_hi_u32 s81, s[sgprStrideBK], s[sgprWorkGroup2] // Stride*WG
s_mul_i32 s80, s[sgprStrideBK], s[sgprWorkGroup2]  // Stride*WG
s_add_u32 s82, s82, s80                            // accum wg term to tilestart
s_addc_u32 s83, s83, s81                           // accum wg term to tilestart
s_lshl_b64 s[82:83], s[82:83], 0x1                 // tileStart *= BPE
s_add_u32 s[sgprSrdB+0], s[sgprAddressB+0], s82    // SRD base = Address+ tileStart0
s_addc_u32 s[sgprSrdB+1], s[sgprAddressB+1], s83   // SRD base = Address+ tileStart1
s_mov_b32 s[sgprSrdB+3], Srd127_96                 // Set bits 127_96 in SRD

/* global read addresses: increments a */
s_mul_i32 s79, s[sgprGSU], DepthU*BpeAGR
s_mov_b32 s[sgprGlobalReadIncsA+0], s79            // incrA (unrollIdx)

/* global read addresses: increments b */
s_mul_i32 s79, s[sgprGSU], DepthU*BpeBGR
s_mov_b32 s[sgprGlobalReadIncsB+0], s79            // incrB (unrollIdx)
/* declare loop num iterations */
s_lshr_b32 s[sgprLoopCounterL], s[sgprSizesSum+0], 6 // s[sgprLoopCounterL] = s[sgprSizesSum+0] / 64
s_cmp_eq_u32 s[sgprGSU], 1                         // GSU == 1 ?
s_cbranch_scc1 label_GSU_1                         // branch if GSU == 1
v_cvt_f32_u32 v0, s[sgprGSU]                       // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_rcp_iflag_f32 v0, v0                             // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_cvt_f32_u32 v1, s[sgprLoopCounterL]              // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_mul_f32 v0, v0, v1                               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_cvt_u32_f32 v0, v0                               // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_mul_u32_u24 v1, v0, s[sgprGSU]                   // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_sub_u32 v1, s[sgprLoopCounterL], v1              // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_cmpx_eq_u32 exec, v1, s[sgprGSU]                 // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_add_u32 v0, 1, v0                                // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_mov_b32 v1, 0                                    // s[sgprGSUSumIdx+1] = s[sgprLoopCounterL] % s[sgprGSU]
s_mov_b64 exec, -1                                 // s[sgprLoopCounterL] = s[sgprLoopCounterL] / s[sgprGSU]
v_readfirstlane_b32 s[sgprLoopCounterL], v0        // quotient
v_readfirstlane_b32 s[sgprGSUSumIdx+1], v1         // remainder
s_add_u32 s80, 1, s[sgprLoopCounterL]              // tmp<-numIterMyWg+1
s_cmp_lt_u32 s[sgprGSUSumIdx], s[sgprGSUSumIdx+1]  // gsuSumIdx < numIterPerWgRemainder
s_cmov_b32 s[sgprLoopCounterL], s80                // numIterMyWg++ if needed
label_GSU_1:
s_mov_b32 s[sgprOrigLoopCounter], s[sgprLoopCounterL] // copy loop counter
s_and_b32 s82, s[sgprStaggerU], 0x1f00
s_lshr_b32 s82, s82, 0x8
s_and_b32 s83, s[sgprStaggerU], 0xe000
s_and_b32 s[sgprStaggerU], s[sgprStaggerU], 0xff
s_mov_b32 s80, s[sgprStaggerU]                     // init staggerU
label_beginStaggerUIter:
s_lshl_b32 s81, s80, s82                           // shift by StaggerUStride
s_cmp_ge_u32 s[sgprOrigLoopCounter], s81           // loopCount >= current shift Count
s_cbranch_scc1 label_endStaggerUIter               // jump to end
s_lshr_b32 s80, s80, 1                             // step down to smaller stagger
s_branch label_beginStaggerUIter                   // jump to begin
label_endStaggerUIter:
s_sub_u32 s81, s80, 1                              // staggerU mask
s_cmp_ge_u32 s80, 1                                // if current staggerU >= 1
s_cselect_b32 s[sgprStaggerUIter], s81, 0          // set Mask
s_cmp_eq_u32 s83, 0x0
s_cbranch_scc1 label_StaggerUMapping_1
s_mov_b32 s80, s[sgprWorkGroup0]
s_branch label_staggerInputEnd
label_StaggerUMapping_1:
s_cmp_eq_u32 s83, 0x2000
s_cbranch_scc1 label_StaggerUMapping_2
s_mov_b32 s80, s[sgprWorkGroup1]
s_branch label_staggerInputEnd
label_StaggerUMapping_2:
s_cmp_eq_u32 s83, 0x4000
s_cbranch_scc1 label_StaggerUMapping_3
s_mov_b32 s80, -0x1
s_branch label_staggerInputEnd
label_StaggerUMapping_3:
s_cmp_eq_u32 s83, 0x6000
s_cbranch_scc1 label_StaggerUMapping_4
s_mul_i32 s81, s[sgprNumWorkGroups0], s[sgprWorkGroup1]
s_add_u32 s80, s80, s81
s_add_u32 s80, s80, s[sgprWorkGroup0]
s_branch label_staggerInputEnd
label_StaggerUMapping_4:
s_cmp_eq_u32 s83, 0x8000
s_cbranch_scc1 label_staggerInputEnd
s_mov_b32 s80, -0x1
s_branch label_staggerInputEnd
label_staggerInputEnd:
s_and_b32 s[sgprStaggerUIter], s[sgprStaggerUIter], s80 // Compute actual stagger start for this tile
s_lshl_b32 s[sgprStaggerUIter], s[sgprStaggerUIter], s82 // shift by StaggerUStride

/* SRDs += (StaggerUIter) * GlobalReadIncsA+0 */
s_mul_hi_i32 s81, s[sgprStaggerUIter], s[sgprGlobalReadIncsA+0] //  stagger byte offset
s_mul_i32 s80, s[sgprStaggerUIter], s[sgprGlobalReadIncsA+0] //  stagger byte offset
s_mul_hi_i32 s[sgprWrapUA+1], s[sgprLoopCounterL], s[sgprGlobalReadIncsA+0] // Number of bytes accessed by the unroll loop
s_mul_i32 s[sgprWrapUA+0], s[sgprLoopCounterL], s[sgprGlobalReadIncsA+0] // Number of bytes accessed by the unroll loop
s_sub_u32 s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0], s[sgprWrapUA+0] // remove one iteration
s_subb_u32 s[sgprWrapUA+1], 0, s[sgprWrapUA+1]     // remove one iteration
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s80        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s81       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s80 // limit -= inc)
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s81 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32

/* SRDs += (StaggerUIter) * GlobalReadIncsB+0 */
s_mul_hi_i32 s81, s[sgprStaggerUIter], s[sgprGlobalReadIncsB+0] //  stagger byte offset
s_mul_i32 s80, s[sgprStaggerUIter], s[sgprGlobalReadIncsB+0] //  stagger byte offset
s_mul_hi_i32 s[sgprWrapUB+1], s[sgprLoopCounterL], s[sgprGlobalReadIncsB+0] // Number of bytes accessed by the unroll loop
s_mul_i32 s[sgprWrapUB+0], s[sgprLoopCounterL], s[sgprGlobalReadIncsB+0] // Number of bytes accessed by the unroll loop
s_sub_u32 s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0], s[sgprWrapUB+0] // remove one iteration
s_subb_u32 s[sgprWrapUB+1], 0, s[sgprWrapUB+1]     // remove one iteration
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s80        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s81       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s80 // limit -= inc)
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s81 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
s_add_u32 s[sgprStaggerUIter], s[sgprStaggerUIter], 2 // Subtract (PGR-1); StaggerUIter now contains target iteration to wrap
/* local read addresses: init pointers a */

/* localReadInitPointers */
/* local read addresses: init pointers b */

/* localReadInitPointers */

/* prefetch: global -> local */
s_cmp_eq_u32 s[sgprLoopCounterL], 0                // at last iteration?
s_cbranch_scc1 label_ShadowInitStart               // skip to ShadowInitStart iter b/c numIter==0
buffer_load_dwordx4 v[vgprG2LB+0:vgprG2LB+0+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_0_0
buffer_load_dwordx4 v[vgprG2LB+4:vgprG2LB+4+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+0] offen offset:0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprG2LB+8:vgprG2LB+8+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+1] offen offset:0 // G -> Reg 0_0_2_0
buffer_load_dwordx4 v[vgprG2LB+12:vgprG2LB+12+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+2] offen offset:0 // G -> Reg 0_0_3_0
buffer_load_dwordx4 v[vgprG2LB+16:vgprG2LB+16+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+3] offen offset:0 // G -> Reg 0_0_4_0
buffer_load_dwordx4 v[vgprG2LB+20:vgprG2LB+20+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+4] offen offset:0 // G -> Reg 0_0_5_0
buffer_load_dwordx4 v[vgprG2LB+24:vgprG2LB+24+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+5] offen offset:0 // G -> Reg 0_0_6_0
.set vgprValuA_X0_I0, vgprValuA_X0_I0_0
.set vgprValuA_X2_I0, vgprValuA_X2_I0_0
buffer_load_dwordx4 v[vgprValuA_X0_I0+0:vgprValuA_X0_I0+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_0_0
buffer_load_dwordx4 v[vgprValuA_X2_I0+0:vgprValuA_X2_I0+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+3] offen offset:0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprValuA_X0_I0+4:vgprValuA_X0_I0+4+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+0] offen offset:0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprValuA_X2_I0+4:vgprValuA_X2_I0+4+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+4] offen offset:0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprValuA_X0_I0+8:vgprValuA_X0_I0+8+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+1] offen offset:0 // G -> Reg 0_0_2_0
buffer_load_dwordx4 v[vgprValuA_X2_I0+8:vgprValuA_X2_I0+8+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+5] offen offset:0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprValuA_X0_I0+12:vgprValuA_X0_I0+12+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+2] offen offset:0 // G -> Reg 0_0_3_0
buffer_load_dwordx4 v[vgprValuA_X2_I0+12:vgprValuA_X2_I0+12+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+6] offen offset:0 // G -> Reg 0_0_1_0

/* global read inc A loopL */
s_add_u32 s82, s[sgprLoopCounterL], 1              // remove pf(1)
s_cmp_eq_u32 s[sgprStaggerUIter], s82              // Is this wrapIter? (pf)
s_cselect_b32 s80, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
s_cselect_b32 s81, s[sgprWrapUA+1], 0              // incUpper <- ?
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s80        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s81       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s80 // limit -= inc)
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s81 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32

/* global read inc B loopL */
s_add_u32 s82, s[sgprLoopCounterL], 1              // remove pf(1)
s_cmp_eq_u32 s[sgprStaggerUIter], s82              // Is this wrapIter? (pf)
s_cselect_b32 s80, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
s_cselect_b32 s81, s[sgprWrapUB+1], 0              // incUpper <- ?
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s80        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s81       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s80 // limit -= inc)
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s81 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32

/******************************************/
/* End setupNewTile                       */
/******************************************/
label_ShadowInitStart:
s_mov_b32 s[sgprSrdD+0], s[sgprAddressD+0]         // init SRD base address (lower)
s_mov_b32 s[sgprSrdD+1], s[sgprAddressD+1]         // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdD+2], 0x80000000
s_mov_b32 s[sgprSrdD+3], Srd127_96                 // Set bits 127_96 in post-loop SRD

s_mov_b32 s[sgprSrdC+0], s[sgprAddressC+0]         // init SRD base address (lower)
s_mov_b32 s[sgprSrdC+1], s[sgprAddressC+1]         // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdC+2], 0x80000000
s_mov_b32 s[sgprSrdC+3], Srd127_96                 // Set bits 127_96 in post-loop SRD


s_mul_i32 s82, MT1, s[sgprWorkGroup1]              // <- wg1*MT1
s_mul_hi_u32 s81, s82, s[sgprStrideC1J]            // ScaleC s82 by Stride
s_mul_i32 s80, s82, s[sgprStrideC1J]               // ScaleC s82 by Stride
s_lshl_b64 s[80:81], s[80:81], s[sgprGSULog2BpeC]  // scale by bpe
s_add_u32 s[sgprSrdC+0], s[sgprAddressC+0], s80    // add lo to SRD
s_addc_u32 s[sgprSrdC+1], s[sgprAddressC+1], s81   // add hi to SRD
s_mul_hi_u32 s81, s82, s[sgprStrideD1J]            // ScaleD s82 by Stride
s_mul_i32 s80, s82, s[sgprStrideD1J]               // ScaleD s82 by Stride
s_lshl_b64 s[80:81], s[80:81], s[sgprGSULog2BpeD]  // scale by bpe
s_add_u32 s[sgprSrdD+0], s[sgprAddressD+0], s80    // add lo to SRD
s_addc_u32 s[sgprSrdD+1], s[sgprAddressD+1], s81   // add hi to SRD

s_mul_hi_u32 s81, s[sgprWorkGroup2], s[sgprStrideCK] // ScaleC s[sgprWorkGroup2] by Stride
s_mul_i32 s80, s[sgprWorkGroup2], s[sgprStrideCK]  // ScaleC s[sgprWorkGroup2] by Stride
s_lshl_b64 s[80:81], s[80:81], s[sgprGSULog2BpeC]  // scale by bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s80        // add lo to SRD
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], s81       // add hi to SRD
s_mul_hi_u32 s81, s[sgprWorkGroup2], s[sgprStrideDK] // ScaleD s[sgprWorkGroup2] by Stride
s_mul_i32 s80, s[sgprWorkGroup2], s[sgprStrideDK]  // ScaleD s[sgprWorkGroup2] by Stride
s_lshl_b64 s[80:81], s[80:81], s[sgprGSULog2BpeD]  // scale by bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s80        // add lo to SRD
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], s81       // add hi to SRD

s_cmp_eq_u32 s[sgprGSU], 1                         // GSU == 1 ?
s_cbranch_scc1 label_GSU_2                         // branch if GSU == 1
// GSU Output Buffer offset: Free0 + (Free1-1)*StrideC1J + (Free2-1)*StrideCK * GSUIdx * bpe%s
s_mul_hi_u32 s81, s[sgprSizesFree+0], s[sgprGSUSumIdx] // Free0
s_mul_i32 s80, s[sgprSizesFree+0], s[sgprGSUSumIdx] // Free0
s_sub_u32 s79, s[sgprSizesFree+1], 1               // Free1
s_mul_i32 s79, s79, s[sgprGSUSumIdx]               // Free1
s_mul_hi_u32 s82, s79, s[sgprStrideC1J]            // Free1
s_mul_i32 s79, s79, s[sgprStrideC1J]               // Free1
s_add_u32 s80, s80, s79                            // Free1
s_addc_u32 s81, s81, s82                           // Free1
s_sub_u32 s79, s[sgprSizesFree+2], 1               // Free2
s_mul_i32 s79, s79, s[sgprGSUSumIdx]               // Free2
s_mul_hi_u32 s82, s79, s[sgprStrideCK]             // Free2
s_mul_i32 s79, s79, s[sgprStrideCK]                // Free2
s_add_u32 s80, s80, s79                            // Free2
s_addc_u32 s81, s81, s82                           // Free2
s_lshl_b64 s[80:81], s[80:81], 2                   // scale by bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s80        // add lo GSU offset to SRD
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], s81       // add hi GSU offset to SRD
label_GSU_2:
.set sgprGSULog2BpeC, UNDEF

/* initC: remove ValuC vgpr buffer [0...0) from pool */

/* initC: remove acc vgpr buffer [0...224) from pool */

/* initC: remove ValuA/B vgpr buffer [0...144) from pool */
v_accvgpr_write acc0, 0x0                          // initC
v_accvgpr_write acc1, 0x0                          // initC
v_accvgpr_write acc2, 0x0                          // initC
v_accvgpr_write acc3, 0x0                          // initC
v_accvgpr_write acc4, 0x0                          // initC
v_accvgpr_write acc5, 0x0                          // initC
v_accvgpr_write acc6, 0x0                          // initC
v_accvgpr_write acc7, 0x0                          // initC
v_accvgpr_write acc8, 0x0                          // initC
v_accvgpr_write acc9, 0x0                          // initC
v_accvgpr_write acc10, 0x0                         // initC
v_accvgpr_write acc11, 0x0                         // initC
v_accvgpr_write acc12, 0x0                         // initC
v_accvgpr_write acc13, 0x0                         // initC
v_accvgpr_write acc14, 0x0                         // initC
v_accvgpr_write acc15, 0x0                         // initC
v_accvgpr_write acc16, 0x0                         // initC
v_accvgpr_write acc17, 0x0                         // initC
v_accvgpr_write acc18, 0x0                         // initC
v_accvgpr_write acc19, 0x0                         // initC
v_accvgpr_write acc20, 0x0                         // initC
v_accvgpr_write acc21, 0x0                         // initC
v_accvgpr_write acc22, 0x0                         // initC
v_accvgpr_write acc23, 0x0                         // initC
v_accvgpr_write acc24, 0x0                         // initC
v_accvgpr_write acc25, 0x0                         // initC
v_accvgpr_write acc26, 0x0                         // initC
v_accvgpr_write acc27, 0x0                         // initC
v_accvgpr_write acc28, 0x0                         // initC
v_accvgpr_write acc29, 0x0                         // initC
v_accvgpr_write acc30, 0x0                         // initC
v_accvgpr_write acc31, 0x0                         // initC
v_accvgpr_write acc32, 0x0                         // initC
v_accvgpr_write acc33, 0x0                         // initC
v_accvgpr_write acc34, 0x0                         // initC
v_accvgpr_write acc35, 0x0                         // initC
v_accvgpr_write acc36, 0x0                         // initC
v_accvgpr_write acc37, 0x0                         // initC
v_accvgpr_write acc38, 0x0                         // initC
v_accvgpr_write acc39, 0x0                         // initC
v_accvgpr_write acc40, 0x0                         // initC
v_accvgpr_write acc41, 0x0                         // initC
v_accvgpr_write acc42, 0x0                         // initC
v_accvgpr_write acc43, 0x0                         // initC
v_accvgpr_write acc44, 0x0                         // initC
v_accvgpr_write acc45, 0x0                         // initC
v_accvgpr_write acc46, 0x0                         // initC
v_accvgpr_write acc47, 0x0                         // initC
v_accvgpr_write acc48, 0x0                         // initC
v_accvgpr_write acc49, 0x0                         // initC
v_accvgpr_write acc50, 0x0                         // initC
v_accvgpr_write acc51, 0x0                         // initC
v_accvgpr_write acc52, 0x0                         // initC
v_accvgpr_write acc53, 0x0                         // initC
v_accvgpr_write acc54, 0x0                         // initC
v_accvgpr_write acc55, 0x0                         // initC
v_accvgpr_write acc56, 0x0                         // initC
v_accvgpr_write acc57, 0x0                         // initC
v_accvgpr_write acc58, 0x0                         // initC
v_accvgpr_write acc59, 0x0                         // initC
v_accvgpr_write acc60, 0x0                         // initC
v_accvgpr_write acc61, 0x0                         // initC
v_accvgpr_write acc62, 0x0                         // initC
v_accvgpr_write acc63, 0x0                         // initC
v_accvgpr_write acc64, 0x0                         // initC
v_accvgpr_write acc65, 0x0                         // initC
v_accvgpr_write acc66, 0x0                         // initC
v_accvgpr_write acc67, 0x0                         // initC
v_accvgpr_write acc68, 0x0                         // initC
v_accvgpr_write acc69, 0x0                         // initC
v_accvgpr_write acc70, 0x0                         // initC
v_accvgpr_write acc71, 0x0                         // initC
v_accvgpr_write acc72, 0x0                         // initC
v_accvgpr_write acc73, 0x0                         // initC
v_accvgpr_write acc74, 0x0                         // initC
v_accvgpr_write acc75, 0x0                         // initC
v_accvgpr_write acc76, 0x0                         // initC
v_accvgpr_write acc77, 0x0                         // initC
v_accvgpr_write acc78, 0x0                         // initC
v_accvgpr_write acc79, 0x0                         // initC
v_accvgpr_write acc80, 0x0                         // initC
v_accvgpr_write acc81, 0x0                         // initC
v_accvgpr_write acc82, 0x0                         // initC
v_accvgpr_write acc83, 0x0                         // initC
v_accvgpr_write acc84, 0x0                         // initC
v_accvgpr_write acc85, 0x0                         // initC
v_accvgpr_write acc86, 0x0                         // initC
v_accvgpr_write acc87, 0x0                         // initC
v_accvgpr_write acc88, 0x0                         // initC
v_accvgpr_write acc89, 0x0                         // initC
v_accvgpr_write acc90, 0x0                         // initC
v_accvgpr_write acc91, 0x0                         // initC
v_accvgpr_write acc92, 0x0                         // initC
v_accvgpr_write acc93, 0x0                         // initC
v_accvgpr_write acc94, 0x0                         // initC
v_accvgpr_write acc95, 0x0                         // initC
v_accvgpr_write acc96, 0x0                         // initC
v_accvgpr_write acc97, 0x0                         // initC
v_accvgpr_write acc98, 0x0                         // initC
v_accvgpr_write acc99, 0x0                         // initC
v_accvgpr_write acc100, 0x0                        // initC
v_accvgpr_write acc101, 0x0                        // initC
v_accvgpr_write acc102, 0x0                        // initC
v_accvgpr_write acc103, 0x0                        // initC
v_accvgpr_write acc104, 0x0                        // initC
v_accvgpr_write acc105, 0x0                        // initC
v_accvgpr_write acc106, 0x0                        // initC
v_accvgpr_write acc107, 0x0                        // initC
v_accvgpr_write acc108, 0x0                        // initC
v_accvgpr_write acc109, 0x0                        // initC
v_accvgpr_write acc110, 0x0                        // initC
v_accvgpr_write acc111, 0x0                        // initC
v_accvgpr_write acc112, 0x0                        // initC
v_accvgpr_write acc113, 0x0                        // initC
v_accvgpr_write acc114, 0x0                        // initC
v_accvgpr_write acc115, 0x0                        // initC
v_accvgpr_write acc116, 0x0                        // initC
v_accvgpr_write acc117, 0x0                        // initC
v_accvgpr_write acc118, 0x0                        // initC
v_accvgpr_write acc119, 0x0                        // initC
v_accvgpr_write acc120, 0x0                        // initC
v_accvgpr_write acc121, 0x0                        // initC
v_accvgpr_write acc122, 0x0                        // initC
v_accvgpr_write acc123, 0x0                        // initC
v_accvgpr_write acc124, 0x0                        // initC
v_accvgpr_write acc125, 0x0                        // initC
v_accvgpr_write acc126, 0x0                        // initC
v_accvgpr_write acc127, 0x0                        // initC
v_accvgpr_write acc128, 0x0                        // initC
v_accvgpr_write acc129, 0x0                        // initC
v_accvgpr_write acc130, 0x0                        // initC
v_accvgpr_write acc131, 0x0                        // initC
v_accvgpr_write acc132, 0x0                        // initC
v_accvgpr_write acc133, 0x0                        // initC
v_accvgpr_write acc134, 0x0                        // initC
v_accvgpr_write acc135, 0x0                        // initC
v_accvgpr_write acc136, 0x0                        // initC
v_accvgpr_write acc137, 0x0                        // initC
v_accvgpr_write acc138, 0x0                        // initC
v_accvgpr_write acc139, 0x0                        // initC
v_accvgpr_write acc140, 0x0                        // initC
v_accvgpr_write acc141, 0x0                        // initC
v_accvgpr_write acc142, 0x0                        // initC
v_accvgpr_write acc143, 0x0                        // initC
v_accvgpr_write acc144, 0x0                        // initC
v_accvgpr_write acc145, 0x0                        // initC
v_accvgpr_write acc146, 0x0                        // initC
v_accvgpr_write acc147, 0x0                        // initC
v_accvgpr_write acc148, 0x0                        // initC
v_accvgpr_write acc149, 0x0                        // initC
v_accvgpr_write acc150, 0x0                        // initC
v_accvgpr_write acc151, 0x0                        // initC
v_accvgpr_write acc152, 0x0                        // initC
v_accvgpr_write acc153, 0x0                        // initC
v_accvgpr_write acc154, 0x0                        // initC
v_accvgpr_write acc155, 0x0                        // initC
v_accvgpr_write acc156, 0x0                        // initC
v_accvgpr_write acc157, 0x0                        // initC
v_accvgpr_write acc158, 0x0                        // initC
v_accvgpr_write acc159, 0x0                        // initC
v_accvgpr_write acc160, 0x0                        // initC
v_accvgpr_write acc161, 0x0                        // initC
v_accvgpr_write acc162, 0x0                        // initC
v_accvgpr_write acc163, 0x0                        // initC
v_accvgpr_write acc164, 0x0                        // initC
v_accvgpr_write acc165, 0x0                        // initC
v_accvgpr_write acc166, 0x0                        // initC
v_accvgpr_write acc167, 0x0                        // initC
v_accvgpr_write acc168, 0x0                        // initC
v_accvgpr_write acc169, 0x0                        // initC
v_accvgpr_write acc170, 0x0                        // initC
v_accvgpr_write acc171, 0x0                        // initC
v_accvgpr_write acc172, 0x0                        // initC
v_accvgpr_write acc173, 0x0                        // initC
v_accvgpr_write acc174, 0x0                        // initC
v_accvgpr_write acc175, 0x0                        // initC
v_accvgpr_write acc176, 0x0                        // initC
v_accvgpr_write acc177, 0x0                        // initC
v_accvgpr_write acc178, 0x0                        // initC
v_accvgpr_write acc179, 0x0                        // initC
v_accvgpr_write acc180, 0x0                        // initC
v_accvgpr_write acc181, 0x0                        // initC
v_accvgpr_write acc182, 0x0                        // initC
v_accvgpr_write acc183, 0x0                        // initC
v_accvgpr_write acc184, 0x0                        // initC
v_accvgpr_write acc185, 0x0                        // initC
v_accvgpr_write acc186, 0x0                        // initC
v_accvgpr_write acc187, 0x0                        // initC
v_accvgpr_write acc188, 0x0                        // initC
v_accvgpr_write acc189, 0x0                        // initC
v_accvgpr_write acc190, 0x0                        // initC
v_accvgpr_write acc191, 0x0                        // initC
v_accvgpr_write acc192, 0x0                        // initC
v_accvgpr_write acc193, 0x0                        // initC
v_accvgpr_write acc194, 0x0                        // initC
v_accvgpr_write acc195, 0x0                        // initC
v_accvgpr_write acc196, 0x0                        // initC
v_accvgpr_write acc197, 0x0                        // initC
v_accvgpr_write acc198, 0x0                        // initC
v_accvgpr_write acc199, 0x0                        // initC
v_accvgpr_write acc200, 0x0                        // initC
v_accvgpr_write acc201, 0x0                        // initC
v_accvgpr_write acc202, 0x0                        // initC
v_accvgpr_write acc203, 0x0                        // initC
v_accvgpr_write acc204, 0x0                        // initC
v_accvgpr_write acc205, 0x0                        // initC
v_accvgpr_write acc206, 0x0                        // initC
v_accvgpr_write acc207, 0x0                        // initC
v_accvgpr_write acc208, 0x0                        // initC
v_accvgpr_write acc209, 0x0                        // initC
v_accvgpr_write acc210, 0x0                        // initC
v_accvgpr_write acc211, 0x0                        // initC
v_accvgpr_write acc212, 0x0                        // initC
v_accvgpr_write acc213, 0x0                        // initC
v_accvgpr_write acc214, 0x0                        // initC
v_accvgpr_write acc215, 0x0                        // initC
v_accvgpr_write acc216, 0x0                        // initC
v_accvgpr_write acc217, 0x0                        // initC
v_accvgpr_write acc218, 0x0                        // initC
v_accvgpr_write acc219, 0x0                        // initC
v_accvgpr_write acc220, 0x0                        // initC
v_accvgpr_write acc221, 0x0                        // initC
v_accvgpr_write acc222, 0x0                        // initC
v_accvgpr_write acc223, 0x0                        // initC
s_cmp_eq_u32 s[sgprLoopCounterL], 0                // at last iteration?

/* after InitC, skip to end of prefetch last iter if numIter==0 */
s_cbranch_scc0 label_NoBranch_GSEZ432FW3PKFYRD_0   // Only branch on scc1
s_getpc_b64 s[80:81]                               // addr of next instr
s_add_i32 s82, label_PrefetchGlobalLastIterEnd, 0x4 // target branch offset
s_add_u32 s80, s80, s82                            // add target branch offset
s_addc_u32 s81, s81, 0                             // add high and carry
s_setpc_b64 s[80:81]                               // branch to label_PrefetchGlobalLastIterEnd
label_NoBranch_GSEZ432FW3PKFYRD_0:
s_waitcnt vmcnt(8)                                 // 8wait for global read

/* local write a */

/* local write b */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:640 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 640
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:1280 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 1280
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:1920 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 1920
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+16:vgprG2LB+16+3] offset:2560 // lwoB_0_0_4_0 = (0*LSCB)*(MT1J+PAD) + (4*LSPB) = 2560
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+20:vgprG2LB+20+3] offset:3200 // lwoB_0_0_5_0 = (0*LSCB)*(MT1J+PAD) + (5*LSPB) = 3200
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+24:vgprG2LB+24+3] offset:3840 // lwoB_0_0_6_0 = (0*LSCB)*(MT1J+PAD) + (6*LSPB) = 3840

/* local write swap a */

/* local write swap b */
s_cmp_eq_u32 s[sgprLoopCounterL], 0x1              // PGR=2 but only 1 loop
s_cbranch_scc1 label_skipPGR2_0                    // PGR=2 but only 1 loop
buffer_load_dwordx4 v[vgprG2LB+0:vgprG2LB+0+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_0_0
buffer_load_dwordx4 v[vgprG2LB+4:vgprG2LB+4+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+0] offen offset:0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprG2LB+8:vgprG2LB+8+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+1] offen offset:0 // G -> Reg 0_0_2_0
buffer_load_dwordx4 v[vgprG2LB+12:vgprG2LB+12+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+2] offen offset:0 // G -> Reg 0_0_3_0
buffer_load_dwordx4 v[vgprG2LB+16:vgprG2LB+16+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+3] offen offset:0 // G -> Reg 0_0_4_0
buffer_load_dwordx4 v[vgprG2LB+20:vgprG2LB+20+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+4] offen offset:0 // G -> Reg 0_0_5_0
buffer_load_dwordx4 v[vgprG2LB+24:vgprG2LB+24+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+5] offen offset:0 // G -> Reg 0_0_6_0
label_skipPGR2_0:
s_waitcnt lgkmcnt(0)                               // 0prefetch wait for local write
// Skip force waitcnt0
s_barrier

/* local read prefetch a */

/* local read prefetch b */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:2560 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:5120 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=2 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:7680 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=3 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:10240 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=4 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:12800 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=5 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:15360 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=6 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:17920 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=7 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+32:vgprValuB_X0_I0+32+3], v[vgprLocalReadAddrB] offset:20480 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=8 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+36:vgprValuB_X0_I0+36+3], v[vgprLocalReadAddrB] offset:23040 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=9 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+40:vgprValuB_X0_I0+40+3], v[vgprLocalReadAddrB] offset:25600 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=10 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+44:vgprValuB_X0_I0+44+3], v[vgprLocalReadAddrB] offset:28160 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=11 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+48:vgprValuB_X0_I0+48+3], v[vgprLocalReadAddrB] offset:30720 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=12 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b128 v[vgprValuB_X0_I0+52:vgprValuB_X0_I0+52+3], v[vgprLocalReadAddrB] offset:33280 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=13 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0

/* local read inc a */
/* N/A, lro->32 */
/* self.localReadDoCntA 1 self.localReadDoCntB 1 */

/* local read inc b */
/* N/A, lro->32 */
/* self.localReadDoCntA 1 self.localReadDoCntB 1 */
.set vgprValuA_X0_I0, vgprValuA_X0_I0_0
.set vgprValuA_X2_I0, vgprValuA_X2_I0_0

/******************************************/
/* Unrolled Loop(s) - Begin               */
/******************************************/
label_openLoopL:
s_cmp_eq_u32 s[sgprLoopCounterL], 0x1              // LoopCounterL < EndCounter
s_cbranch_scc1 label_LoopEndL_odd_NoLoadLoop
s_cmp_le_u32 s[sgprLoopCounterL], 0x2              // LoopCounterL < EndCounter
s_cbranch_scc1 label_LoopEndL_even                 // do not enter LoopL
label_LoopBeginL:

/******************************************/
/* Unrolled Loop 1/2 - Begin              */
/******************************************/

s_waitcnt vmcnt(7)

/* Begin Each Unroll: Check VGPR.checkin for INT8 LW */

/* iter 0 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
buffer_load_dwordx4 v[vgprValuA_X0_I0_1+0:vgprValuA_X0_I0_1+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_0_0
buffer_load_dwordx4 v[vgprValuA_X2_I0_1+0:vgprValuA_X2_I0_1+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+3] offen offset:0 // G -> Reg 0_0_1_0
/* global read inc B loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s80, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
s_cselect_b32 s81, s[sgprWrapUB+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s80        // gra SRD += inc(lower)
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
ds_read_b128 v[vgprValuB_X2_I0+4:vgprValuB_X2_I0+4+3], v[vgprLocalReadAddrB] offset:2624 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s81       // gra SRD += inc(upper)
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s80 // limit -= inc)
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s81 // limit -= inc)
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
ds_read_b128 v[vgprValuB_X2_I0+8:vgprValuB_X2_I0+8+3], v[vgprLocalReadAddrB] offset:5184 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=2 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
ds_read_b128 v[vgprValuB_X2_I0+12:vgprValuB_X2_I0+12+3], v[vgprLocalReadAddrB] offset:7744 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=3 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
ds_read_b128 v[vgprValuB_X2_I0+16:vgprValuB_X2_I0+16+3], v[vgprLocalReadAddrB] offset:10304 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=4 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:16  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:17  */
ds_read_b128 v[vgprValuB_X2_I0+20:vgprValuB_X2_I0+20+3], v[vgprLocalReadAddrB] offset:12864 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=5 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:18  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+24:vgprValuB_X2_I0+24+3], v[vgprLocalReadAddrB] offset:15424 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=6 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+28:vgprValuB_X2_I0+28+3], v[vgprLocalReadAddrB] offset:17984 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=7 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+32:vgprValuB_X2_I0+32+3], v[vgprLocalReadAddrB] offset:20544 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=8 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprValuA_X0_I0_1+4:vgprValuA_X0_I0_1+4+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+0] offen offset:0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprValuA_X2_I0_1+4:vgprValuA_X2_I0_1+4+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+4] offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+36:vgprValuB_X2_I0+36+3], v[vgprLocalReadAddrB] offset:23104 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=9 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+40:vgprValuB_X2_I0+40+3], v[vgprLocalReadAddrB] offset:25664 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=10 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+44:vgprValuB_X2_I0+44+3], v[vgprLocalReadAddrB] offset:28224 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=11 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:36  */
/* schedule remaining localreads for 1LDSB */
/* localReadsVacancy: latencyLeft 2 */
/* 1 LDS buffer: read-sync-write */
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:37  */
/* sched write - iter 0 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:38  */
ds_read_b128 v[vgprValuB_X2_I0+48:vgprValuB_X2_I0+48+3], v[vgprLocalReadAddrB] offset:30784 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=12 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:39  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:40  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:41  */
ds_read_b128 v[vgprValuB_X2_I0+52:vgprValuB_X2_I0+52+3], v[vgprLocalReadAddrB] offset:33344 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=13 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:42  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:43  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:44  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:45  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:46  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:47  */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:48  */
/* sched write - iter 0 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:49  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:50  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:51  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:52  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:53  */
buffer_load_dwordx4 v[vgprValuA_X0_I0_1+8:vgprValuA_X0_I0_1+8+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+1] offen offset:0 // G -> Reg 0_0_2_0
buffer_load_dwordx4 v[vgprValuA_X2_I0_1+8:vgprValuA_X2_I0_1+8+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+5] offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:54  */
s_waitcnt lgkmcnt(0)
s_barrier
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:55  */
s_waitcnt vmcnt(12)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */

/* iter 1 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:56  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:57  */
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:58  */
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:59  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:60  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:61  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:62  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:63  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:64  */
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:65  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:66  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:67  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:68  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:69  */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:70  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:71  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:72  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:73  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:74  */
s_waitcnt vmcnt(11)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:75  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:640 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 640
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:76  */
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:77  */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:78  */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:79  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:80  */
buffer_load_dwordx4 v[vgprValuA_X0_I0_1+12:vgprValuA_X0_I0_1+12+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+2] offen offset:0 // G -> Reg 0_0_3_0
buffer_load_dwordx4 v[vgprValuA_X2_I0_1+12:vgprValuA_X2_I0_1+12+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+6] offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:81  */
/* sched write - iter 1 writesPerItem=1 */
/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:82  */
s_cselect_b32 s80, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:83  */
s_cselect_b32 s81, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:84  */
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s80        // gra SRD += inc(lower)
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:85  */
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s81       // gra SRD += inc(upper)
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:86  */
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s80 // limit -= inc)
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:87  */
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s81 // limit -= inc)
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:88  */
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:89  */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:90  */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:91  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:92  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:93  */
s_waitcnt vmcnt(12)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:94  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:1280 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 1280
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:95  */
buffer_load_dwordx4 v[vgprG2LB+0:vgprG2LB+0+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_0_0
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:96  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:97  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:98  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:99  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:100  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:101  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:102  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:103  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:104  */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:105  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:106  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:107  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:108  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:109  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:110  */
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:111  */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */

/* iter 2 (reset local read pointers iteration)  (swap local read pointers iteration)  */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:112  */
s_waitcnt vmcnt(12)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:113  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:1920 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 1920
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:114  */
/* sched write - iter 2 writesPerItem=1 */
buffer_load_dwordx4 v[vgprG2LB+4:vgprG2LB+4+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+0] offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:115  */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:116  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:117  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:118  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:119  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:120  */
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:121  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:122  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:123  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:124  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:125  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:126  */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:127  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:128  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:129  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:130  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:131  */
s_waitcnt vmcnt(12)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:132  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+16:vgprG2LB+16+3] offset:2560 // lwoB_0_0_4_0 = (0*LSCB)*(MT1J+PAD) + (4*LSPB) = 2560
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:133  */
buffer_load_dwordx4 v[vgprG2LB+8:vgprG2LB+8+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+1] offen offset:0 // G -> Reg 0_0_2_0
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:134  */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:135  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:136  */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:137  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:138  */
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:139  */
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:140  */
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:141  */
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:142  */
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:143  */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:144  */
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:145  */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:146  */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:147  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:148  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:149  */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:150  */
s_waitcnt vmcnt(12)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:151  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+20:vgprG2LB+20+3] offset:3200 // lwoB_0_0_5_0 = (0*LSCB)*(MT1J+PAD) + (5*LSPB) = 3200
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:152  */
buffer_load_dwordx4 v[vgprG2LB+12:vgprG2LB+12+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+2] offen offset:0 // G -> Reg 0_0_3_0
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:153  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:154  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:155  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:156  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:157  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:158  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:159  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:160  */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:161  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:162  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:163  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:164  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:165  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:166  */
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:167  */

/* local read swap offsets a */

/* local read swap offsets b */

/* local read init pointers a */

/* localReadInitPointers */

/* local read init pointers b */

/* localReadInitPointers */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=14 */

/* iter 3 (swap and reset local write pointers iteration)  */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:168  */
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:169  */
s_waitcnt vmcnt(12)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:170  */
/* sched write - iter 3 writesPerItem=1 */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+24:vgprG2LB+24+3] offset:3840 // lwoB_0_0_6_0 = (0*LSCB)*(MT1J+PAD) + (6*LSPB) = 3840
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:171  */
buffer_load_dwordx4 v[vgprG2LB+16:vgprG2LB+16+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+3] offen offset:0 // G -> Reg 0_0_4_0
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:172  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:173  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:174  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:175  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:176  */
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:177  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:178  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:179  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:180  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:181  */
/* sched write - iter 3 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:182  */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:183  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:184  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:185  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:186  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:187  */
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:188  */
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:189  */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:190  */
buffer_load_dwordx4 v[vgprG2LB+20:vgprG2LB+20+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+4] offen offset:0 // G -> Reg 0_0_5_0
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:191  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:192  */
/* sched write - iter 3 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:193  */

/* local write swap offsets a */

/* local write swap offsets b */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:194  */
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:195  */
s_waitcnt lgkmcnt(0)                               // 3wait for local write
// Skip force waitcnt0
s_barrier
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:196  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:197  */
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:198  */
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:2560 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:199  */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:200  */
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:5120 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=2 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:201  */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:202  */
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:7680 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=3 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:203  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:204  */
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:10240 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=4 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:205  */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:206  */
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:12800 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=5 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:207  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:208  */
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:15360 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=6 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:209  */
buffer_load_dwordx4 v[vgprG2LB+24:vgprG2LB+24+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+5] offen offset:0 // G -> Reg 0_0_6_0
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:210  */
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:17920 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=7 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:211  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:212  */
ds_read_b128 v[vgprValuB_X0_I0+32:vgprValuB_X0_I0+32+3], v[vgprLocalReadAddrB] offset:20480 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=8 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:213  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:214  */
ds_read_b128 v[vgprValuB_X0_I0+36:vgprValuB_X0_I0+36+3], v[vgprLocalReadAddrB] offset:23040 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=9 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:215  */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:216  */
ds_read_b128 v[vgprValuB_X0_I0+40:vgprValuB_X0_I0+40+3], v[vgprLocalReadAddrB] offset:25600 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=10 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:217  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:218  */
ds_read_b128 v[vgprValuB_X0_I0+44:vgprValuB_X0_I0+44+3], v[vgprLocalReadAddrB] offset:28160 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=11 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:219  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:220  */
ds_read_b128 v[vgprValuB_X0_I0+48:vgprValuB_X0_I0+48+3], v[vgprLocalReadAddrB] offset:30720 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=12 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:221  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:222  */
ds_read_b128 v[vgprValuB_X0_I0+52:vgprValuB_X0_I0+52+3], v[vgprLocalReadAddrB] offset:33280 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=13 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:223  */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=1 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */

/******************************************/
/* Unrolled Loop - End                    */
/******************************************/

/* closeLoop loopL finalLoop=1 tailLoop=0 */
s_sub_u32 s[sgprLoopCounterL], s[sgprLoopCounterL], 1 // dec counterL
s_cmp_eq_i32 s[sgprLoopCounterL], 0x2                 // counterL==1
s_cbranch_scc1 label_LoopEndL_odd          // to End

/******************************************/
/* Unrolled Loop 2/2 - Begin              */
/******************************************/

.set vgprValuA_X0_I0, vgprValuA_X0_I0_1
.set vgprValuA_X2_I0, vgprValuA_X2_I0_1

s_waitcnt vmcnt(7)

/* Begin Each Unroll: Check VGPR.checkin for INT8 LW */

/* iter 0 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
buffer_load_dwordx4 v[vgprValuA_X0_I0_0+0:vgprValuA_X0_I0_0+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_0_0
buffer_load_dwordx4 v[vgprValuA_X2_I0_0+0:vgprValuA_X2_I0_0+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+3] offen offset:0 // G -> Reg 0_0_1_0
/* global read inc B loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s80, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
s_cselect_b32 s81, s[sgprWrapUB+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s80        // gra SRD += inc(lower)
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
ds_read_b128 v[vgprValuB_X2_I0+4:vgprValuB_X2_I0+4+3], v[vgprLocalReadAddrB] offset:2624 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s81       // gra SRD += inc(upper)
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s80 // limit -= inc)
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s81 // limit -= inc)
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
ds_read_b128 v[vgprValuB_X2_I0+8:vgprValuB_X2_I0+8+3], v[vgprLocalReadAddrB] offset:5184 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=2 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
ds_read_b128 v[vgprValuB_X2_I0+12:vgprValuB_X2_I0+12+3], v[vgprLocalReadAddrB] offset:7744 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=3 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
ds_read_b128 v[vgprValuB_X2_I0+16:vgprValuB_X2_I0+16+3], v[vgprLocalReadAddrB] offset:10304 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=4 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:16  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:17  */
ds_read_b128 v[vgprValuB_X2_I0+20:vgprValuB_X2_I0+20+3], v[vgprLocalReadAddrB] offset:12864 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=5 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:18  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+24:vgprValuB_X2_I0+24+3], v[vgprLocalReadAddrB] offset:15424 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=6 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+28:vgprValuB_X2_I0+28+3], v[vgprLocalReadAddrB] offset:17984 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=7 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+32:vgprValuB_X2_I0+32+3], v[vgprLocalReadAddrB] offset:20544 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=8 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprValuA_X0_I0_0+4:vgprValuA_X0_I0_0+4+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+0] offen offset:0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprValuA_X2_I0_0+4:vgprValuA_X2_I0_0+4+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+4] offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+36:vgprValuB_X2_I0+36+3], v[vgprLocalReadAddrB] offset:23104 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=9 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+40:vgprValuB_X2_I0+40+3], v[vgprLocalReadAddrB] offset:25664 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=10 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+44:vgprValuB_X2_I0+44+3], v[vgprLocalReadAddrB] offset:28224 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=11 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:36  */
/* schedule remaining localreads for 1LDSB */
/* localReadsVacancy: latencyLeft 2 */
/* 1 LDS buffer: read-sync-write */
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:37  */
/* sched write - iter 0 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:38  */
ds_read_b128 v[vgprValuB_X2_I0+48:vgprValuB_X2_I0+48+3], v[vgprLocalReadAddrB] offset:30784 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=12 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:39  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:40  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:41  */
ds_read_b128 v[vgprValuB_X2_I0+52:vgprValuB_X2_I0+52+3], v[vgprLocalReadAddrB] offset:33344 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=13 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:42  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:43  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:44  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:45  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:46  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:47  */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:48  */
/* sched write - iter 0 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:49  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:50  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:51  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:52  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:53  */
buffer_load_dwordx4 v[vgprValuA_X0_I0_0+8:vgprValuA_X0_I0_0+8+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+1] offen offset:0 // G -> Reg 0_0_2_0
buffer_load_dwordx4 v[vgprValuA_X2_I0_0+8:vgprValuA_X2_I0_0+8+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+5] offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:54  */
s_waitcnt lgkmcnt(0)
s_barrier
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:55  */
s_waitcnt vmcnt(12)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */

/* iter 1 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:56  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:57  */
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:58  */
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:59  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:60  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:61  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:62  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:63  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:64  */
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:65  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:66  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:67  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:68  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:69  */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:70  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:71  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:72  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:73  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:74  */
s_waitcnt vmcnt(11)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:75  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:640 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 640
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:76  */
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:77  */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:78  */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:79  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:80  */
buffer_load_dwordx4 v[vgprValuA_X0_I0_0+12:vgprValuA_X0_I0_0+12+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+2] offen offset:0 // G -> Reg 0_0_3_0
buffer_load_dwordx4 v[vgprValuA_X2_I0_0+12:vgprValuA_X2_I0_0+12+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+6] offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:81  */
/* sched write - iter 1 writesPerItem=1 */
/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:82  */
s_cselect_b32 s80, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:83  */
s_cselect_b32 s81, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:84  */
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s80        // gra SRD += inc(lower)
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:85  */
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s81       // gra SRD += inc(upper)
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:86  */
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s80 // limit -= inc)
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:87  */
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s81 // limit -= inc)
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:88  */
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:89  */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:90  */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:91  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:92  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:93  */
s_waitcnt vmcnt(12)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:94  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:1280 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 1280
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:95  */
buffer_load_dwordx4 v[vgprG2LB+0:vgprG2LB+0+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // G -> Reg 0_0_0_0
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:96  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:97  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:98  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:99  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:100  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:101  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:102  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:103  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:104  */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:105  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:106  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:107  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:108  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:109  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:110  */
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:111  */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */

/* iter 2 (reset local read pointers iteration)  (swap local read pointers iteration)  */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:112  */
s_waitcnt vmcnt(12)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:113  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:1920 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 1920
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:114  */
/* sched write - iter 2 writesPerItem=1 */
buffer_load_dwordx4 v[vgprG2LB+4:vgprG2LB+4+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+0] offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:115  */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:116  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:117  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:118  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:119  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:120  */
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:121  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:122  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:123  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:124  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:125  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:126  */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:127  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:128  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:129  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:130  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:131  */
s_waitcnt vmcnt(12)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:132  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+16:vgprG2LB+16+3] offset:2560 // lwoB_0_0_4_0 = (0*LSCB)*(MT1J+PAD) + (4*LSPB) = 2560
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:133  */
buffer_load_dwordx4 v[vgprG2LB+8:vgprG2LB+8+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+1] offen offset:0 // G -> Reg 0_0_2_0
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:134  */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:135  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:136  */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:137  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:138  */
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:139  */
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:140  */
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:141  */
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:142  */
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:143  */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:144  */
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:145  */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:146  */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:147  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:148  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:149  */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:150  */
s_waitcnt vmcnt(12)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:151  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+20:vgprG2LB+20+3] offset:3200 // lwoB_0_0_5_0 = (0*LSCB)*(MT1J+PAD) + (5*LSPB) = 3200
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:152  */
buffer_load_dwordx4 v[vgprG2LB+12:vgprG2LB+12+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+2] offen offset:0 // G -> Reg 0_0_3_0
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:153  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:154  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:155  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:156  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:157  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:158  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:159  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:160  */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:161  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:162  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:163  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:164  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:165  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:166  */
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:167  */

/* local read swap offsets a */

/* local read swap offsets b */

/* local read init pointers a */

/* localReadInitPointers */

/* local read init pointers b */

/* localReadInitPointers */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=14 */

/* iter 3 (swap and reset local write pointers iteration)  */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:168  */
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:169  */
s_waitcnt vmcnt(12)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:170  */
/* sched write - iter 3 writesPerItem=1 */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+24:vgprG2LB+24+3] offset:3840 // lwoB_0_0_6_0 = (0*LSCB)*(MT1J+PAD) + (6*LSPB) = 3840
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:171  */
buffer_load_dwordx4 v[vgprG2LB+16:vgprG2LB+16+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+3] offen offset:0 // G -> Reg 0_0_4_0
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:172  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:173  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:174  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:175  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:176  */
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:177  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:178  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:179  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:180  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:181  */
/* sched write - iter 3 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:182  */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:183  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:184  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:185  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:186  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:187  */
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:188  */
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:189  */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:190  */
buffer_load_dwordx4 v[vgprG2LB+20:vgprG2LB+20+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+4] offen offset:0 // G -> Reg 0_0_5_0
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:191  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:192  */
/* sched write - iter 3 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:193  */

/* local write swap offsets a */

/* local write swap offsets b */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:194  */
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:195  */
s_waitcnt lgkmcnt(0)                               // 3wait for local write
// Skip force waitcnt0
s_barrier
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:196  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:197  */
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:198  */
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:2560 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:199  */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:200  */
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:5120 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=2 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:201  */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:202  */
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:7680 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=3 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:203  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:204  */
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:10240 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=4 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:205  */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:206  */
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:12800 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=5 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:207  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:208  */
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:15360 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=6 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:209  */
buffer_load_dwordx4 v[vgprG2LB+24:vgprG2LB+24+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+5] offen offset:0 // G -> Reg 0_0_6_0
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:210  */
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:17920 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=7 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:211  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:212  */
ds_read_b128 v[vgprValuB_X0_I0+32:vgprValuB_X0_I0+32+3], v[vgprLocalReadAddrB] offset:20480 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=8 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:213  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:214  */
ds_read_b128 v[vgprValuB_X0_I0+36:vgprValuB_X0_I0+36+3], v[vgprLocalReadAddrB] offset:23040 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=9 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:215  */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:216  */
ds_read_b128 v[vgprValuB_X0_I0+40:vgprValuB_X0_I0+40+3], v[vgprLocalReadAddrB] offset:25600 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=10 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:217  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:218  */
ds_read_b128 v[vgprValuB_X0_I0+44:vgprValuB_X0_I0+44+3], v[vgprLocalReadAddrB] offset:28160 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=11 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:219  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:220  */
ds_read_b128 v[vgprValuB_X0_I0+48:vgprValuB_X0_I0+48+3], v[vgprLocalReadAddrB] offset:30720 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=12 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:221  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:222  */
ds_read_b128 v[vgprValuB_X0_I0+52:vgprValuB_X0_I0+52+3], v[vgprLocalReadAddrB] offset:33280 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=13 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:223  */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=1 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */

.set vgprValuA_X0_I0, vgprValuA_X0_I0_0
.set vgprValuA_X2_I0, vgprValuA_X2_I0_0
/******************************************/
/* Unrolled Loop - End                    */
/******************************************/

/* closeLoop loopL finalLoop=1 tailLoop=0 */
s_sub_u32 s[sgprLoopCounterL], s[sgprLoopCounterL], 1 // dec counterL
s_cmp_eq_i32 s[sgprLoopCounterL], 0x2              // counterL==2
s_cbranch_scc0 label_LoopBeginL                    // restart LoopL
label_LoopEndL_even:

/* Before NLL: Check VGPR.checkin for INT8 LW */

/******************************************/
/* Ord. NoGlobalLoadLoop - Begin          */
/******************************************/

s_waitcnt vmcnt(7)

/* Begin Each Unroll: Check VGPR.checkin for INT8 LW */

/* iter 0 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
buffer_load_dwordx4 v[vgprValuA_X0_I0_1+0:vgprValuA_X0_I0_1+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_0_0
buffer_load_dwordx4 v[vgprValuA_X2_I0_1+0:vgprValuA_X2_I0_1+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+3] offen offset:0 // G -> Reg 0_0_1_0
/* global read inc B loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s80, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
s_cselect_b32 s81, s[sgprWrapUB+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s80        // gra SRD += inc(lower)
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
ds_read_b128 v[vgprValuB_X2_I0+4:vgprValuB_X2_I0+4+3], v[vgprLocalReadAddrB] offset:2624 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s81       // gra SRD += inc(upper)
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s80 // limit -= inc)
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s81 // limit -= inc)
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
ds_read_b128 v[vgprValuB_X2_I0+8:vgprValuB_X2_I0+8+3], v[vgprLocalReadAddrB] offset:5184 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=2 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
ds_read_b128 v[vgprValuB_X2_I0+12:vgprValuB_X2_I0+12+3], v[vgprLocalReadAddrB] offset:7744 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=3 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
ds_read_b128 v[vgprValuB_X2_I0+16:vgprValuB_X2_I0+16+3], v[vgprLocalReadAddrB] offset:10304 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=4 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:16  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:17  */
ds_read_b128 v[vgprValuB_X2_I0+20:vgprValuB_X2_I0+20+3], v[vgprLocalReadAddrB] offset:12864 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=5 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:18  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+24:vgprValuB_X2_I0+24+3], v[vgprLocalReadAddrB] offset:15424 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=6 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+28:vgprValuB_X2_I0+28+3], v[vgprLocalReadAddrB] offset:17984 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=7 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+32:vgprValuB_X2_I0+32+3], v[vgprLocalReadAddrB] offset:20544 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=8 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprValuA_X0_I0_1+4:vgprValuA_X0_I0_1+4+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+0] offen offset:0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprValuA_X2_I0_1+4:vgprValuA_X2_I0_1+4+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+4] offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+36:vgprValuB_X2_I0+36+3], v[vgprLocalReadAddrB] offset:23104 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=9 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+40:vgprValuB_X2_I0+40+3], v[vgprLocalReadAddrB] offset:25664 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=10 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+44:vgprValuB_X2_I0+44+3], v[vgprLocalReadAddrB] offset:28224 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=11 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:36  */
/* schedule remaining localreads for 1LDSB */
/* localReadsVacancy: latencyLeft 2 */
/* 1 LDS buffer: read-sync-write */
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:37  */
/* sched write - iter 0 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:38  */
ds_read_b128 v[vgprValuB_X2_I0+48:vgprValuB_X2_I0+48+3], v[vgprLocalReadAddrB] offset:30784 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=12 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:39  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:40  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:41  */
ds_read_b128 v[vgprValuB_X2_I0+52:vgprValuB_X2_I0+52+3], v[vgprLocalReadAddrB] offset:33344 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=13 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:42  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:43  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:44  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:45  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:46  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:47  */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:48  */
/* sched write - iter 0 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:49  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:50  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:51  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:52  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:53  */
buffer_load_dwordx4 v[vgprValuA_X0_I0_1+8:vgprValuA_X0_I0_1+8+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+1] offen offset:0 // G -> Reg 0_0_2_0
buffer_load_dwordx4 v[vgprValuA_X2_I0_1+8:vgprValuA_X2_I0_1+8+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+5] offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:54  */
s_waitcnt lgkmcnt(0)
s_barrier
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:55  */
s_waitcnt vmcnt(12)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */

/* iter 1 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:56  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:57  */
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:58  */
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:59  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:60  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:61  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:62  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:63  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:64  */
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:65  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:66  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:67  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:68  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:69  */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:70  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:71  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:72  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:73  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:74  */
s_waitcnt vmcnt(11)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:75  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:640 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 640
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:76  */
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:77  */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:78  */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:79  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:80  */
buffer_load_dwordx4 v[vgprValuA_X0_I0_1+12:vgprValuA_X0_I0_1+12+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+2] offen offset:0 // G -> Reg 0_0_3_0
buffer_load_dwordx4 v[vgprValuA_X2_I0_1+12:vgprValuA_X2_I0_1+12+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+6] offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:81  */
/* sched write - iter 1 writesPerItem=1 */
/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:82  */
s_cselect_b32 s80, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:83  */
s_cselect_b32 s81, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:84  */
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s80        // gra SRD += inc(lower)
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:85  */
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s81       // gra SRD += inc(upper)
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:86  */
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s80 // limit -= inc)
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:87  */
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s81 // limit -= inc)
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:88  */
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:89  */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:90  */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:91  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:92  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:93  */
s_waitcnt vmcnt(12)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:94  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:1280 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 1280
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:95  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:96  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:97  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:98  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:99  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:100  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:101  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:102  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:103  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:104  */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:105  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:106  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:107  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:108  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:109  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:110  */
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:111  */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */

/* iter 2 (reset local read pointers iteration)  (swap local read pointers iteration)  */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:112  */
s_waitcnt vmcnt(11)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:113  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:1920 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 1920
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:114  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:115  */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:116  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:117  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:118  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:119  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:120  */
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:121  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:122  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:123  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:124  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:125  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:126  */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:127  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:128  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:129  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:130  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:131  */
s_waitcnt vmcnt(10)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:132  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+16:vgprG2LB+16+3] offset:2560 // lwoB_0_0_4_0 = (0*LSCB)*(MT1J+PAD) + (4*LSPB) = 2560
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:133  */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:134  */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:135  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:136  */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:137  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:138  */
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:139  */
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:140  */
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:141  */
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:142  */
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:143  */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:144  */
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:145  */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:146  */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:147  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:148  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:149  */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:150  */
s_waitcnt vmcnt(9)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:151  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+20:vgprG2LB+20+3] offset:3200 // lwoB_0_0_5_0 = (0*LSCB)*(MT1J+PAD) + (5*LSPB) = 3200
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:152  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:153  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:154  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:155  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:156  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:157  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:158  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:159  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:160  */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:161  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:162  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:163  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:164  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:165  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:166  */
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:167  */

/* local read swap offsets a */

/* local read swap offsets b */

/* local read init pointers a */

/* localReadInitPointers */

/* local read init pointers b */

/* localReadInitPointers */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=14 */

/* iter 3 (swap and reset local write pointers iteration)  */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:168  */
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:169  */
s_waitcnt vmcnt(8)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:170  */
/* sched write - iter 3 writesPerItem=1 */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+24:vgprG2LB+24+3] offset:3840 // lwoB_0_0_6_0 = (0*LSCB)*(MT1J+PAD) + (6*LSPB) = 3840
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:171  */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:172  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:173  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:174  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:175  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:176  */
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:177  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:178  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:179  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:180  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:181  */
/* sched write - iter 3 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:182  */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:183  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:184  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:185  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:186  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:187  */
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:188  */
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:189  */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:190  */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:191  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:192  */
/* sched write - iter 3 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:193  */

/* local write swap offsets a */

/* local write swap offsets b */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:194  */
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:195  */
s_waitcnt lgkmcnt(0)                               // 3wait for local write
// Skip force waitcnt0
s_barrier
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:196  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:197  */
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:198  */
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:2560 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:199  */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:200  */
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:5120 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=2 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:201  */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:202  */
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:7680 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=3 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:203  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:204  */
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:10240 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=4 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:205  */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:206  */
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:12800 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=5 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:207  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:208  */
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:15360 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=6 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:209  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:210  */
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:17920 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=7 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:211  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:212  */
ds_read_b128 v[vgprValuB_X0_I0+32:vgprValuB_X0_I0+32+3], v[vgprLocalReadAddrB] offset:20480 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=8 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:213  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:214  */
ds_read_b128 v[vgprValuB_X0_I0+36:vgprValuB_X0_I0+36+3], v[vgprLocalReadAddrB] offset:23040 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=9 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:215  */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:216  */
ds_read_b128 v[vgprValuB_X0_I0+40:vgprValuB_X0_I0+40+3], v[vgprLocalReadAddrB] offset:25600 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=10 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:217  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:218  */
ds_read_b128 v[vgprValuB_X0_I0+44:vgprValuB_X0_I0+44+3], v[vgprLocalReadAddrB] offset:28160 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=11 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:219  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:220  */
ds_read_b128 v[vgprValuB_X0_I0+48:vgprValuB_X0_I0+48+3], v[vgprLocalReadAddrB] offset:30720 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=12 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:221  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:222  */
ds_read_b128 v[vgprValuB_X0_I0+52:vgprValuB_X0_I0+52+3], v[vgprLocalReadAddrB] offset:33280 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=13 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:223  */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=1 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */

.set vgprValuA_X0_I0, vgprValuA_X0_I0_1
.set vgprValuA_X2_I0, vgprValuA_X2_I0_1

/******************************************/
/* Ord. NoLoadLoop - Begin                */
/******************************************/
s_waitcnt vmcnt(0)

.set vgprValuA_X0_I0, vgprValuA_X0_I0_1
.set vgprValuA_X2_I0, vgprValuA_X2_I0_1

/* iter 0 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
ds_read_b128 v[vgprValuB_X2_I0+4:vgprValuB_X2_I0+4+3], v[vgprLocalReadAddrB] offset:2624 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
ds_read_b128 v[vgprValuB_X2_I0+8:vgprValuB_X2_I0+8+3], v[vgprLocalReadAddrB] offset:5184 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=2 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:16  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:17  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:18  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+12:vgprValuB_X2_I0+12+3], v[vgprLocalReadAddrB] offset:7744 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=3 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+16:vgprValuB_X2_I0+16+3], v[vgprLocalReadAddrB] offset:10304 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=4 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:36  */
/* schedule remaining localreads for 1LDSB */
/* localReadsVacancy: latencyLeft 2 */
/* 1 LDS buffer: read-sync-write */
ds_read_b128 v[vgprValuB_X2_I0+20:vgprValuB_X2_I0+20+3], v[vgprLocalReadAddrB] offset:12864 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=5 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:37  */
/* sched write - iter 0 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:38  */
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:39  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:40  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:41  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:42  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:43  */
ds_read_b128 v[vgprValuB_X2_I0+24:vgprValuB_X2_I0+24+3], v[vgprLocalReadAddrB] offset:15424 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=6 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:44  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:45  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:46  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:47  */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:48  */
/* sched write - iter 0 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:49  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:50  */
ds_read_b128 v[vgprValuB_X2_I0+28:vgprValuB_X2_I0+28+3], v[vgprLocalReadAddrB] offset:17984 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=7 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:51  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:52  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:53  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:54  */
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:55  */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */

/* iter 1 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:56  */
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:57  */
ds_read_b128 v[vgprValuB_X2_I0+32:vgprValuB_X2_I0+32+3], v[vgprLocalReadAddrB] offset:20544 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=8 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:58  */
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:59  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:60  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:61  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:62  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:63  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:64  */
ds_read_b128 v[vgprValuB_X2_I0+36:vgprValuB_X2_I0+36+3], v[vgprLocalReadAddrB] offset:23104 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=9 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:65  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:66  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:67  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:68  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:69  */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:70  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:71  */
ds_read_b128 v[vgprValuB_X2_I0+40:vgprValuB_X2_I0+40+3], v[vgprLocalReadAddrB] offset:25664 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=10 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:72  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:73  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:74  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:75  */
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:76  */
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:77  */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:78  */
ds_read_b128 v[vgprValuB_X2_I0+44:vgprValuB_X2_I0+44+3], v[vgprLocalReadAddrB] offset:28224 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=11 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:79  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:80  */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:81  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:82  */
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:83  */
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:84  */
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:85  */
ds_read_b128 v[vgprValuB_X2_I0+48:vgprValuB_X2_I0+48+3], v[vgprLocalReadAddrB] offset:30784 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=12 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:86  */
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:87  */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:88  */
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:89  */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:90  */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:91  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:92  */
/* sched write - iter 1 writesPerItem=1 */
ds_read_b128 v[vgprValuB_X2_I0+52:vgprValuB_X2_I0+52+3], v[vgprLocalReadAddrB] offset:33344 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=13 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:93  */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:94  */
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:95  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:96  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:97  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:98  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:99  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:100  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:101  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:102  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:103  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:104  */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:105  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:106  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:107  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:108  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:109  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:110  */
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:111  */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */

/* iter 2 (reset local read pointers iteration)  (swap local read pointers iteration)  */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:112  */
s_waitcnt lgkmcnt(0)                                // wait for global read before writing to local
s_barrier
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:113  */
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:114  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:115  */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:116  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:117  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:118  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:119  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:120  */
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:121  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:122  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:123  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:124  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:125  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:126  */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:127  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:128  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:129  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:130  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:131  */
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:132  */
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:133  */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:134  */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:135  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:136  */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:137  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:138  */
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:139  */
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:140  */
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:141  */
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:142  */
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:143  */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:144  */
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:145  */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:146  */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:147  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:148  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:149  */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:150  */
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:151  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:152  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:153  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:154  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:155  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:156  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:157  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:158  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:159  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:160  */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:161  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:162  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:163  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:164  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:165  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:166  */
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:167  */

/* local read swap offsets a */

/* local read swap offsets b */

/* local read init pointers a */

/* localReadInitPointers */

/* local read init pointers b */

/* localReadInitPointers */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=14 */

/* iter 3 (swap and reset local write pointers iteration)  */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:168  */
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:169  */
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:170  */
/* sched write - iter 3 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:171  */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:172  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:173  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:174  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:175  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:176  */
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:177  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:178  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:179  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:180  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:181  */
/* sched write - iter 3 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:182  */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:183  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:184  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:185  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:186  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:187  */
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:188  */
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:189  */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:190  */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:191  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:192  */
/* sched write - iter 3 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:193  */

/* local write swap offsets a */

/* local write swap offsets b */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:194  */
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:195  */
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:196  */
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:197  */
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:198  */
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:199  */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:200  */
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:201  */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:202  */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:203  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:204  */
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:205  */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:206  */
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:207  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:208  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:209  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:210  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:211  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:212  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:213  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:214  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:215  */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:216  */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:217  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:218  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:219  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:220  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:221  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:222  */
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:223  */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=1 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */


label_Summation_End_OptNLL:
s_cmpk_eq_u32 s[sgprBeta], 0x0                     // Beta == 0
s_cbranch_scc1 label_NoBranch_8LPIIFXSXDF65T1T_0   // Only branch on scc0
s_getpc_b64 s[80:81]                               // addr of next instr
s_add_i32 s82, label_PrefetchGlobalLastIterEnd, 0x4               // target branch offset
s_add_u32 s80, s80, s82                            // add target branch offset
s_addc_u32 s81, s81, 0                             // add high and carry
s_setpc_b64 s[80:81]                               // branch to label_PrefetchGlobalLastIterEnd
label_NoBranch_8LPIIFXSXDF65T1T_0:

s_cmp_eq_u32 s[sgprAlpha], 1.0                     // Alpha == 1.0 ?
s_cbranch_scc1 label_NoBranch_0QQ8EWJF28EFK9PQ_0   // Only branch on scc0
// branch if alpha != 1
s_getpc_b64 s[80:81]                               // addr of next instr
s_add_i32 s82, label_PrefetchGlobalLastIterEnd, 0x4               // target branch offset
s_add_u32 s80, s80, s82                            // add target branch offset
s_addc_u32 s81, s81, 0                             // add high and carry
s_setpc_b64 s[80:81]                               // branch to label_PrefetchGlobalLastIterEnd
label_NoBranch_0QQ8EWJF28EFK9PQ_0:

s_and_b32 s80, 255, s[sgprSizeI]                   // s80 = s[sgprSizeI] % 256
s_add_u32 s81, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s81                // wg0 >= nwg0-1 ?
s_cselect_b32 s80, s80, 0                          // set rMT0
s_cmpk_gt_u32 s80, 0x0                             // rMT0 > 0
s_cbranch_scc0 label_NoBranch_TKTMKZZ4I5JTTOTJ_0   // Only branch on scc1
// jump if edges required
s_getpc_b64 s[80:81]                               // addr of next instr
s_add_i32 s82, label_PrefetchGlobalLastIterEnd, 0x4               // target branch offset
s_add_u32 s80, s80, s82                            // add target branch offset
s_addc_u32 s81, s81, 0                             // add high and carry
s_setpc_b64 s[80:81]                               // branch to label_PrefetchGlobalLastIterEnd
label_NoBranch_TKTMKZZ4I5JTTOTJ_0:
s_mov_b32 s83, 0x0                                 // STATIC_DIV: divisior=224
s_mul_i32 s82, 0x249, s[sgprSizeJ]                 // tmp1 = dividend * magic hi
s_lshl_b64 s[82:83], s[82:83], 0x10                // left shift 16 bits
s_mul_i32 s81, s[sgprSizeJ], 0x2493                // tmp0 = dividend * magic lo
s_add_u32 s82, s81, s82                            // add lo
s_addc_u32 s83, s83, 0x0                           // add hi
s_lshr_b64 s[82:83], s[82:83], 0x21                // tmp1 = (dividend * magic) << shift
s_mov_b32 s81, s82                                 // quotient
s_mul_i32 s82, s81, 0xe0                           // quotient*divisor
s_sub_u32 s80, s[sgprSizeJ], s82                   // rReg = dividend - quotient*divisor
s_add_u32 s81, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s81                // wg1 >= nwg1-1
s_cselect_b32 s80, s80, 0                          // set rMT1
s_cmpk_gt_u32 s80, 0x0                             // rMT1 > 0
s_cbranch_scc0 label_NoBranch_XUR4PX4TF7WZEHOK_0   // Only branch on scc1
// jump if edges required
s_getpc_b64 s[80:81]                               // addr of next instr
s_add_i32 s82, label_PrefetchGlobalLastIterEnd, 0x4               // target branch offset
s_add_u32 s80, s80, s82                            // add target branch offset
s_addc_u32 s81, s81, 0                             // add high and carry
s_setpc_b64 s[80:81]                               // branch to label_PrefetchGlobalLastIterEnd
label_NoBranch_XUR4PX4TF7WZEHOK_0:

s_and_b32 s81, 63, s[sgprSizesSum+0]               // s81 = s[sgprSizesSum+0] % 64
s_cmp_eq_u32 s81, 0x0                              // numIterL == 0
s_cbranch_scc1 label_NoBranch_AKQRUIVWMM2MRGWZ_0   // Only branch on scc0
// skip if tail loop required
s_getpc_b64 s[80:81]                               // addr of next instr
s_add_i32 s82, label_PrefetchGlobalLastIterEnd, 0x4               // target branch offset
s_add_u32 s80, s80, s82                            // add target branch offset
s_addc_u32 s81, s81, 0                             // add high and carry
s_setpc_b64 s[80:81]                               // branch to label_PrefetchGlobalLastIterEnd
label_NoBranch_AKQRUIVWMM2MRGWZ_0:
/* endSummation: add vgpr [0...210) to pool */
/* load store sgprs */
.set sgprAddressScaleAlphaVec, 48
.set sgprAddressBias, 50
.set sgprBiasType, 52
.set sgprBiasStride, 53
.set sgpractivationAlpha, 54
.set sgpractivationBeta, 55
.set sgprActivationType, 56
/* Check if custom structure pointer is null */
s_cmp_eq_u32 s[sgprArgType], 2                     // ArgType == 2 ?
s_cbranch_scc1 label_LoadExternalEpilogueStruct    // branch if ArgType == 2
s_load_dwordx8 s[48:55], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x58
s_load_dword s56, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x78
s_branch label_LoadExternalEpilogueStructEnd
label_LoadExternalEpilogueStruct:
s_load_dwordx4 s[48:51], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x90
s_load_dwordx2 s[52:53], s[sgprKernArgAddress:sgprKernArgAddress+1], 0xa0
s_load_dwordx2 s[54:55], s[sgprKernArgAddress:sgprKernArgAddress+1], 0xb8
s_load_dword s56, s[sgprKernArgAddress:sgprKernArgAddress+1], 0xc0
label_LoadExternalEpilogueStructEnd:
.set sgprSrdScaleAlphaVec, 32
.set sgprSrdBias, 40

/* Mapping of Acc register -> C Vgpr register */
/* computeStoreVgprs */
v_lshrrev_b32 v4, 6, v[vgprSerial]                 // v4 = v[vgprSerial] / 64
v_lshrrev_b32 v5, 2, v4                            // v5 = v4 / 4
v_mul_lo_u32 v5, 0x10, v5                          // wave coordination offset 1
v_and_b32 v1, 63, v[vgprSerial]                    // v1 = v[vgprSerial] % 64
v_lshrrev_b32 v1, 4, v1                            // v1 = v1 / 16
v_lshlrev_b32 v1, 0x2, v1                          // thread0 * continuous_output
v_add_lshl_u32 v1, v5, v1, 0                       // coordination 1 = vwB *(wave_id1 + tid1)
v_mul_lo_u32 v2, v1, s[sgprStrideC1J]              //  offset 1
v_mul_lo_u32 v3, v1, s[sgprStrideD1J]              //  offset 1
v_and_b32 v0, 3, v4                                // v0 = v4 % 4
v_mul_lo_u32 v0, 0x10, v0                          // wave coordination offset 0
v_and_b32 v5, 15, v[vgprSerial]                    // v5 = v[vgprSerial] % 16
v_add_lshl_u32 v0, v5, v0, 2                       // coordination 0 = vwA * (wave_id0 + tid0)
s_mul_i32 s8, 256, s[sgprWorkGroup0]               // wgp0 * MT0
v_add_u32 v0, s8, v0                               // coord 0 = (tid0/MI_m)*4 + waveG0*MIB_m + MT0*SG0
s_mul_i32 s8, 224, s[sgprWorkGroup1]               // wgp1 * MT1
v_add_u32 v1, s8, v1                               // coord 1 = (tid0%MI_m) + waveG1*MIB_n + MT1*SG1

/******************************************/
/* Global Write Elements                  */
/******************************************/
s_waitcnt lgkmcnt(0)                               // wait for 36 bytes of kern args.
s_mov_b32 s[sgprSrdScaleAlphaVec+0], s[sgprAddressScaleAlphaVec+0] // init SRD base address (lower)
s_mov_b32 s[sgprSrdScaleAlphaVec+1], s[sgprAddressScaleAlphaVec+1] // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdScaleAlphaVec+3], Srd127_96     // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], 0 // s[AddressScaleAlphaVec] == 0 ?
s_cbranch_scc0 label_ScaleAlphaVecAddrValid        // branch if s[AddressScaleAlphaVec] != 0
s_mov_b32 s[sgprSrdScaleAlphaVec+2], 0
s_branch label_ScaleAlphaVecAddrValid_End
label_ScaleAlphaVecAddrValid:
s_mov_b32 s[sgprSrdScaleAlphaVec+2], s[sgprSizeI]
label_ScaleAlphaVecAddrValid_End:

s_mul_i32 s[sgprSrdScaleAlphaVec+2], 0x4, s[sgprSrdScaleAlphaVec+2] // ScaleAlphaVec scaled by BPE
s_add_u32 s8, s[sgprWorkGroup2], 0x1
s_mul_i32 s8, s[sgprBiasStride], s8                // stride * (wg+1)
s_cmp_eq_u32 s8, 0x0                               // bias stride = 0?
s_cselect_b32 s8, s[sgprSizeI], s8
s_mov_b32 s[sgprSrdBias+0], s[sgprAddressBias+0]   // init SRD base address (lower)
s_mov_b32 s[sgprSrdBias+1], s[sgprAddressBias+1]   // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdBias+3], Srd127_96              // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressBias:sgprAddressBias+1], 0 // s[AddressBias] == 0 ?
s_cbranch_scc0 label_BiasAddrValid                 // branch if s[AddressBias] != 0
s_mov_b32 s[sgprSrdBias+2], 0
s_branch label_BiasAddrValid_End
label_BiasAddrValid:
s_mov_b32 s[sgprSrdBias+2], s8
label_BiasAddrValid_End:

label_Load_Biasf32_0:
s_cmpk_lg_u32 s[sgprBiasType], 0                   // BiasType != 0
s_cbranch_scc1 label_Load_Biasbf16_0               // Branch if true

/******************************************/
/* Read Bias to LDS                       */
/******************************************/
s_mul_i32 s[sgprSrdBias+2], 0x4, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s8, 256, s[sgprWorkGroup0]               // wgp0 * MT0
v_add_u32 v8, s8, v[vgprSerial]                    // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s8, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v8, s8, v8                               // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v8, 0x2, v8                          // Global bias address scaled by BPE
buffer_load_dword v4, v8, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // load bias
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local bias address scaled by BPE
s_waitcnt vmcnt(0)                                 // wait for bias load
s_barrier                                          // Wait for all wavefronts
ds_write_b32 v8, v4 offset:0                       // store bias
s_branch label_Load_Bias_End                       // Branch to load bias end
label_Load_Biasbf16_0:
s_cmpk_lg_u32 s[sgprBiasType], 7                   // BiasType != 7
s_cbranch_scc1 label_Load_Bias_End                 // Branch if true

/******************************************/
/* Read Bias to LDS                       */
/******************************************/
s_mul_i32 s[sgprSrdBias+2], 0x2, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s8, 256, s[sgprWorkGroup0]               // wgp0 * MT0
v_add_u32 v8, s8, v[vgprSerial]                    // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s8, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v8, s8, v8                               // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v8, 0x1, v8                          // Global bias address scaled by BPE
buffer_load_short_d16 v4, v8, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // load bias
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local bias address scaled by BPE
s_waitcnt vmcnt(0)                                 // wait for bias load
s_barrier                                          // Wait for all wavefronts
v_lshlrev_b32 v4, 16, v4                           // cvt bf16 to fp32. 
ds_write_b32 v8, v4 offset:0                       // store bias
s_branch label_Load_Bias_End                       // Branch to load bias end
label_Load_Bias_End:
s_cmpk_eq_u32 s[sgprActivationType], 1             // activationType == 1
s_cbranch_scc1 label_To_Activation_Abs_VW4         // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 2             // activationType == 2
s_cbranch_scc1 label_To_Activation_Clippedrelu_VW4 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 3             // activationType == 3
s_cbranch_scc1 label_To_Activation_Gelu_VW4        // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 4             // activationType == 4
s_cbranch_scc1 label_To_Activation_Leakyrelu_VW4   // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 5             // activationType == 5
s_cbranch_scc1 label_To_Activation_Relu_VW4        // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 6             // activationType == 6
s_cbranch_scc1 label_To_Activation_Sigmoid_VW4     // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 7             // activationType == 7
s_cbranch_scc1 label_To_Activation_Tanh_VW4        // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 9             // activationType == 9
s_cbranch_scc1 label_To_Activation_Geluscaling_VW4 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 10            // activationType == 10
s_cbranch_scc1 label_To_Activation_Silu_VW4        // Branch if true
label_To_Activation_None_VW4:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_None_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd
label_To_Activation_Abs_VW4:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Abs_VW4, 0x4        // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd
label_To_Activation_Clippedrelu_VW4:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Clippedrelu_VW4, 0x4 // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd
label_To_Activation_Gelu_VW4:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Gelu_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd
label_To_Activation_Leakyrelu_VW4:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Leakyrelu_VW4, 0x4  // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd
label_To_Activation_Relu_VW4:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Relu_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd
label_To_Activation_Sigmoid_VW4:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Sigmoid_VW4, 0x4    // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd
label_To_Activation_Tanh_VW4:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Tanh_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd
label_To_Activation_Geluscaling_VW4:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Geluscaling_VW4, 0x4 // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd
label_To_Activation_Silu_VW4:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Silu_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd
label_ActivationSetPCAddrEnd:
label_GW_B0_E0:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=16 */
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 biasDim=0 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4); (1,0,0,0:vw4); (1,0,1,0:vw4); (1,0,2,0:vw4); (1,0,3,0:vw4); (2,0,0,0:vw4); (2,0,1,0:vw4); (2,0,2,0:vw4); (2,0,3,0:vw4); (3,0,0,0:vw4); (3,0,1,0:vw4); (3,0,2,0:vw4); (3,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s60
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[20:23], v15 offset:0                // load bias
v_lshlrev_b32 v16, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[24:27], v16, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
/* (d1,vc1,d0,vc0)=(2,0,0,0) */
/* (d1,vc1,d0,vc0)=(2,1,0,0) */
/* (d1,vc1,d0,vc0)=(2,2,0,0) */
/* (d1,vc1,d0,vc0)=(2,3,0,0) */
/* (d1,vc1,d0,vc0)=(3,0,0,0) */
/* (d1,vc1,d0,vc0)=(3,1,0,0) */
/* (d1,vc1,d0,vc0)=(3,2,0,0) */
/* (d1,vc1,d0,vc0)=(3,3,0,0) */
v_add_lshl_u32 v13, v3, v0, 0x1                    // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+28], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+29], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+30], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+31], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+40], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+41], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+42], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+43], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+44], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+45], acc20          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+46], acc24          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+47], acc28          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+48], acc17          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+49], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+50], acc25          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+51], acc29          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+52], acc18          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+53], acc22          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+54], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+55], acc30          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+56], acc19          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+57], acc23          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+58], acc27          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+59], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+60], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+61], acc36          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+62], acc40          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+63], acc44          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+64], acc33          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+65], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+66], acc41          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+67], acc45          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+68], acc34          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+69], acc38          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+70], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+71], acc46          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+72], acc35          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+73], acc39          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+74], acc43          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+75], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+76], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+77], acc52          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+78], acc56          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+79], acc60          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+80], acc49          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+81], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+82], acc57          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+83], acc61          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+84], acc50          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+85], acc54          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+86], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+87], acc62          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+88], acc51          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+89], acc55          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+90], acc59          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+91], acc63          // copy acc to vreg[63]

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v32, v4
v_mov_b32 v33, v5
v_mov_b32 v34, v6
v_mov_b32 v35, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+32], v[vgprValuC+32] // check Nan
v_bfe_u32 v9, v[vgprValuC+32], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+32], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+32], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+32], 16, v[vgprValuC+32] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_and_or_b32 v32, v[vgprValuC+33], v10, v[vgprValuC+32] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+34], v[vgprValuC+34] // check Nan
v_bfe_u32 v9, v[vgprValuC+34], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+34], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+34], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+34], 16, v[vgprValuC+34] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+35], v[vgprValuC+35] // check Nan
v_bfe_u32 v9, v[vgprValuC+35], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+35], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+35], v9, v11, s[60:61]
v_and_or_b32 v33, v[vgprValuC+35], v10, v[vgprValuC+34] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[24:25], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[26:27], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_mov_b32 v41, v5
v_mov_b32 v42, v6
v_mov_b32 v43, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+40], 16, v[vgprValuC+40] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+41], v[vgprValuC+41] // check Nan
v_bfe_u32 v9, v[vgprValuC+41], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+41], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+41], v9, v11, s[60:61]
v_and_or_b32 v40, v[vgprValuC+41], v10, v[vgprValuC+40] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+42], v[vgprValuC+42] // check Nan
v_bfe_u32 v9, v[vgprValuC+42], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+42], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+42], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+42], 16, v[vgprValuC+42] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+43], v[vgprValuC+43] // check Nan
v_bfe_u32 v9, v[vgprValuC+43], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+43], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+43], v9, v11, s[60:61]
v_and_or_b32 v41, v[vgprValuC+43], v10, v[vgprValuC+42] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[24:25], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[26:27], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_mov_b32 v45, v5
v_mov_b32 v46, v6
v_mov_b32 v47, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+44], 16, v[vgprValuC+44] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_and_or_b32 v44, v[vgprValuC+45], v10, v[vgprValuC+44] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+46], v[vgprValuC+46] // check Nan
v_bfe_u32 v9, v[vgprValuC+46], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+46], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+46], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+46], 16, v[vgprValuC+46] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+47], v[vgprValuC+47] // check Nan
v_bfe_u32 v9, v[vgprValuC+47], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+47], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+47], v9, v11, s[60:61]
v_and_or_b32 v45, v[vgprValuC+47], v10, v[vgprValuC+46] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_mov_b32 v49, v5
v_mov_b32 v50, v6
v_mov_b32 v51, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+48], 16, v[vgprValuC+48] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+49], v[vgprValuC+49] // check Nan
v_bfe_u32 v9, v[vgprValuC+49], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+49], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+49], v9, v11, s[60:61]
v_and_or_b32 v48, v[vgprValuC+49], v10, v[vgprValuC+48] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+50], 16, v[vgprValuC+50] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+51], v[vgprValuC+51] // check Nan
v_bfe_u32 v9, v[vgprValuC+51], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+51], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+51], v9, v11, s[60:61]
v_and_or_b32 v49, v[vgprValuC+51], v10, v[vgprValuC+50] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[24:25], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[26:27], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_mov_b32 v53, v5
v_mov_b32 v54, v6
v_mov_b32 v55, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+52], 16, v[vgprValuC+52] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+53], v[vgprValuC+53] // check Nan
v_bfe_u32 v9, v[vgprValuC+53], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+53], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+53], v9, v11, s[60:61]
v_and_or_b32 v52, v[vgprValuC+53], v10, v[vgprValuC+52] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+54], v[vgprValuC+54] // check Nan
v_bfe_u32 v9, v[vgprValuC+54], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+54], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+54], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+54], 16, v[vgprValuC+54] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_and_or_b32 v53, v[vgprValuC+55], v10, v[vgprValuC+54] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[52:53], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[24:25], v[vgprValuC+56:vgprValuC+56+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[26:27], v[vgprValuC+58:vgprValuC+58+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+58:vgprValuC+58+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_mov_b32 v57, v5
v_mov_b32 v58, v6
v_mov_b32 v59, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+56], 16, v[vgprValuC+56] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+57], v[vgprValuC+57] // check Nan
v_bfe_u32 v9, v[vgprValuC+57], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+57], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+57], v9, v11, s[60:61]
v_and_or_b32 v56, v[vgprValuC+57], v10, v[vgprValuC+56] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+58], v[vgprValuC+58] // check Nan
v_bfe_u32 v9, v[vgprValuC+58], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+58], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+58], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+58], 16, v[vgprValuC+58] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+59], v[vgprValuC+59] // check Nan
v_bfe_u32 v9, v[vgprValuC+59], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+59], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+59], v9, v11, s[60:61]
v_and_or_b32 v57, v[vgprValuC+59], v10, v[vgprValuC+58] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[56:57], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[24:25], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[26:27], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_mov_b32 v61, v5
v_mov_b32 v62, v6
v_mov_b32 v63, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+60], 16, v[vgprValuC+60] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+61], v[vgprValuC+61] // check Nan
v_bfe_u32 v9, v[vgprValuC+61], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+61], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+61], v9, v11, s[60:61]
v_and_or_b32 v60, v[vgprValuC+61], v10, v[vgprValuC+60] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+62], v[vgprValuC+62] // check Nan
v_bfe_u32 v9, v[vgprValuC+62], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+62], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+62], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+62], 16, v[vgprValuC+62] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+63], v[vgprValuC+63] // check Nan
v_bfe_u32 v9, v[vgprValuC+63], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+63], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+63], v9, v11, s[60:61]
v_and_or_b32 v61, v[vgprValuC+63], v10, v[vgprValuC+62] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_mov_b32 v65, v5
v_mov_b32 v66, v6
v_mov_b32 v67, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+64], 16, v[vgprValuC+64] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_and_or_b32 v64, v[vgprValuC+65], v10, v[vgprValuC+64] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+66], v[vgprValuC+66] // check Nan
v_bfe_u32 v9, v[vgprValuC+66], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+66], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+66], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+66], 16, v[vgprValuC+66] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+67], v[vgprValuC+67] // check Nan
v_bfe_u32 v9, v[vgprValuC+67], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+67], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+67], v9, v11, s[60:61]
v_and_or_b32 v65, v[vgprValuC+67], v10, v[vgprValuC+66] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[64:65], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[24:25], v[vgprValuC+68:vgprValuC+68+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[26:27], v[vgprValuC+70:vgprValuC+70+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+70:vgprValuC+70+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_mov_b32 v69, v5
v_mov_b32 v70, v6
v_mov_b32 v71, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+68], 16, v[vgprValuC+68] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+69], v[vgprValuC+69] // check Nan
v_bfe_u32 v9, v[vgprValuC+69], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+69], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+69], v9, v11, s[60:61]
v_and_or_b32 v68, v[vgprValuC+69], v10, v[vgprValuC+68] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+70], 16, v[vgprValuC+70] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+71], v[vgprValuC+71] // check Nan
v_bfe_u32 v9, v[vgprValuC+71], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+71], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+71], v9, v11, s[60:61]
v_and_or_b32 v69, v[vgprValuC+71], v10, v[vgprValuC+70] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+74:vgprValuC+74+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_mov_b32 v73, v5
v_mov_b32 v74, v6
v_mov_b32 v75, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+72], 16, v[vgprValuC+72] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+73], v[vgprValuC+73] // check Nan
v_bfe_u32 v9, v[vgprValuC+73], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+73], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+73], v9, v11, s[60:61]
v_and_or_b32 v72, v[vgprValuC+73], v10, v[vgprValuC+72] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+74], v[vgprValuC+74] // check Nan
v_bfe_u32 v9, v[vgprValuC+74], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+74], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+74], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+74], 16, v[vgprValuC+74] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_and_or_b32 v73, v[vgprValuC+75], v10, v[vgprValuC+74] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[72:73], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[24:25], v[vgprValuC+76:vgprValuC+76+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[26:27], v[vgprValuC+78:vgprValuC+78+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+78:vgprValuC+78+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_mov_b32 v77, v5
v_mov_b32 v78, v6
v_mov_b32 v79, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+76], 16, v[vgprValuC+76] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+77], v[vgprValuC+77] // check Nan
v_bfe_u32 v9, v[vgprValuC+77], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+77], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+77], v9, v11, s[60:61]
v_and_or_b32 v76, v[vgprValuC+77], v10, v[vgprValuC+76] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+78], v[vgprValuC+78] // check Nan
v_bfe_u32 v9, v[vgprValuC+78], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+78], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+78], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+78], 16, v[vgprValuC+78] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+79], v[vgprValuC+79] // check Nan
v_bfe_u32 v9, v[vgprValuC+79], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+79], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+79], v9, v11, s[60:61]
v_and_or_b32 v77, v[vgprValuC+79], v10, v[vgprValuC+78] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_mov_b32 v81, v5
v_mov_b32 v82, v6
v_mov_b32 v83, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+80], 16, v[vgprValuC+80] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+81], v[vgprValuC+81] // check Nan
v_bfe_u32 v9, v[vgprValuC+81], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+81], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+81], v9, v11, s[60:61]
v_and_or_b32 v80, v[vgprValuC+81], v10, v[vgprValuC+80] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+82], v[vgprValuC+82] // check Nan
v_bfe_u32 v9, v[vgprValuC+82], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+82], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+82], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+82], 16, v[vgprValuC+82] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+83], v[vgprValuC+83] // check Nan
v_bfe_u32 v9, v[vgprValuC+83], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+83], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+83], v9, v11, s[60:61]
v_and_or_b32 v81, v[vgprValuC+83], v10, v[vgprValuC+82] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[80:81], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[24:25], v[vgprValuC+84:vgprValuC+84+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[26:27], v[vgprValuC+86:vgprValuC+86+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+86:vgprValuC+86+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_mov_b32 v85, v5
v_mov_b32 v86, v6
v_mov_b32 v87, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+84], 16, v[vgprValuC+84] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_and_or_b32 v84, v[vgprValuC+85], v10, v[vgprValuC+84] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+86], v[vgprValuC+86] // check Nan
v_bfe_u32 v9, v[vgprValuC+86], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+86], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+86], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+86], 16, v[vgprValuC+86] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+87], v[vgprValuC+87] // check Nan
v_bfe_u32 v9, v[vgprValuC+87], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+87], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+87], v9, v11, s[60:61]
v_and_or_b32 v85, v[vgprValuC+87], v10, v[vgprValuC+86] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[84:85], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[24:25], v[vgprValuC+88:vgprValuC+88+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[26:27], v[vgprValuC+90:vgprValuC+90+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+90:vgprValuC+90+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v88, v4
v_mov_b32 v89, v5
v_mov_b32 v90, v6
v_mov_b32 v91, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+88], v[vgprValuC+88] // check Nan
v_bfe_u32 v9, v[vgprValuC+88], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+88], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+88], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+88], 16, v[vgprValuC+88] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+89], v[vgprValuC+89] // check Nan
v_bfe_u32 v9, v[vgprValuC+89], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+89], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+89], v9, v11, s[60:61]
v_and_or_b32 v88, v[vgprValuC+89], v10, v[vgprValuC+88] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+90], 16, v[vgprValuC+90] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+91], v[vgprValuC+91] // check Nan
v_bfe_u32 v9, v[vgprValuC+91], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+91], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+91], v9, v11, s[60:61]
v_and_or_b32 v89, v[vgprValuC+91], v10, v[vgprValuC+90] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[88:89], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 biasDim=0 */

/******************************************/
/* Global Write Batch #1 (d1,d0,vc1,vc0) = */
/*    (4,0,0,0:vw4); (4,0,1,0:vw4); (4,0,2,0:vw4); (4,0,3,0:vw4); (5,0,0,0:vw4); (5,0,1,0:vw4); (5,0,2,0:vw4); (5,0,3,0:vw4); (6,0,0,0:vw4); (6,0,1,0:vw4); (6,0,2,0:vw4); (6,0,3,0:vw4); (7,0,0,0:vw4); (7,0,1,0:vw4); (7,0,2,0:vw4); (7,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(4,0,0,0) */
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s60
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
ds_read_b128 v[20:23], v15 offset:0                // load bias
buffer_load_dwordx4 v[24:27], v16, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(4,1,0,0) */
/* (d1,vc1,d0,vc0)=(4,2,0,0) */
/* (d1,vc1,d0,vc0)=(4,3,0,0) */
/* (d1,vc1,d0,vc0)=(5,0,0,0) */
/* (d1,vc1,d0,vc0)=(5,1,0,0) */
/* (d1,vc1,d0,vc0)=(5,2,0,0) */
/* (d1,vc1,d0,vc0)=(5,3,0,0) */
/* (d1,vc1,d0,vc0)=(6,0,0,0) */
/* (d1,vc1,d0,vc0)=(6,1,0,0) */
/* (d1,vc1,d0,vc0)=(6,2,0,0) */
/* (d1,vc1,d0,vc0)=(6,3,0,0) */
/* (d1,vc1,d0,vc0)=(7,0,0,0) */
/* (d1,vc1,d0,vc0)=(7,1,0,0) */
/* (d1,vc1,d0,vc0)=(7,2,0,0) */
/* (d1,vc1,d0,vc0)=(7,3,0,0) */
v_accvgpr_read_b32 v[vgprValuC+28], acc64          // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+29], acc68          // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+30], acc72          // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+31], acc76          // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+32], acc65          // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+33], acc69          // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+34], acc73          // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+35], acc77          // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+36], acc66          // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+37], acc70          // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+38], acc74          // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+39], acc78          // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+40], acc67          // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+41], acc71          // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+42], acc75          // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+43], acc79          // copy acc to vreg[79]
v_accvgpr_read_b32 v[vgprValuC+44], acc80          // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+45], acc84          // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+46], acc88          // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+47], acc92          // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+48], acc81          // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+49], acc85          // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+50], acc89          // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+51], acc93          // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+52], acc82          // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+53], acc86          // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+54], acc90          // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+55], acc94          // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+56], acc83          // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+57], acc87          // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+58], acc91          // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+59], acc95          // copy acc to vreg[95]
v_accvgpr_read_b32 v[vgprValuC+60], acc96          // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+61], acc100         // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+62], acc104         // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+63], acc108         // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+64], acc97          // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+65], acc101         // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+66], acc105         // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+67], acc109         // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+68], acc98          // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+69], acc102         // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+70], acc106         // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+71], acc110         // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+72], acc99          // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+73], acc103         // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+74], acc107         // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+75], acc111         // copy acc to vreg[111]
v_accvgpr_read_b32 v[vgprValuC+76], acc112         // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+77], acc116         // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+78], acc120         // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+79], acc124         // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+80], acc113         // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+81], acc117         // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+82], acc121         // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+83], acc125         // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+84], acc114         // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+85], acc118         // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+86], acc122         // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+87], acc126         // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+88], acc115         // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+89], acc119         // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+90], acc123         // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+91], acc127         // copy acc to vreg[127]

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v32, v4
v_mov_b32 v33, v5
v_mov_b32 v34, v6
v_mov_b32 v35, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+32], v[vgprValuC+32] // check Nan
v_bfe_u32 v9, v[vgprValuC+32], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+32], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+32], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+32], 16, v[vgprValuC+32] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_and_or_b32 v32, v[vgprValuC+33], v10, v[vgprValuC+32] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+34], v[vgprValuC+34] // check Nan
v_bfe_u32 v9, v[vgprValuC+34], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+34], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+34], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+34], 16, v[vgprValuC+34] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+35], v[vgprValuC+35] // check Nan
v_bfe_u32 v9, v[vgprValuC+35], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+35], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+35], v9, v11, s[60:61]
v_and_or_b32 v33, v[vgprValuC+35], v10, v[vgprValuC+34] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[24:25], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[26:27], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_mov_b32 v41, v5
v_mov_b32 v42, v6
v_mov_b32 v43, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+40], 16, v[vgprValuC+40] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+41], v[vgprValuC+41] // check Nan
v_bfe_u32 v9, v[vgprValuC+41], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+41], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+41], v9, v11, s[60:61]
v_and_or_b32 v40, v[vgprValuC+41], v10, v[vgprValuC+40] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+42], v[vgprValuC+42] // check Nan
v_bfe_u32 v9, v[vgprValuC+42], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+42], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+42], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+42], 16, v[vgprValuC+42] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+43], v[vgprValuC+43] // check Nan
v_bfe_u32 v9, v[vgprValuC+43], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+43], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+43], v9, v11, s[60:61]
v_and_or_b32 v41, v[vgprValuC+43], v10, v[vgprValuC+42] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[24:25], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[26:27], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_mov_b32 v45, v5
v_mov_b32 v46, v6
v_mov_b32 v47, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+44], 16, v[vgprValuC+44] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_and_or_b32 v44, v[vgprValuC+45], v10, v[vgprValuC+44] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+46], v[vgprValuC+46] // check Nan
v_bfe_u32 v9, v[vgprValuC+46], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+46], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+46], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+46], 16, v[vgprValuC+46] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+47], v[vgprValuC+47] // check Nan
v_bfe_u32 v9, v[vgprValuC+47], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+47], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+47], v9, v11, s[60:61]
v_and_or_b32 v45, v[vgprValuC+47], v10, v[vgprValuC+46] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_mov_b32 v49, v5
v_mov_b32 v50, v6
v_mov_b32 v51, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+48], 16, v[vgprValuC+48] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+49], v[vgprValuC+49] // check Nan
v_bfe_u32 v9, v[vgprValuC+49], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+49], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+49], v9, v11, s[60:61]
v_and_or_b32 v48, v[vgprValuC+49], v10, v[vgprValuC+48] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+50], 16, v[vgprValuC+50] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+51], v[vgprValuC+51] // check Nan
v_bfe_u32 v9, v[vgprValuC+51], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+51], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+51], v9, v11, s[60:61]
v_and_or_b32 v49, v[vgprValuC+51], v10, v[vgprValuC+50] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[24:25], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[26:27], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_mov_b32 v53, v5
v_mov_b32 v54, v6
v_mov_b32 v55, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+52], 16, v[vgprValuC+52] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+53], v[vgprValuC+53] // check Nan
v_bfe_u32 v9, v[vgprValuC+53], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+53], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+53], v9, v11, s[60:61]
v_and_or_b32 v52, v[vgprValuC+53], v10, v[vgprValuC+52] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+54], v[vgprValuC+54] // check Nan
v_bfe_u32 v9, v[vgprValuC+54], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+54], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+54], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+54], 16, v[vgprValuC+54] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_and_or_b32 v53, v[vgprValuC+55], v10, v[vgprValuC+54] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[52:53], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[24:25], v[vgprValuC+56:vgprValuC+56+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[26:27], v[vgprValuC+58:vgprValuC+58+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+58:vgprValuC+58+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_mov_b32 v57, v5
v_mov_b32 v58, v6
v_mov_b32 v59, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+56], 16, v[vgprValuC+56] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+57], v[vgprValuC+57] // check Nan
v_bfe_u32 v9, v[vgprValuC+57], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+57], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+57], v9, v11, s[60:61]
v_and_or_b32 v56, v[vgprValuC+57], v10, v[vgprValuC+56] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+58], v[vgprValuC+58] // check Nan
v_bfe_u32 v9, v[vgprValuC+58], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+58], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+58], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+58], 16, v[vgprValuC+58] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+59], v[vgprValuC+59] // check Nan
v_bfe_u32 v9, v[vgprValuC+59], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+59], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+59], v9, v11, s[60:61]
v_and_or_b32 v57, v[vgprValuC+59], v10, v[vgprValuC+58] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[56:57], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[24:25], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[26:27], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_mov_b32 v61, v5
v_mov_b32 v62, v6
v_mov_b32 v63, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+60], 16, v[vgprValuC+60] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+61], v[vgprValuC+61] // check Nan
v_bfe_u32 v9, v[vgprValuC+61], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+61], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+61], v9, v11, s[60:61]
v_and_or_b32 v60, v[vgprValuC+61], v10, v[vgprValuC+60] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+62], v[vgprValuC+62] // check Nan
v_bfe_u32 v9, v[vgprValuC+62], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+62], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+62], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+62], 16, v[vgprValuC+62] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+63], v[vgprValuC+63] // check Nan
v_bfe_u32 v9, v[vgprValuC+63], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+63], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+63], v9, v11, s[60:61]
v_and_or_b32 v61, v[vgprValuC+63], v10, v[vgprValuC+62] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_mov_b32 v65, v5
v_mov_b32 v66, v6
v_mov_b32 v67, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+64], 16, v[vgprValuC+64] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_and_or_b32 v64, v[vgprValuC+65], v10, v[vgprValuC+64] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+66], v[vgprValuC+66] // check Nan
v_bfe_u32 v9, v[vgprValuC+66], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+66], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+66], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+66], 16, v[vgprValuC+66] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+67], v[vgprValuC+67] // check Nan
v_bfe_u32 v9, v[vgprValuC+67], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+67], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+67], v9, v11, s[60:61]
v_and_or_b32 v65, v[vgprValuC+67], v10, v[vgprValuC+66] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[64:65], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[24:25], v[vgprValuC+68:vgprValuC+68+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[26:27], v[vgprValuC+70:vgprValuC+70+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+70:vgprValuC+70+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_mov_b32 v69, v5
v_mov_b32 v70, v6
v_mov_b32 v71, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+68], 16, v[vgprValuC+68] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+69], v[vgprValuC+69] // check Nan
v_bfe_u32 v9, v[vgprValuC+69], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+69], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+69], v9, v11, s[60:61]
v_and_or_b32 v68, v[vgprValuC+69], v10, v[vgprValuC+68] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+70], 16, v[vgprValuC+70] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+71], v[vgprValuC+71] // check Nan
v_bfe_u32 v9, v[vgprValuC+71], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+71], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+71], v9, v11, s[60:61]
v_and_or_b32 v69, v[vgprValuC+71], v10, v[vgprValuC+70] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+74:vgprValuC+74+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_mov_b32 v73, v5
v_mov_b32 v74, v6
v_mov_b32 v75, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+72], 16, v[vgprValuC+72] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+73], v[vgprValuC+73] // check Nan
v_bfe_u32 v9, v[vgprValuC+73], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+73], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+73], v9, v11, s[60:61]
v_and_or_b32 v72, v[vgprValuC+73], v10, v[vgprValuC+72] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+74], v[vgprValuC+74] // check Nan
v_bfe_u32 v9, v[vgprValuC+74], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+74], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+74], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+74], 16, v[vgprValuC+74] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_and_or_b32 v73, v[vgprValuC+75], v10, v[vgprValuC+74] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[72:73], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[24:25], v[vgprValuC+76:vgprValuC+76+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[26:27], v[vgprValuC+78:vgprValuC+78+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+78:vgprValuC+78+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_mov_b32 v77, v5
v_mov_b32 v78, v6
v_mov_b32 v79, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+76], 16, v[vgprValuC+76] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+77], v[vgprValuC+77] // check Nan
v_bfe_u32 v9, v[vgprValuC+77], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+77], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+77], v9, v11, s[60:61]
v_and_or_b32 v76, v[vgprValuC+77], v10, v[vgprValuC+76] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+78], v[vgprValuC+78] // check Nan
v_bfe_u32 v9, v[vgprValuC+78], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+78], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+78], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+78], 16, v[vgprValuC+78] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+79], v[vgprValuC+79] // check Nan
v_bfe_u32 v9, v[vgprValuC+79], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+79], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+79], v9, v11, s[60:61]
v_and_or_b32 v77, v[vgprValuC+79], v10, v[vgprValuC+78] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_mov_b32 v81, v5
v_mov_b32 v82, v6
v_mov_b32 v83, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+80], 16, v[vgprValuC+80] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+81], v[vgprValuC+81] // check Nan
v_bfe_u32 v9, v[vgprValuC+81], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+81], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+81], v9, v11, s[60:61]
v_and_or_b32 v80, v[vgprValuC+81], v10, v[vgprValuC+80] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+82], v[vgprValuC+82] // check Nan
v_bfe_u32 v9, v[vgprValuC+82], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+82], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+82], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+82], 16, v[vgprValuC+82] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+83], v[vgprValuC+83] // check Nan
v_bfe_u32 v9, v[vgprValuC+83], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+83], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+83], v9, v11, s[60:61]
v_and_or_b32 v81, v[vgprValuC+83], v10, v[vgprValuC+82] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[80:81], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[24:25], v[vgprValuC+84:vgprValuC+84+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[26:27], v[vgprValuC+86:vgprValuC+86+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+86:vgprValuC+86+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_mov_b32 v85, v5
v_mov_b32 v86, v6
v_mov_b32 v87, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+84], 16, v[vgprValuC+84] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_and_or_b32 v84, v[vgprValuC+85], v10, v[vgprValuC+84] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+86], v[vgprValuC+86] // check Nan
v_bfe_u32 v9, v[vgprValuC+86], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+86], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+86], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+86], 16, v[vgprValuC+86] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+87], v[vgprValuC+87] // check Nan
v_bfe_u32 v9, v[vgprValuC+87], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+87], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+87], v9, v11, s[60:61]
v_and_or_b32 v85, v[vgprValuC+87], v10, v[vgprValuC+86] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[84:85], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[24:25], v[vgprValuC+88:vgprValuC+88+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[26:27], v[vgprValuC+90:vgprValuC+90+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+90:vgprValuC+90+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v88, v4
v_mov_b32 v89, v5
v_mov_b32 v90, v6
v_mov_b32 v91, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+88], v[vgprValuC+88] // check Nan
v_bfe_u32 v9, v[vgprValuC+88], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+88], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+88], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+88], 16, v[vgprValuC+88] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+89], v[vgprValuC+89] // check Nan
v_bfe_u32 v9, v[vgprValuC+89], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+89], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+89], v9, v11, s[60:61]
v_and_or_b32 v88, v[vgprValuC+89], v10, v[vgprValuC+88] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+90], 16, v[vgprValuC+90] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+91], v[vgprValuC+91] // check Nan
v_bfe_u32 v9, v[vgprValuC+91], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+91], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+91], v9, v11, s[60:61]
v_and_or_b32 v89, v[vgprValuC+91], v10, v[vgprValuC+90] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[88:89], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 biasDim=0 */

/******************************************/
/* Global Write Batch #2 (d1,d0,vc1,vc0) = */
/*    (8,0,0,0:vw4); (8,0,1,0:vw4); (8,0,2,0:vw4); (8,0,3,0:vw4); (9,0,0,0:vw4); (9,0,1,0:vw4); (9,0,2,0:vw4); (9,0,3,0:vw4); (10,0,0,0:vw4); (10,0,1,0:vw4); (10,0,2,0:vw4); (10,0,3,0:vw4); (11,0,0,0:vw4); (11,0,1,0:vw4); (11,0,2,0:vw4); (11,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(8,0,0,0) */
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s60
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
ds_read_b128 v[20:23], v15 offset:0                // load bias
buffer_load_dwordx4 v[24:27], v16, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(8,1,0,0) */
/* (d1,vc1,d0,vc0)=(8,2,0,0) */
/* (d1,vc1,d0,vc0)=(8,3,0,0) */
/* (d1,vc1,d0,vc0)=(9,0,0,0) */
/* (d1,vc1,d0,vc0)=(9,1,0,0) */
/* (d1,vc1,d0,vc0)=(9,2,0,0) */
/* (d1,vc1,d0,vc0)=(9,3,0,0) */
/* (d1,vc1,d0,vc0)=(10,0,0,0) */
/* (d1,vc1,d0,vc0)=(10,1,0,0) */
/* (d1,vc1,d0,vc0)=(10,2,0,0) */
/* (d1,vc1,d0,vc0)=(10,3,0,0) */
/* (d1,vc1,d0,vc0)=(11,0,0,0) */
/* (d1,vc1,d0,vc0)=(11,1,0,0) */
/* (d1,vc1,d0,vc0)=(11,2,0,0) */
/* (d1,vc1,d0,vc0)=(11,3,0,0) */
v_accvgpr_read_b32 v[vgprValuC+28], acc128         // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+29], acc132         // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+30], acc136         // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+31], acc140         // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+32], acc129         // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+33], acc133         // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+34], acc137         // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+35], acc141         // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+36], acc130         // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+37], acc134         // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+38], acc138         // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+39], acc142         // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+40], acc131         // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+41], acc135         // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+42], acc139         // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+43], acc143         // copy acc to vreg[143]
v_accvgpr_read_b32 v[vgprValuC+44], acc144         // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+45], acc148         // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+46], acc152         // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+47], acc156         // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+48], acc145         // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+49], acc149         // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+50], acc153         // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+51], acc157         // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+52], acc146         // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+53], acc150         // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+54], acc154         // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+55], acc158         // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+56], acc147         // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+57], acc151         // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+58], acc155         // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+59], acc159         // copy acc to vreg[159]
v_accvgpr_read_b32 v[vgprValuC+60], acc160         // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+61], acc164         // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+62], acc168         // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+63], acc172         // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+64], acc161         // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+65], acc165         // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+66], acc169         // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+67], acc173         // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+68], acc162         // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+69], acc166         // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+70], acc170         // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+71], acc174         // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+72], acc163         // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+73], acc167         // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+74], acc171         // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+75], acc175         // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+76], acc176         // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+77], acc180         // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+78], acc184         // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+79], acc188         // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+80], acc177         // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+81], acc181         // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+82], acc185         // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+83], acc189         // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+84], acc178         // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+85], acc182         // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+86], acc186         // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+87], acc190         // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+88], acc179         // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+89], acc183         // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+90], acc187         // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+91], acc191         // copy acc to vreg[191]

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v32, v4
v_mov_b32 v33, v5
v_mov_b32 v34, v6
v_mov_b32 v35, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+32], v[vgprValuC+32] // check Nan
v_bfe_u32 v9, v[vgprValuC+32], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+32], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+32], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+32], 16, v[vgprValuC+32] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_and_or_b32 v32, v[vgprValuC+33], v10, v[vgprValuC+32] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+34], v[vgprValuC+34] // check Nan
v_bfe_u32 v9, v[vgprValuC+34], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+34], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+34], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+34], 16, v[vgprValuC+34] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+35], v[vgprValuC+35] // check Nan
v_bfe_u32 v9, v[vgprValuC+35], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+35], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+35], v9, v11, s[60:61]
v_and_or_b32 v33, v[vgprValuC+35], v10, v[vgprValuC+34] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[24:25], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[26:27], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_mov_b32 v41, v5
v_mov_b32 v42, v6
v_mov_b32 v43, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+40], 16, v[vgprValuC+40] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+41], v[vgprValuC+41] // check Nan
v_bfe_u32 v9, v[vgprValuC+41], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+41], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+41], v9, v11, s[60:61]
v_and_or_b32 v40, v[vgprValuC+41], v10, v[vgprValuC+40] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+42], v[vgprValuC+42] // check Nan
v_bfe_u32 v9, v[vgprValuC+42], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+42], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+42], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+42], 16, v[vgprValuC+42] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+43], v[vgprValuC+43] // check Nan
v_bfe_u32 v9, v[vgprValuC+43], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+43], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+43], v9, v11, s[60:61]
v_and_or_b32 v41, v[vgprValuC+43], v10, v[vgprValuC+42] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[24:25], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[26:27], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_mov_b32 v45, v5
v_mov_b32 v46, v6
v_mov_b32 v47, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+44], 16, v[vgprValuC+44] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_and_or_b32 v44, v[vgprValuC+45], v10, v[vgprValuC+44] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+46], v[vgprValuC+46] // check Nan
v_bfe_u32 v9, v[vgprValuC+46], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+46], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+46], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+46], 16, v[vgprValuC+46] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+47], v[vgprValuC+47] // check Nan
v_bfe_u32 v9, v[vgprValuC+47], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+47], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+47], v9, v11, s[60:61]
v_and_or_b32 v45, v[vgprValuC+47], v10, v[vgprValuC+46] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_mov_b32 v49, v5
v_mov_b32 v50, v6
v_mov_b32 v51, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+48], 16, v[vgprValuC+48] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+49], v[vgprValuC+49] // check Nan
v_bfe_u32 v9, v[vgprValuC+49], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+49], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+49], v9, v11, s[60:61]
v_and_or_b32 v48, v[vgprValuC+49], v10, v[vgprValuC+48] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+50], 16, v[vgprValuC+50] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+51], v[vgprValuC+51] // check Nan
v_bfe_u32 v9, v[vgprValuC+51], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+51], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+51], v9, v11, s[60:61]
v_and_or_b32 v49, v[vgprValuC+51], v10, v[vgprValuC+50] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[24:25], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[26:27], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_mov_b32 v53, v5
v_mov_b32 v54, v6
v_mov_b32 v55, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+52], 16, v[vgprValuC+52] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+53], v[vgprValuC+53] // check Nan
v_bfe_u32 v9, v[vgprValuC+53], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+53], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+53], v9, v11, s[60:61]
v_and_or_b32 v52, v[vgprValuC+53], v10, v[vgprValuC+52] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+54], v[vgprValuC+54] // check Nan
v_bfe_u32 v9, v[vgprValuC+54], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+54], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+54], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+54], 16, v[vgprValuC+54] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_and_or_b32 v53, v[vgprValuC+55], v10, v[vgprValuC+54] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[52:53], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[24:25], v[vgprValuC+56:vgprValuC+56+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[26:27], v[vgprValuC+58:vgprValuC+58+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+58:vgprValuC+58+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_mov_b32 v57, v5
v_mov_b32 v58, v6
v_mov_b32 v59, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+56], 16, v[vgprValuC+56] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+57], v[vgprValuC+57] // check Nan
v_bfe_u32 v9, v[vgprValuC+57], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+57], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+57], v9, v11, s[60:61]
v_and_or_b32 v56, v[vgprValuC+57], v10, v[vgprValuC+56] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+58], v[vgprValuC+58] // check Nan
v_bfe_u32 v9, v[vgprValuC+58], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+58], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+58], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+58], 16, v[vgprValuC+58] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+59], v[vgprValuC+59] // check Nan
v_bfe_u32 v9, v[vgprValuC+59], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+59], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+59], v9, v11, s[60:61]
v_and_or_b32 v57, v[vgprValuC+59], v10, v[vgprValuC+58] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[56:57], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[24:25], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[26:27], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_mov_b32 v61, v5
v_mov_b32 v62, v6
v_mov_b32 v63, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+60], 16, v[vgprValuC+60] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+61], v[vgprValuC+61] // check Nan
v_bfe_u32 v9, v[vgprValuC+61], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+61], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+61], v9, v11, s[60:61]
v_and_or_b32 v60, v[vgprValuC+61], v10, v[vgprValuC+60] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+62], v[vgprValuC+62] // check Nan
v_bfe_u32 v9, v[vgprValuC+62], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+62], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+62], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+62], 16, v[vgprValuC+62] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+63], v[vgprValuC+63] // check Nan
v_bfe_u32 v9, v[vgprValuC+63], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+63], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+63], v9, v11, s[60:61]
v_and_or_b32 v61, v[vgprValuC+63], v10, v[vgprValuC+62] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_mov_b32 v65, v5
v_mov_b32 v66, v6
v_mov_b32 v67, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+64], 16, v[vgprValuC+64] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_and_or_b32 v64, v[vgprValuC+65], v10, v[vgprValuC+64] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+66], v[vgprValuC+66] // check Nan
v_bfe_u32 v9, v[vgprValuC+66], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+66], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+66], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+66], 16, v[vgprValuC+66] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+67], v[vgprValuC+67] // check Nan
v_bfe_u32 v9, v[vgprValuC+67], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+67], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+67], v9, v11, s[60:61]
v_and_or_b32 v65, v[vgprValuC+67], v10, v[vgprValuC+66] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[64:65], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[24:25], v[vgprValuC+68:vgprValuC+68+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[26:27], v[vgprValuC+70:vgprValuC+70+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+70:vgprValuC+70+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_mov_b32 v69, v5
v_mov_b32 v70, v6
v_mov_b32 v71, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+68], 16, v[vgprValuC+68] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+69], v[vgprValuC+69] // check Nan
v_bfe_u32 v9, v[vgprValuC+69], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+69], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+69], v9, v11, s[60:61]
v_and_or_b32 v68, v[vgprValuC+69], v10, v[vgprValuC+68] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+70], 16, v[vgprValuC+70] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+71], v[vgprValuC+71] // check Nan
v_bfe_u32 v9, v[vgprValuC+71], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+71], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+71], v9, v11, s[60:61]
v_and_or_b32 v69, v[vgprValuC+71], v10, v[vgprValuC+70] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+74:vgprValuC+74+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_mov_b32 v73, v5
v_mov_b32 v74, v6
v_mov_b32 v75, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+72], 16, v[vgprValuC+72] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+73], v[vgprValuC+73] // check Nan
v_bfe_u32 v9, v[vgprValuC+73], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+73], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+73], v9, v11, s[60:61]
v_and_or_b32 v72, v[vgprValuC+73], v10, v[vgprValuC+72] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+74], v[vgprValuC+74] // check Nan
v_bfe_u32 v9, v[vgprValuC+74], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+74], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+74], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+74], 16, v[vgprValuC+74] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_and_or_b32 v73, v[vgprValuC+75], v10, v[vgprValuC+74] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[72:73], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[24:25], v[vgprValuC+76:vgprValuC+76+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[26:27], v[vgprValuC+78:vgprValuC+78+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+78:vgprValuC+78+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_mov_b32 v77, v5
v_mov_b32 v78, v6
v_mov_b32 v79, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+76], 16, v[vgprValuC+76] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+77], v[vgprValuC+77] // check Nan
v_bfe_u32 v9, v[vgprValuC+77], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+77], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+77], v9, v11, s[60:61]
v_and_or_b32 v76, v[vgprValuC+77], v10, v[vgprValuC+76] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+78], v[vgprValuC+78] // check Nan
v_bfe_u32 v9, v[vgprValuC+78], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+78], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+78], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+78], 16, v[vgprValuC+78] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+79], v[vgprValuC+79] // check Nan
v_bfe_u32 v9, v[vgprValuC+79], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+79], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+79], v9, v11, s[60:61]
v_and_or_b32 v77, v[vgprValuC+79], v10, v[vgprValuC+78] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_mov_b32 v81, v5
v_mov_b32 v82, v6
v_mov_b32 v83, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+80], 16, v[vgprValuC+80] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+81], v[vgprValuC+81] // check Nan
v_bfe_u32 v9, v[vgprValuC+81], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+81], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+81], v9, v11, s[60:61]
v_and_or_b32 v80, v[vgprValuC+81], v10, v[vgprValuC+80] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+82], v[vgprValuC+82] // check Nan
v_bfe_u32 v9, v[vgprValuC+82], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+82], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+82], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+82], 16, v[vgprValuC+82] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+83], v[vgprValuC+83] // check Nan
v_bfe_u32 v9, v[vgprValuC+83], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+83], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+83], v9, v11, s[60:61]
v_and_or_b32 v81, v[vgprValuC+83], v10, v[vgprValuC+82] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[80:81], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[24:25], v[vgprValuC+84:vgprValuC+84+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[26:27], v[vgprValuC+86:vgprValuC+86+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+86:vgprValuC+86+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_mov_b32 v85, v5
v_mov_b32 v86, v6
v_mov_b32 v87, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+84], 16, v[vgprValuC+84] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_and_or_b32 v84, v[vgprValuC+85], v10, v[vgprValuC+84] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+86], v[vgprValuC+86] // check Nan
v_bfe_u32 v9, v[vgprValuC+86], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+86], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+86], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+86], 16, v[vgprValuC+86] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+87], v[vgprValuC+87] // check Nan
v_bfe_u32 v9, v[vgprValuC+87], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+87], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+87], v9, v11, s[60:61]
v_and_or_b32 v85, v[vgprValuC+87], v10, v[vgprValuC+86] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[84:85], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[24:25], v[vgprValuC+88:vgprValuC+88+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[26:27], v[vgprValuC+90:vgprValuC+90+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+90:vgprValuC+90+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v88, v4
v_mov_b32 v89, v5
v_mov_b32 v90, v6
v_mov_b32 v91, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+88], v[vgprValuC+88] // check Nan
v_bfe_u32 v9, v[vgprValuC+88], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+88], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+88], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+88], 16, v[vgprValuC+88] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+89], v[vgprValuC+89] // check Nan
v_bfe_u32 v9, v[vgprValuC+89], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+89], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+89], v9, v11, s[60:61]
v_and_or_b32 v88, v[vgprValuC+89], v10, v[vgprValuC+88] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+90], 16, v[vgprValuC+90] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+91], v[vgprValuC+91] // check Nan
v_bfe_u32 v9, v[vgprValuC+91], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+91], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+91], v9, v11, s[60:61]
v_and_or_b32 v89, v[vgprValuC+91], v10, v[vgprValuC+90] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[88:89], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 biasDim=0 */

/******************************************/
/* Global Write Batch #3 (d1,d0,vc1,vc0) = */
/*    (12,0,0,0:vw4); (12,0,1,0:vw4); (12,0,2,0:vw4); (12,0,3,0:vw4); (13,0,0,0:vw4); (13,0,1,0:vw4); (13,0,2,0:vw4); (13,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(12,0,0,0) */
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s60
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
ds_read_b128 v[20:23], v15 offset:0                // load bias
buffer_load_dwordx4 v[24:27], v16, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(12,1,0,0) */
/* (d1,vc1,d0,vc0)=(12,2,0,0) */
/* (d1,vc1,d0,vc0)=(12,3,0,0) */
/* (d1,vc1,d0,vc0)=(13,0,0,0) */
/* (d1,vc1,d0,vc0)=(13,1,0,0) */
/* (d1,vc1,d0,vc0)=(13,2,0,0) */
/* (d1,vc1,d0,vc0)=(13,3,0,0) */
v_accvgpr_read_b32 v[vgprValuC+28], acc192         // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+29], acc196         // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+30], acc200         // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+31], acc204         // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+32], acc193         // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+33], acc197         // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+34], acc201         // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+35], acc205         // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+36], acc194         // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+37], acc198         // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+38], acc202         // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+39], acc206         // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+40], acc195         // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+41], acc199         // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+42], acc203         // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+43], acc207         // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+44], acc208         // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+45], acc212         // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+46], acc216         // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+47], acc220         // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+48], acc209         // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+49], acc213         // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+50], acc217         // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+51], acc221         // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+52], acc210         // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+53], acc214         // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+54], acc218         // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+55], acc222         // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+56], acc211         // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+57], acc215         // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+58], acc219         // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+59], acc223         // copy acc to vreg[223]

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v32, v4
v_mov_b32 v33, v5
v_mov_b32 v34, v6
v_mov_b32 v35, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+32], v[vgprValuC+32] // check Nan
v_bfe_u32 v9, v[vgprValuC+32], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+32], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+32], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+32], 16, v[vgprValuC+32] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_and_or_b32 v32, v[vgprValuC+33], v10, v[vgprValuC+32] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+34], v[vgprValuC+34] // check Nan
v_bfe_u32 v9, v[vgprValuC+34], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+34], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+34], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+34], 16, v[vgprValuC+34] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+35], v[vgprValuC+35] // check Nan
v_bfe_u32 v9, v[vgprValuC+35], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+35], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+35], v9, v11, s[60:61]
v_and_or_b32 v33, v[vgprValuC+35], v10, v[vgprValuC+34] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[24:25], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[26:27], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_mov_b32 v41, v5
v_mov_b32 v42, v6
v_mov_b32 v43, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+40], 16, v[vgprValuC+40] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+41], v[vgprValuC+41] // check Nan
v_bfe_u32 v9, v[vgprValuC+41], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+41], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+41], v9, v11, s[60:61]
v_and_or_b32 v40, v[vgprValuC+41], v10, v[vgprValuC+40] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+42], v[vgprValuC+42] // check Nan
v_bfe_u32 v9, v[vgprValuC+42], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+42], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+42], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+42], 16, v[vgprValuC+42] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+43], v[vgprValuC+43] // check Nan
v_bfe_u32 v9, v[vgprValuC+43], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+43], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+43], v9, v11, s[60:61]
v_and_or_b32 v41, v[vgprValuC+43], v10, v[vgprValuC+42] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[24:25], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[26:27], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_mov_b32 v45, v5
v_mov_b32 v46, v6
v_mov_b32 v47, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+44], 16, v[vgprValuC+44] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_and_or_b32 v44, v[vgprValuC+45], v10, v[vgprValuC+44] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+46], v[vgprValuC+46] // check Nan
v_bfe_u32 v9, v[vgprValuC+46], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+46], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+46], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+46], 16, v[vgprValuC+46] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+47], v[vgprValuC+47] // check Nan
v_bfe_u32 v9, v[vgprValuC+47], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+47], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+47], v9, v11, s[60:61]
v_and_or_b32 v45, v[vgprValuC+47], v10, v[vgprValuC+46] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_mov_b32 v49, v5
v_mov_b32 v50, v6
v_mov_b32 v51, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+48], 16, v[vgprValuC+48] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+49], v[vgprValuC+49] // check Nan
v_bfe_u32 v9, v[vgprValuC+49], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+49], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+49], v9, v11, s[60:61]
v_and_or_b32 v48, v[vgprValuC+49], v10, v[vgprValuC+48] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+50], 16, v[vgprValuC+50] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+51], v[vgprValuC+51] // check Nan
v_bfe_u32 v9, v[vgprValuC+51], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+51], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+51], v9, v11, s[60:61]
v_and_or_b32 v49, v[vgprValuC+51], v10, v[vgprValuC+50] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[24:25], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[26:27], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_mov_b32 v53, v5
v_mov_b32 v54, v6
v_mov_b32 v55, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+52], 16, v[vgprValuC+52] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+53], v[vgprValuC+53] // check Nan
v_bfe_u32 v9, v[vgprValuC+53], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+53], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+53], v9, v11, s[60:61]
v_and_or_b32 v52, v[vgprValuC+53], v10, v[vgprValuC+52] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+54], v[vgprValuC+54] // check Nan
v_bfe_u32 v9, v[vgprValuC+54], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+54], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+54], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+54], 16, v[vgprValuC+54] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_and_or_b32 v53, v[vgprValuC+55], v10, v[vgprValuC+54] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[52:53], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[24:25], v[vgprValuC+56:vgprValuC+56+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[26:27], v[vgprValuC+58:vgprValuC+58+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+58:vgprValuC+58+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_mov_b32 v57, v5
v_mov_b32 v58, v6
v_mov_b32 v59, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+56], 16, v[vgprValuC+56] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+57], v[vgprValuC+57] // check Nan
v_bfe_u32 v9, v[vgprValuC+57], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+57], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+57], v9, v11, s[60:61]
v_and_or_b32 v56, v[vgprValuC+57], v10, v[vgprValuC+56] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+58], v[vgprValuC+58] // check Nan
v_bfe_u32 v9, v[vgprValuC+58], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+58], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+58], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+58], 16, v[vgprValuC+58] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+59], v[vgprValuC+59] // check Nan
v_bfe_u32 v9, v[vgprValuC+59], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+59], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+59], v9, v11, s[60:61]
v_and_or_b32 v57, v[vgprValuC+59], v10, v[vgprValuC+58] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[56:57], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End                              // jump to end
label_GW_End:

s_endpgm                                           // Kernel End

label_LoopEndL_odd:
.set vgprValuA_X0_I0, vgprValuA_X0_I0_1
.set vgprValuA_X2_I0, vgprValuA_X2_I0_1

/* Before NLL: Check VGPR.checkin for INT8 LW */

/******************************************/
/* Ord. NoGlobalLoadLoop - Begin          */
/******************************************/
s_waitcnt vmcnt(7)

/* Begin Each Unroll: Check VGPR.checkin for INT8 LW */

/* iter 0 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
buffer_load_dwordx4 v[vgprValuA_X0_I0_0+0:vgprValuA_X0_I0_0+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // G -> Reg 0_0_0_0
buffer_load_dwordx4 v[vgprValuA_X2_I0_0+0:vgprValuA_X2_I0_0+0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+3] offen offset:0 // G -> Reg 0_0_1_0
/* global read inc B loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_cselect_b32 s80, s[sgprWrapUB+0], s[sgprGlobalReadIncsB+0] // incLower <- ?
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
s_cselect_b32 s81, s[sgprWrapUB+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s80        // gra SRD += inc(lower)
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
ds_read_b128 v[vgprValuB_X2_I0+4:vgprValuB_X2_I0+4+3], v[vgprLocalReadAddrB] offset:2624 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s81       // gra SRD += inc(upper)
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s80 // limit -= inc)
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s81 // limit -= inc)
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
ds_read_b128 v[vgprValuB_X2_I0+8:vgprValuB_X2_I0+8+3], v[vgprLocalReadAddrB] offset:5184 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=2 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
ds_read_b128 v[vgprValuB_X2_I0+12:vgprValuB_X2_I0+12+3], v[vgprLocalReadAddrB] offset:7744 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=3 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
ds_read_b128 v[vgprValuB_X2_I0+16:vgprValuB_X2_I0+16+3], v[vgprLocalReadAddrB] offset:10304 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=4 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:16  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:17  */
ds_read_b128 v[vgprValuB_X2_I0+20:vgprValuB_X2_I0+20+3], v[vgprLocalReadAddrB] offset:12864 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=5 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:18  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+24:vgprValuB_X2_I0+24+3], v[vgprLocalReadAddrB] offset:15424 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=6 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+28:vgprValuB_X2_I0+28+3], v[vgprLocalReadAddrB] offset:17984 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=7 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+32:vgprValuB_X2_I0+32+3], v[vgprLocalReadAddrB] offset:20544 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=8 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
buffer_load_dwordx4 v[vgprValuA_X0_I0_0+4:vgprValuA_X0_I0_0+4+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+0] offen offset:0 // G -> Reg 0_0_1_0
buffer_load_dwordx4 v[vgprValuA_X2_I0_0+4:vgprValuA_X2_I0_0+4+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+4] offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+36:vgprValuB_X2_I0+36+3], v[vgprLocalReadAddrB] offset:23104 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=9 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+40:vgprValuB_X2_I0+40+3], v[vgprLocalReadAddrB] offset:25664 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=10 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+44:vgprValuB_X2_I0+44+3], v[vgprLocalReadAddrB] offset:28224 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=11 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:36  */
/* schedule remaining localreads for 1LDSB */
/* localReadsVacancy: latencyLeft 2 */
/* 1 LDS buffer: read-sync-write */
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:37  */
/* sched write - iter 0 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:38  */
ds_read_b128 v[vgprValuB_X2_I0+48:vgprValuB_X2_I0+48+3], v[vgprLocalReadAddrB] offset:30784 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=12 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:39  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:40  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:41  */
ds_read_b128 v[vgprValuB_X2_I0+52:vgprValuB_X2_I0+52+3], v[vgprLocalReadAddrB] offset:33344 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=13 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:42  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:43  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:44  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:45  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:46  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:47  */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:48  */
/* sched write - iter 0 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:49  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:50  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:51  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:52  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:53  */
buffer_load_dwordx4 v[vgprValuA_X0_I0_0+8:vgprValuA_X0_I0_0+8+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+1] offen offset:0 // G -> Reg 0_0_2_0
buffer_load_dwordx4 v[vgprValuA_X2_I0_0+8:vgprValuA_X2_I0_0+8+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+5] offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:54  */
s_waitcnt lgkmcnt(0)
s_barrier
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:55  */
s_waitcnt vmcnt(12)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */

/* iter 1 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:56  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:57  */
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:58  */
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:59  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:60  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:61  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:62  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:63  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:64  */
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:65  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:66  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:67  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:68  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:69  */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:70  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:71  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:72  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:73  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:74  */
s_waitcnt vmcnt(11)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:75  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:640 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 640
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:76  */
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:77  */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:78  */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:79  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:80  */
buffer_load_dwordx4 v[vgprValuA_X0_I0_0+12:vgprValuA_X0_I0_0+12+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+2] offen offset:0 // G -> Reg 0_0_3_0
buffer_load_dwordx4 v[vgprValuA_X2_I0_0+12:vgprValuA_X2_I0_0+12+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+6] offen offset:0 // G -> Reg 0_0_1_0
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:81  */
/* sched write - iter 1 writesPerItem=1 */
/* global read inc A loopL */
s_cmp_eq_u32 s[sgprLoopCounterL], s[sgprStaggerUIter] // Is this the wrapIter?
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:82  */
s_cselect_b32 s80, s[sgprWrapUA+0], s[sgprGlobalReadIncsA+0] // incLower <- ?
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:83  */
s_cselect_b32 s81, s[sgprWrapUA+1], 0              // incUpper <- ?
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:84  */
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s80        // gra SRD += inc(lower)
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:85  */
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s81       // gra SRD += inc(upper)
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:86  */
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s80 // limit -= inc)
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:87  */
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s81 // limit -= inc)
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:88  */
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:89  */
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:90  */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:91  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:92  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:93  */
s_waitcnt vmcnt(12)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:94  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:1280 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 1280
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:95  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:96  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:97  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:98  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:99  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:100  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:101  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:102  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:103  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:104  */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:105  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:106  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:107  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:108  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:109  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:110  */
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:111  */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */

/* iter 2 (reset local read pointers iteration)  (swap local read pointers iteration)  */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:112  */
s_waitcnt vmcnt(11)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:113  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:1920 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 1920
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:114  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:115  */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:116  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:117  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:118  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:119  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:120  */
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:121  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:122  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:123  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:124  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:125  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:126  */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:127  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:128  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:129  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:130  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:131  */
s_waitcnt vmcnt(10)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:132  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+16:vgprG2LB+16+3] offset:2560 // lwoB_0_0_4_0 = (0*LSCB)*(MT1J+PAD) + (4*LSPB) = 2560
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:133  */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:134  */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:135  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:136  */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:137  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:138  */
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:139  */
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:140  */
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:141  */
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:142  */
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:143  */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:144  */
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:145  */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:146  */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:147  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:148  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:149  */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:150  */
s_waitcnt vmcnt(9)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:151  */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+20:vgprG2LB+20+3] offset:3200 // lwoB_0_0_5_0 = (0*LSCB)*(MT1J+PAD) + (5*LSPB) = 3200
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:152  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:153  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:154  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:155  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:156  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:157  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:158  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:159  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:160  */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:161  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:162  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:163  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:164  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:165  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:166  */
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:167  */

/* local read swap offsets a */

/* local read swap offsets b */

/* local read init pointers a */

/* localReadInitPointers */

/* local read init pointers b */

/* localReadInitPointers */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=14 */

/* iter 3 (swap and reset local write pointers iteration)  */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:168  */
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:169  */
s_waitcnt vmcnt(8)                                // wait for global read before writing to local
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:170  */
/* sched write - iter 3 writesPerItem=1 */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+24:vgprG2LB+24+3] offset:3840 // lwoB_0_0_6_0 = (0*LSCB)*(MT1J+PAD) + (6*LSPB) = 3840
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:171  */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:172  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:173  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:174  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:175  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:176  */
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:177  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:178  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:179  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:180  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:181  */
/* sched write - iter 3 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:182  */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:183  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:184  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:185  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:186  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:187  */
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:188  */
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:189  */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:190  */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:191  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:192  */
/* sched write - iter 3 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:193  */

/* local write swap offsets a */

/* local write swap offsets b */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:194  */
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:195  */
s_waitcnt lgkmcnt(0)                               // 3wait for local write
// Skip force waitcnt0
s_barrier
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:196  */
ds_read_b128 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+3], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:197  */
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:198  */
ds_read_b128 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+3], v[vgprLocalReadAddrB] offset:2560 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:199  */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:200  */
ds_read_b128 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+3], v[vgprLocalReadAddrB] offset:5120 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=2 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:201  */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:202  */
ds_read_b128 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+3], v[vgprLocalReadAddrB] offset:7680 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=3 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:203  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:204  */
ds_read_b128 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+3], v[vgprLocalReadAddrB] offset:10240 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=4 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:205  */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:206  */
ds_read_b128 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+3], v[vgprLocalReadAddrB] offset:12800 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=5 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:207  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:208  */
ds_read_b128 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+3], v[vgprLocalReadAddrB] offset:15360 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=6 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:209  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:210  */
ds_read_b128 v[vgprValuB_X0_I0+28:vgprValuB_X0_I0+28+3], v[vgprLocalReadAddrB] offset:17920 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=7 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:211  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:212  */
ds_read_b128 v[vgprValuB_X0_I0+32:vgprValuB_X0_I0+32+3], v[vgprLocalReadAddrB] offset:20480 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=8 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:213  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:214  */
ds_read_b128 v[vgprValuB_X0_I0+36:vgprValuB_X0_I0+36+3], v[vgprLocalReadAddrB] offset:23040 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=9 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:215  */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:216  */
ds_read_b128 v[vgprValuB_X0_I0+40:vgprValuB_X0_I0+40+3], v[vgprLocalReadAddrB] offset:25600 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=10 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:217  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:218  */
ds_read_b128 v[vgprValuB_X0_I0+44:vgprValuB_X0_I0+44+3], v[vgprLocalReadAddrB] offset:28160 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=11 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:219  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:220  */
ds_read_b128 v[vgprValuB_X0_I0+48:vgprValuB_X0_I0+48+3], v[vgprLocalReadAddrB] offset:30720 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=12 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:221  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:222  */
ds_read_b128 v[vgprValuB_X0_I0+52:vgprValuB_X0_I0+52+3], v[vgprLocalReadAddrB] offset:33280 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=13 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:223  */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=1 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */


.set vgprValuA_X0_I0, vgprValuA_X0_I0_0
.set vgprValuA_X2_I0, vgprValuA_X2_I0_0

s_branch label_LoopEndL_odd_NoLoadLoop

label_LoopEndL_odd_NoLoadLoop:

/******************************************/
/* Ord. NoLoadLoop - Begin                */
/******************************************/
s_waitcnt vmcnt(0)

/* iter 0 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:0  */
s_waitcnt lgkmcnt(0)                               // wait for prior local read local write old=0, new=0 newLW=0 newLR=0
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:1  */
ds_read_b128 v[vgprValuB_X2_I0+0:vgprValuB_X2_I0+0+3], v[vgprLocalReadAddrB] offset:64 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:2  */
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:3  */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:4  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:5  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:6  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:7  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:8  */
ds_read_b128 v[vgprValuB_X2_I0+4:vgprValuB_X2_I0+4+3], v[vgprLocalReadAddrB] offset:2624 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:9  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:10  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:11  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:12  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:13  */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:14  */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:15  */
ds_read_b128 v[vgprValuB_X2_I0+8:vgprValuB_X2_I0+8+3], v[vgprLocalReadAddrB] offset:5184 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=2 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:16  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:17  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:18  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:19  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:20  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:21  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:22  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+12:vgprValuB_X2_I0+12+3], v[vgprLocalReadAddrB] offset:7744 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=3 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:23  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:24  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:25  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:26  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:27  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:28  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:29  */
/* localReadsVacancy: latencyLeft 2 */
ds_read_b128 v[vgprValuB_X2_I0+16:vgprValuB_X2_I0+16+3], v[vgprLocalReadAddrB] offset:10304 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=4 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:30  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:31  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X0_I0+28+0+0:vgprValuB_X0_I0+28+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:32  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:33  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:34  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:35  */
/* localReadsVacancy: latencyLeft 2 */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X0_I0+32+0+0:vgprValuB_X0_I0+32+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:36  */
/* schedule remaining localreads for 1LDSB */
/* localReadsVacancy: latencyLeft 2 */
/* 1 LDS buffer: read-sync-write */
ds_read_b128 v[vgprValuB_X2_I0+20:vgprValuB_X2_I0+20+3], v[vgprLocalReadAddrB] offset:12864 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=5 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:37  */
/* sched write - iter 0 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:38  */
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:39  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X0_I0+36+0+0:vgprValuB_X0_I0+36+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:40  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:41  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:42  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:43  */
ds_read_b128 v[vgprValuB_X2_I0+24:vgprValuB_X2_I0+24+3], v[vgprLocalReadAddrB] offset:15424 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=6 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X0_I0+40+0+0:vgprValuB_X0_I0+40+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:44  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:45  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:46  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:47  */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X0_I0+44+0+0:vgprValuB_X0_I0+44+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:48  */
/* sched write - iter 0 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:49  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:50  */
ds_read_b128 v[vgprValuB_X2_I0+28:vgprValuB_X2_I0+28+3], v[vgprLocalReadAddrB] offset:17984 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=7 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:51  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X0_I0+48+0+0:vgprValuB_X0_I0+48+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:52  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:53  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:54  */
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+8+0+0:vgprValuA_X0_I0+8+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:55  */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X0_I0+52+0+0:vgprValuB_X0_I0+52+0+0+1], v[vgprValuA_X0_I0+12+0+0:vgprValuA_X0_I0+12+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */

/* iter 1 */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:56  */
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:57  */
ds_read_b128 v[vgprValuB_X2_I0+32:vgprValuB_X2_I0+32+3], v[vgprLocalReadAddrB] offset:20544 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=8 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:58  */
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:59  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X0_I0+0+2+0:vgprValuB_X0_I0+0+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:60  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:61  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:62  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:63  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X0_I0+4+2+0:vgprValuB_X0_I0+4+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:64  */
ds_read_b128 v[vgprValuB_X2_I0+36:vgprValuB_X2_I0+36+3], v[vgprLocalReadAddrB] offset:23104 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=9 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:65  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:66  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:67  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X0_I0+8+2+0:vgprValuB_X0_I0+8+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:68  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:69  */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:70  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:71  */
ds_read_b128 v[vgprValuB_X2_I0+40:vgprValuB_X2_I0+40+3], v[vgprLocalReadAddrB] offset:25664 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=10 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X0_I0+12+2+0:vgprValuB_X0_I0+12+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:72  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:73  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:74  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:75  */
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X0_I0+16+2+0:vgprValuB_X0_I0+16+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:76  */
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:77  */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:78  */
ds_read_b128 v[vgprValuB_X2_I0+44:vgprValuB_X2_I0+44+3], v[vgprLocalReadAddrB] offset:28224 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=11 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:79  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X0_I0+20+2+0:vgprValuB_X0_I0+20+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:80  */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:81  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:82  */
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:83  */
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X0_I0+24+2+0:vgprValuB_X0_I0+24+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:84  */
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:85  */
ds_read_b128 v[vgprValuB_X2_I0+48:vgprValuB_X2_I0+48+3], v[vgprLocalReadAddrB] offset:30784 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=12 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:86  */
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:87  */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X0_I0+28+2+0:vgprValuB_X0_I0+28+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:88  */
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:89  */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:90  */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:91  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X0_I0+32+2+0:vgprValuB_X0_I0+32+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:92  */
/* sched write - iter 1 writesPerItem=1 */
ds_read_b128 v[vgprValuB_X2_I0+52:vgprValuB_X2_I0+52+3], v[vgprLocalReadAddrB] offset:33344 // L -> Reg lro=32 swapByteOffset=0 ti=16 vIdx=13 eIdx=0 rIdx=0 oIdx=0 buffer=2 iui=0
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:93  */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:94  */
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:95  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X0_I0+36+2+0:vgprValuB_X0_I0+36+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:96  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:97  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:98  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:99  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X0_I0+40+2+0:vgprValuB_X0_I0+40+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:100  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:101  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:102  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:103  */
/* sched write - iter 1 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X0_I0+44+2+0:vgprValuB_X0_I0+44+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:104  */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:105  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:106  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:107  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X0_I0+48+2+0:vgprValuB_X0_I0+48+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:108  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+0+2+0:vgprValuA_X0_I0+0+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:109  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+4+2+0:vgprValuA_X0_I0+4+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:110  */
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+8+2+0:vgprValuA_X0_I0+8+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:111  */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X0_I0+52+2+0:vgprValuB_X0_I0+52+2+0+1], v[vgprValuA_X0_I0+12+2+0:vgprValuA_X0_I0+12+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=-1 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=-1 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */

/* iter 2 (reset local read pointers iteration)  (swap local read pointers iteration)  */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:112  */
s_waitcnt lgkmcnt(0)                                // wait for global read before writing to local
s_barrier
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:113  */
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:114  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:115  */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X2_I0+0+0+0:vgprValuB_X2_I0+0+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:116  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:117  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:118  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:119  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X2_I0+4+0+0:vgprValuB_X2_I0+4+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:120  */
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:121  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:122  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:123  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X2_I0+8+0+0:vgprValuB_X2_I0+8+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:124  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:125  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:126  */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:127  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X2_I0+12+0+0:vgprValuB_X2_I0+12+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:128  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:129  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:130  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:131  */
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X2_I0+16+0+0:vgprValuB_X2_I0+16+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:132  */
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:133  */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:134  */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:135  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X2_I0+20+0+0:vgprValuB_X2_I0+20+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:136  */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:137  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:138  */
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:139  */
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X2_I0+24+0+0:vgprValuB_X2_I0+24+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:140  */
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:141  */
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:142  */
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:143  */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X2_I0+28+0+0:vgprValuB_X2_I0+28+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:144  */
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:145  */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:146  */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:147  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X2_I0+32+0+0:vgprValuB_X2_I0+32+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:148  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:149  */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:150  */
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:151  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X2_I0+36+0+0:vgprValuB_X2_I0+36+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:152  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:153  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:154  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:155  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X2_I0+40+0+0:vgprValuB_X2_I0+40+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:156  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:157  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:158  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:159  */
/* sched write - iter 2 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X2_I0+44+0+0:vgprValuB_X2_I0+44+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:160  */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:161  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:162  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:163  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X2_I0+48+0+0:vgprValuB_X2_I0+48+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:164  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+0+0+0:vgprValuA_X2_I0+0+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:165  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+4+0+0:vgprValuA_X2_I0+4+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:166  */
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+8+0+0:vgprValuA_X2_I0+8+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:167  */

/* local read swap offsets a */

/* local read swap offsets b */

/* local read init pointers a */

/* localReadInitPointers */

/* local read init pointers b */

/* localReadInitPointers */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X2_I0+52+0+0:vgprValuB_X2_I0+52+0+0+1], v[vgprValuA_X2_I0+12+0+0:vgprValuA_X2_I0+12+0+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=0 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=0 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=0 readsPerIterB=14 */

/* iter 3 (swap and reset local write pointers iteration)  */
/*  grEndMfmaIndex:18, lwStartMfmaIndex:37, lwEndMfmaIndex:193  */
/*  numMfmaForLR:28, syncPlrMfmaIndex:195  */
/*  mfmaIndex:168  */
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[0:3] // left value = acc[0+0:3+0]
/*  mfmaIndex:169  */
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[4:7] // left value = acc[4+0:7+0]
/*  mfmaIndex:170  */
/* sched write - iter 3 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[8:11] // left value = acc[8+0:11+0]
/*  mfmaIndex:171  */
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X2_I0+0+2+0:vgprValuB_X2_I0+0+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[12:15] // left value = acc[12+0:15+0]
/*  mfmaIndex:172  */
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[16:19] // left value = acc[16+0:19+0]
/*  mfmaIndex:173  */
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[20:23] // left value = acc[20+0:23+0]
/*  mfmaIndex:174  */
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[24:27] // left value = acc[24+0:27+0]
/*  mfmaIndex:175  */
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X2_I0+4+2+0:vgprValuB_X2_I0+4+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[28:31] // left value = acc[28+0:31+0]
/*  mfmaIndex:176  */
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[32:35] // left value = acc[32+0:35+0]
/*  mfmaIndex:177  */
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[36:39] // left value = acc[36+0:39+0]
/*  mfmaIndex:178  */
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[40:43] // left value = acc[40+0:43+0]
/*  mfmaIndex:179  */
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X2_I0+8+2+0:vgprValuB_X2_I0+8+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[44:47] // left value = acc[44+0:47+0]
/*  mfmaIndex:180  */
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[48:51] // left value = acc[48+0:51+0]
/*  mfmaIndex:181  */
/* sched write - iter 3 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[52:55] // left value = acc[52+0:55+0]
/*  mfmaIndex:182  */
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[56:59] // left value = acc[56+0:59+0]
/*  mfmaIndex:183  */
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X2_I0+12+2+0:vgprValuB_X2_I0+12+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[60:63] // left value = acc[60+0:63+0]
/*  mfmaIndex:184  */
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[64:67] // left value = acc[64+0:67+0]
/*  mfmaIndex:185  */
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[68:71] // left value = acc[68+0:71+0]
/*  mfmaIndex:186  */
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[72:75] // left value = acc[72+0:75+0]
/*  mfmaIndex:187  */
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X2_I0+16+2+0:vgprValuB_X2_I0+16+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[76:79] // left value = acc[76+0:79+0]
/*  mfmaIndex:188  */
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[80:83] // left value = acc[80+0:83+0]
/*  mfmaIndex:189  */
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[84:87] // left value = acc[84+0:87+0]
/*  mfmaIndex:190  */
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[88:91] // left value = acc[88+0:91+0]
/*  mfmaIndex:191  */
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X2_I0+20+2+0:vgprValuB_X2_I0+20+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[92:95] // left value = acc[92+0:95+0]
/*  mfmaIndex:192  */
/* sched write - iter 3 writesPerItem=1 */
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[96:99] // left value = acc[96+0:99+0]
/*  mfmaIndex:193  */

/* local write swap offsets a */

/* local write swap offsets b */
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[100:103] // left value = acc[100+0:103+0]
/*  mfmaIndex:194  */
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[104:107] // left value = acc[104+0:107+0]
/*  mfmaIndex:195  */
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X2_I0+24+2+0:vgprValuB_X2_I0+24+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[108:111] // left value = acc[108+0:111+0]
/*  mfmaIndex:196  */
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[112:115] // left value = acc[112+0:115+0]
/*  mfmaIndex:197  */
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[116:119] // left value = acc[116+0:119+0]
/*  mfmaIndex:198  */
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[120:123] // left value = acc[120+0:123+0]
/*  mfmaIndex:199  */
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X2_I0+28+2+0:vgprValuB_X2_I0+28+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[124:127] // left value = acc[124+0:127+0]
/*  mfmaIndex:200  */
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[128:131] // left value = acc[128+0:131+0]
/*  mfmaIndex:201  */
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[132:135] // left value = acc[132+0:135+0]
/*  mfmaIndex:202  */
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[136:139] // left value = acc[136+0:139+0]
/*  mfmaIndex:203  */
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X2_I0+32+2+0:vgprValuB_X2_I0+32+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[140:143] // left value = acc[140+0:143+0]
/*  mfmaIndex:204  */
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[144:147] // left value = acc[144+0:147+0]
/*  mfmaIndex:205  */
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[148:151] // left value = acc[148+0:151+0]
/*  mfmaIndex:206  */
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[152:155] // left value = acc[152+0:155+0]
/*  mfmaIndex:207  */
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X2_I0+36+2+0:vgprValuB_X2_I0+36+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[156:159] // left value = acc[156+0:159+0]
/*  mfmaIndex:208  */
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[160:163] // left value = acc[160+0:163+0]
/*  mfmaIndex:209  */
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[164:167] // left value = acc[164+0:167+0]
/*  mfmaIndex:210  */
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[168:171] // left value = acc[168+0:171+0]
/*  mfmaIndex:211  */
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X2_I0+40+2+0:vgprValuB_X2_I0+40+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[172:175] // left value = acc[172+0:175+0]
/*  mfmaIndex:212  */
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[176:179] // left value = acc[176+0:179+0]
/*  mfmaIndex:213  */
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[180:183] // left value = acc[180+0:183+0]
/*  mfmaIndex:214  */
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[184:187] // left value = acc[184+0:187+0]
/*  mfmaIndex:215  */
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X2_I0+44+2+0:vgprValuB_X2_I0+44+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[188:191] // left value = acc[188+0:191+0]
/*  mfmaIndex:216  */
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[192:195] // left value = acc[192+0:195+0]
/*  mfmaIndex:217  */
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[196:199] // left value = acc[196+0:199+0]
/*  mfmaIndex:218  */
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[200:203] // left value = acc[200+0:203+0]
/*  mfmaIndex:219  */
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X2_I0+48+2+0:vgprValuB_X2_I0+48+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[204:207] // left value = acc[204+0:207+0]
/*  mfmaIndex:220  */
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+0+2+0:vgprValuA_X2_I0+0+2+0+1], acc[208:211] // left value = acc[208+0:211+0]
/*  mfmaIndex:221  */
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+4+2+0:vgprValuA_X2_I0+4+2+0+1], acc[212:215] // left value = acc[212+0:215+0]
/*  mfmaIndex:222  */
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+8+2+0:vgprValuA_X2_I0+8+2+0+1], acc[216:219] // left value = acc[216+0:219+0]
/*  mfmaIndex:223  */
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X2_I0+52+2+0:vgprValuB_X2_I0+52+2+0+1], v[vgprValuA_X2_I0+12+2+0:vgprValuA_X2_I0+12+2+0+1], acc[220:223] // left value = acc[220+0:223+0]
/* numPrefetchIter=1 */
/* dataAtIterA=0 numReadsIterA=1 skipReadsIterA=1 readsPerIterA=4 */
/* dataAtIterB=0 numReadsIterB=1 skipReadsIterB=1 readsPerIterB=14 */

s_branch label_Summation_End_OptNLL

label_PrefetchGlobalLastIterEnd:
.set vgprValuA_X0_I0, vgprValuA_X0_I0_1
.set vgprValuA_X2_I0, vgprValuA_X2_I0_1

/******************************************/
/* Tail Loop                              */
/******************************************/

/* Tail: add ValuA/B vgpr buffer [0...144) to pool */

/* local write reset offsets a */

/* local write reset offsets b */

// numIterL = LOCAL_SPLITU * min(sizeL % LOCAL_DEPTHU, DEPTHU / LOCAL_SPLITU)
s_and_b32 s[sgprLoopCounterL], 63, s[sgprSizesSum+0] // s[sgprLoopCounterL] = s[sgprSizesSum+0] % 64
s_cmp_lg_u32 s[sgprGSUSumIdx], s[sgprGSUSumIdx+1]  // gsuSumIdx == numIterPerWgRemainder
s_cmov_b32 s[sgprLoopCounterL], 0x0                // numIter=0 if gsuSimIdx != numIterPerWgRemainder
s_cmp_eq_u32 s[sgprLoopCounterL], 0x0              // numIterL == 0
s_mov_b32 s[sgprOrigLoopCounter], 0                // repurpose to count each localRead increment
s_cbranch_scc1 label_SkipTailLoopL                 // skip to end of tail loop b/c numIter==0

/* remove stagger offsets for tail loop */
s_sub_i32 s80, 3, s[sgprStaggerUIter]
s_mul_hi_i32 s81, s80, s[sgprGlobalReadIncsA+0]    // start offset S in bytes
s_mul_i32 s80, s80, s[sgprGlobalReadIncsA+0]       // start offset S in bytes
s_sub_u32 s80, s80, s[sgprWrapUA]                  // S - WrapU
s_subb_u32 s81, s81, s[sgprWrapUA+1]               // S - WrapU
s_add_u32 s[sgprSrdA+0], s[sgprSrdA+0], s80        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdA+1], s[sgprSrdA+1], s81       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitA+0], s[sgprShadowLimitA+0], s80 // limit -= inc)
s_subb_u32 s[sgprShadowLimitA+1], s[sgprShadowLimitA+1], s81 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitA+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdA+2], s[sgprShadowLimitA+0], BufferLimit // Move shadow to real if we are within 2^32
s_sub_i32 s80, 3, s[sgprStaggerUIter]
s_mul_hi_i32 s81, s80, s[sgprGlobalReadIncsB+0]    // start offset S in bytes
s_mul_i32 s80, s80, s[sgprGlobalReadIncsB+0]       // start offset S in bytes
s_sub_u32 s80, s80, s[sgprWrapUB]                  // S - WrapU
s_subb_u32 s81, s81, s[sgprWrapUB+1]               // S - WrapU
s_add_u32 s[sgprSrdB+0], s[sgprSrdB+0], s80        // gra SRD += inc(lower)
s_addc_u32 s[sgprSrdB+1], s[sgprSrdB+1], s81       // gra SRD += inc(upper)
s_sub_u32 s[sgprShadowLimitB+0], s[sgprShadowLimitB+0], s80 // limit -= inc)
s_subb_u32 s[sgprShadowLimitB+1], s[sgprShadowLimitB+1], s81 // limit -= inc)
s_cmp_eq_u32 s[sgprShadowLimitB+1], 0              // are we within 2^32?
s_cselect_b32 s[sgprSrdB+2], s[sgprShadowLimitB+0], BufferLimit // Move shadow to real if we are within 2^32

/* Recalc global read offsets */
v_and_b32 v1, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_and_b32 v0 15, v1                               // 1. M offset: mIdx = wtid % MI_M(16)
v_mul_lo_u32 v0, s[sgprStrideA0I], v0              // 1. M offset: mOffset = mIdx * mStride(k)
v_lshlrev_b32 v0, 0x2, v0                          // 4. apply VectorWidth: bnOffset = bnOffset * vw(4)
v_and_b32 v1, 63, v[vgprSerial]                    // 5. thread id in wave: wtid = tid % wavelength(64)
v_lshrrev_b32 v1, 4, v1                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
v_lshlrev_b32 v1, 0x2, v1                          // 5. K offset: lrKOffset = kIdx * mStride(4)
v_add_u32 v0, v1, v0                               // 6. offset in wave: lrOffset = bnOffset + lrKOffset
v_lshrrev_b32 v1, 6, v[vgprSerial]                 // 7. wave offset in M dimen: wtid = tid / dividedForWaveId(64)
v_and_b32 v1, 3, v1                                // 7. wave offset in M dimen: wtid0 = wtid % num1DWaves(4)
v_mul_lo_u32 v1, s[sgprStrideA0I], v1                          // 7. wave offset in M dimen: wOffset = wtid0 * s[sgprStrideA0I](8192)
v_lshlrev_b32 v1, 0x6, v1                          // 7. wave offset in M dimen: wOffset = wOffset * 16 * vw(4)
v_add_u32 v[vgprGlobalReadOffsetA], v1, v0                               // 7. final local read offset: flrOffset = lrOffset + WOffset
v_add_u32 v[vgprGlobalReadOffsetA] 0x8 v[vgprGlobalReadOffsetA]      // add prepad for pointer shift
v_lshlrev_b32 v[vgprGlobalReadOffsetA] 0x1 v[vgprGlobalReadOffsetA]  // offset *= bytes/element

s_mul_i32 s[sgprScalarGlobalReadOffsetA+0], s[sgprStrideA0I], 1 // compute offset diff (scaled tileDim)
s_lshl_b32 s[sgprScalarGlobalReadOffsetA+0], s[sgprScalarGlobalReadOffsetA+0], 0x1 // scalar offset *= bytes/element
s_mul_i32 s[sgprScalarGlobalReadOffsetA+1], s[sgprStrideA0I], 2 // compute offset diff (scaled tileDim)
s_lshl_b32 s[sgprScalarGlobalReadOffsetA+1], s[sgprScalarGlobalReadOffsetA+1], 0x1 // scalar offset *= bytes/element
s_mul_i32 s[sgprScalarGlobalReadOffsetA+2], s[sgprStrideA0I], 3 // compute offset diff (scaled tileDim)
s_lshl_b32 s[sgprScalarGlobalReadOffsetA+2], s[sgprScalarGlobalReadOffsetA+2], 0x1 // scalar offset *= bytes/element
s_mul_i32 s[sgprScalarGlobalReadOffsetA+3], 1, 16 // compute offset diff (scaled tileDim)
s_lshl_b32 s[sgprScalarGlobalReadOffsetA+3], s[sgprScalarGlobalReadOffsetA+3], 0x1 // scalar offset *= bytes/element
s_add_u32 s[sgprScalarGlobalReadOffsetA+4], s[sgprScalarGlobalReadOffsetA+0], s[sgprScalarGlobalReadOffsetA+3]
s_add_u32 s[sgprScalarGlobalReadOffsetA+5], s[sgprScalarGlobalReadOffsetA+1], s[sgprScalarGlobalReadOffsetA+3]
s_add_u32 s[sgprScalarGlobalReadOffsetA+6], s[sgprScalarGlobalReadOffsetA+2], s[sgprScalarGlobalReadOffsetA+3]

/* Update M0 for DTLDS */

/* global read A */
/* g2l=0, load component 0 */
buffer_load_short_d16 v[vgprValuA_X0_I0+0], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:0 // load one buffer value
/* g2l=0, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X0_I0+0], v[vgprValuA_X0_I0+0], v0      // HasEccHalf: pack
/* g2l=0, load component 2 */
buffer_load_short_d16 v[vgprValuA_X0_I0+1], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:4 // load one buffer value
/* g2l=0, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X0_I0+1], v[vgprValuA_X0_I0+1], v0      // HasEccHalf: pack
/* g2l=0, load component 0 */
buffer_load_short_d16 v[vgprValuA_X0_I0+8], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+3] offen offset:0 // load one buffer value
/* g2l=0, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+3] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X0_I0+8], v[vgprValuA_X0_I0+8], v0      // HasEccHalf: pack
/* g2l=0, load component 2 */
buffer_load_short_d16 v[vgprValuA_X0_I0+9], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+3] offen offset:4 // load one buffer value
/* g2l=0, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+3] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X0_I0+9], v[vgprValuA_X0_I0+9], v0      // HasEccHalf: pack
s_add_u32 s[sgprScalarGlobalReadOffsetA+3], s[sgprScalarGlobalReadOffsetA+3], 64
/* g2l=0, load component 0 */
buffer_load_short_d16 v[vgprValuA_X2_I0+0], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:64 // load one buffer value
/* g2l=0, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:66 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X2_I0+0], v[vgprValuA_X2_I0+0], v0      // HasEccHalf: pack
/* g2l=0, load component 2 */
buffer_load_short_d16 v[vgprValuA_X2_I0+1], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:68 // load one buffer value
/* g2l=0, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], 0 offen offset:70 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X2_I0+1], v[vgprValuA_X2_I0+1], v0      // HasEccHalf: pack
/* g2l=0, load component 0 */
buffer_load_short_d16 v[vgprValuA_X2_I0+8], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+3] offen offset:0 // load one buffer value
/* g2l=0, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+3] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X2_I0+8], v[vgprValuA_X2_I0+8], v0      // HasEccHalf: pack
/* g2l=0, load component 2 */
buffer_load_short_d16 v[vgprValuA_X2_I0+9], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+3] offen offset:4 // load one buffer value
/* g2l=0, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+3] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X2_I0+9], v[vgprValuA_X2_I0+9], v0      // HasEccHalf: pack
/* g2l=0, load component 4 */
buffer_load_short_d16 v[vgprValuA_X0_I0+2], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+0] offen offset:0 // load one buffer value
/* g2l=0, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+0] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X0_I0+2], v[vgprValuA_X0_I0+2], v0      // HasEccHalf: pack
/* g2l=0, load component 6 */
buffer_load_short_d16 v[vgprValuA_X0_I0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+0] offen offset:4 // load one buffer value
/* g2l=0, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+0] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X0_I0+3], v[vgprValuA_X0_I0+3], v0      // HasEccHalf: pack
s_add_u32 s[sgprScalarGlobalReadOffsetA+0], s[sgprScalarGlobalReadOffsetA+0], 64
/* g2l=0, load component 4 */
buffer_load_short_d16 v[vgprValuA_X0_I0+10], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+4] offen offset:0 // load one buffer value
/* g2l=0, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+4] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X0_I0+10], v[vgprValuA_X0_I0+10], v0      // HasEccHalf: pack
/* g2l=0, load component 6 */
buffer_load_short_d16 v[vgprValuA_X0_I0+11], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+4] offen offset:4 // load one buffer value
/* g2l=0, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+4] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X0_I0+11], v[vgprValuA_X0_I0+11], v0      // HasEccHalf: pack
s_add_u32 s[sgprScalarGlobalReadOffsetA+4], s[sgprScalarGlobalReadOffsetA+4], 64
/* g2l=0, load component 4 */
buffer_load_short_d16 v[vgprValuA_X2_I0+2], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+0] offen offset:0 // load one buffer value
/* g2l=0, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+0] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X2_I0+2], v[vgprValuA_X2_I0+2], v0      // HasEccHalf: pack
/* g2l=0, load component 6 */
buffer_load_short_d16 v[vgprValuA_X2_I0+3], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+0] offen offset:4 // load one buffer value
/* g2l=0, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+0] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X2_I0+3], v[vgprValuA_X2_I0+3], v0      // HasEccHalf: pack
/* g2l=0, load component 4 */
buffer_load_short_d16 v[vgprValuA_X2_I0+10], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+4] offen offset:0 // load one buffer value
/* g2l=0, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+4] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X2_I0+10], v[vgprValuA_X2_I0+10], v0      // HasEccHalf: pack
/* g2l=0, load component 6 */
buffer_load_short_d16 v[vgprValuA_X2_I0+11], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+4] offen offset:4 // load one buffer value
/* g2l=0, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+4] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X2_I0+11], v[vgprValuA_X2_I0+11], v0      // HasEccHalf: pack
/* g2l=0, load component 8 */
buffer_load_short_d16 v[vgprValuA_X0_I0+4], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+1] offen offset:0 // load one buffer value
/* g2l=0, load component 9 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+1] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X0_I0+4], v[vgprValuA_X0_I0+4], v0      // HasEccHalf: pack
/* g2l=0, load component 10 */
buffer_load_short_d16 v[vgprValuA_X0_I0+5], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+1] offen offset:4 // load one buffer value
/* g2l=0, load component 11 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+1] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X0_I0+5], v[vgprValuA_X0_I0+5], v0      // HasEccHalf: pack
s_add_u32 s[sgprScalarGlobalReadOffsetA+1], s[sgprScalarGlobalReadOffsetA+1], 64
/* g2l=0, load component 8 */
buffer_load_short_d16 v[vgprValuA_X0_I0+12], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+5] offen offset:0 // load one buffer value
/* g2l=0, load component 9 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+5] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X0_I0+12], v[vgprValuA_X0_I0+12], v0      // HasEccHalf: pack
/* g2l=0, load component 10 */
buffer_load_short_d16 v[vgprValuA_X0_I0+13], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+5] offen offset:4 // load one buffer value
/* g2l=0, load component 11 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+5] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X0_I0+13], v[vgprValuA_X0_I0+13], v0      // HasEccHalf: pack
s_add_u32 s[sgprScalarGlobalReadOffsetA+5], s[sgprScalarGlobalReadOffsetA+5], 64
/* g2l=0, load component 8 */
buffer_load_short_d16 v[vgprValuA_X2_I0+4], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+1] offen offset:0 // load one buffer value
/* g2l=0, load component 9 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+1] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X2_I0+4], v[vgprValuA_X2_I0+4], v0      // HasEccHalf: pack
/* g2l=0, load component 10 */
buffer_load_short_d16 v[vgprValuA_X2_I0+5], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+1] offen offset:4 // load one buffer value
/* g2l=0, load component 11 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+1] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X2_I0+5], v[vgprValuA_X2_I0+5], v0      // HasEccHalf: pack
/* g2l=0, load component 8 */
buffer_load_short_d16 v[vgprValuA_X2_I0+12], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+5] offen offset:0 // load one buffer value
/* g2l=0, load component 9 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+5] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X2_I0+12], v[vgprValuA_X2_I0+12], v0      // HasEccHalf: pack
/* g2l=0, load component 10 */
buffer_load_short_d16 v[vgprValuA_X2_I0+13], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+5] offen offset:4 // load one buffer value
/* g2l=0, load component 11 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+5] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X2_I0+13], v[vgprValuA_X2_I0+13], v0      // HasEccHalf: pack
/* g2l=0, load component 12 */
buffer_load_short_d16 v[vgprValuA_X0_I0+6], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+2] offen offset:0 // load one buffer value
/* g2l=0, load component 13 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+2] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X0_I0+6], v[vgprValuA_X0_I0+6], v0      // HasEccHalf: pack
/* g2l=0, load component 14 */
buffer_load_short_d16 v[vgprValuA_X0_I0+7], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+2] offen offset:4 // load one buffer value
/* g2l=0, load component 15 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+2] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X0_I0+7], v[vgprValuA_X0_I0+7], v0      // HasEccHalf: pack
s_add_u32 s[sgprScalarGlobalReadOffsetA+2], s[sgprScalarGlobalReadOffsetA+2], 64
/* g2l=0, load component 12 */
buffer_load_short_d16 v[vgprValuA_X0_I0+14], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+6] offen offset:0 // load one buffer value
/* g2l=0, load component 13 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+6] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X0_I0+14], v[vgprValuA_X0_I0+14], v0      // HasEccHalf: pack
/* g2l=0, load component 14 */
buffer_load_short_d16 v[vgprValuA_X0_I0+15], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+6] offen offset:4 // load one buffer value
/* g2l=0, load component 15 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+6] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X0_I0+15], v[vgprValuA_X0_I0+15], v0      // HasEccHalf: pack
s_add_u32 s[sgprScalarGlobalReadOffsetA+6], s[sgprScalarGlobalReadOffsetA+6], 64
/* g2l=0, load component 12 */
buffer_load_short_d16 v[vgprValuA_X2_I0+6], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+2] offen offset:0 // load one buffer value
/* g2l=0, load component 13 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+2] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X2_I0+6], v[vgprValuA_X2_I0+6], v0      // HasEccHalf: pack
/* g2l=0, load component 14 */
buffer_load_short_d16 v[vgprValuA_X2_I0+7], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+2] offen offset:4 // load one buffer value
/* g2l=0, load component 15 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+2] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X2_I0+7], v[vgprValuA_X2_I0+7], v0      // HasEccHalf: pack
/* g2l=0, load component 12 */
buffer_load_short_d16 v[vgprValuA_X2_I0+14], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+6] offen offset:0 // load one buffer value
/* g2l=0, load component 13 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+6] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X2_I0+14], v[vgprValuA_X2_I0+14], v0      // HasEccHalf: pack
/* g2l=0, load component 14 */
buffer_load_short_d16 v[vgprValuA_X2_I0+15], v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+6] offen offset:4 // load one buffer value
/* g2l=0, load component 15 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetA+0], s[sgprSrdA:sgprSrdA+3], s[sgprScalarGlobalReadOffsetA+6] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprValuA_X2_I0+15], v[vgprValuA_X2_I0+15], v0      // HasEccHalf: pack

/* Update M0 for DTLDS */

/* global read B */
/* g2l=0, load component 0 */
buffer_load_short_d16 v[vgprG2LB+0+0], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:0 // load one buffer value
/* g2l=0, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+0+0], v[vgprG2LB+0+0], v0      // HasEccHalf: pack
/* g2l=0, load component 2 */
buffer_load_short_d16 v[vgprG2LB+0+1], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:4 // load one buffer value
/* g2l=0, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+0+1], v[vgprG2LB+0+1], v0      // HasEccHalf: pack
/* g2l=0, load component 4 */
buffer_load_short_d16 v[vgprG2LB+0+2], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:8 // load one buffer value
/* g2l=0, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+0+2], v[vgprG2LB+0+2], v0      // HasEccHalf: pack
/* g2l=0, load component 6 */
buffer_load_short_d16 v[vgprG2LB+0+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:12 // load one buffer value
/* g2l=0, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], 0 offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+0+3], v[vgprG2LB+0+3], v0      // HasEccHalf: pack
/* g2l=4, load component 0 */
buffer_load_short_d16 v[vgprG2LB+4+0], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+0] offen offset:0 // load one buffer value
/* g2l=4, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+0] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+4+0], v[vgprG2LB+4+0], v0      // HasEccHalf: pack
/* g2l=4, load component 2 */
buffer_load_short_d16 v[vgprG2LB+4+1], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+0] offen offset:4 // load one buffer value
/* g2l=4, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+0] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+4+1], v[vgprG2LB+4+1], v0      // HasEccHalf: pack
/* g2l=4, load component 4 */
buffer_load_short_d16 v[vgprG2LB+4+2], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+0] offen offset:8 // load one buffer value
/* g2l=4, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+0] offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+4+2], v[vgprG2LB+4+2], v0      // HasEccHalf: pack
/* g2l=4, load component 6 */
buffer_load_short_d16 v[vgprG2LB+4+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+0] offen offset:12 // load one buffer value
/* g2l=4, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+0] offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+4+3], v[vgprG2LB+4+3], v0      // HasEccHalf: pack
/* g2l=8, load component 0 */
buffer_load_short_d16 v[vgprG2LB+8+0], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+1] offen offset:0 // load one buffer value
/* g2l=8, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+1] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+8+0], v[vgprG2LB+8+0], v0      // HasEccHalf: pack
/* g2l=8, load component 2 */
buffer_load_short_d16 v[vgprG2LB+8+1], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+1] offen offset:4 // load one buffer value
/* g2l=8, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+1] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+8+1], v[vgprG2LB+8+1], v0      // HasEccHalf: pack
/* g2l=8, load component 4 */
buffer_load_short_d16 v[vgprG2LB+8+2], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+1] offen offset:8 // load one buffer value
/* g2l=8, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+1] offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+8+2], v[vgprG2LB+8+2], v0      // HasEccHalf: pack
/* g2l=8, load component 6 */
buffer_load_short_d16 v[vgprG2LB+8+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+1] offen offset:12 // load one buffer value
/* g2l=8, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+1] offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+8+3], v[vgprG2LB+8+3], v0      // HasEccHalf: pack
/* g2l=12, load component 0 */
buffer_load_short_d16 v[vgprG2LB+12+0], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+2] offen offset:0 // load one buffer value
/* g2l=12, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+2] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+12+0], v[vgprG2LB+12+0], v0    // HasEccHalf: pack
/* g2l=12, load component 2 */
buffer_load_short_d16 v[vgprG2LB+12+1], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+2] offen offset:4 // load one buffer value
/* g2l=12, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+2] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+12+1], v[vgprG2LB+12+1], v0    // HasEccHalf: pack
/* g2l=12, load component 4 */
buffer_load_short_d16 v[vgprG2LB+12+2], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+2] offen offset:8 // load one buffer value
/* g2l=12, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+2] offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+12+2], v[vgprG2LB+12+2], v0    // HasEccHalf: pack
/* g2l=12, load component 6 */
buffer_load_short_d16 v[vgprG2LB+12+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+2] offen offset:12 // load one buffer value
/* g2l=12, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+2] offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+12+3], v[vgprG2LB+12+3], v0    // HasEccHalf: pack
/* g2l=16, load component 0 */
buffer_load_short_d16 v[vgprG2LB+16+0], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+3] offen offset:0 // load one buffer value
/* g2l=16, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+3] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+16+0], v[vgprG2LB+16+0], v0    // HasEccHalf: pack
/* g2l=16, load component 2 */
buffer_load_short_d16 v[vgprG2LB+16+1], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+3] offen offset:4 // load one buffer value
/* g2l=16, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+3] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+16+1], v[vgprG2LB+16+1], v0    // HasEccHalf: pack
/* g2l=16, load component 4 */
buffer_load_short_d16 v[vgprG2LB+16+2], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+3] offen offset:8 // load one buffer value
/* g2l=16, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+3] offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+16+2], v[vgprG2LB+16+2], v0    // HasEccHalf: pack
/* g2l=16, load component 6 */
buffer_load_short_d16 v[vgprG2LB+16+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+3] offen offset:12 // load one buffer value
/* g2l=16, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+3] offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+16+3], v[vgprG2LB+16+3], v0    // HasEccHalf: pack
/* g2l=20, load component 0 */
buffer_load_short_d16 v[vgprG2LB+20+0], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+4] offen offset:0 // load one buffer value
/* g2l=20, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+4] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+20+0], v[vgprG2LB+20+0], v0    // HasEccHalf: pack
/* g2l=20, load component 2 */
buffer_load_short_d16 v[vgprG2LB+20+1], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+4] offen offset:4 // load one buffer value
/* g2l=20, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+4] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+20+1], v[vgprG2LB+20+1], v0    // HasEccHalf: pack
/* g2l=20, load component 4 */
buffer_load_short_d16 v[vgprG2LB+20+2], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+4] offen offset:8 // load one buffer value
/* g2l=20, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+4] offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+20+2], v[vgprG2LB+20+2], v0    // HasEccHalf: pack
/* g2l=20, load component 6 */
buffer_load_short_d16 v[vgprG2LB+20+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+4] offen offset:12 // load one buffer value
/* g2l=20, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+4] offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+20+3], v[vgprG2LB+20+3], v0    // HasEccHalf: pack
/* g2l=24, load component 0 */
buffer_load_short_d16 v[vgprG2LB+24+0], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+5] offen offset:0 // load one buffer value
/* g2l=24, load component 1 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+5] offen offset:2 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+24+0], v[vgprG2LB+24+0], v0    // HasEccHalf: pack
/* g2l=24, load component 2 */
buffer_load_short_d16 v[vgprG2LB+24+1], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+5] offen offset:4 // load one buffer value
/* g2l=24, load component 3 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+5] offen offset:6 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+24+1], v[vgprG2LB+24+1], v0    // HasEccHalf: pack
/* g2l=24, load component 4 */
buffer_load_short_d16 v[vgprG2LB+24+2], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+5] offen offset:8 // load one buffer value
/* g2l=24, load component 5 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+5] offen offset:10 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+24+2], v[vgprG2LB+24+2], v0    // HasEccHalf: pack
/* g2l=24, load component 6 */
buffer_load_short_d16 v[vgprG2LB+24+3], v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+5] offen offset:12 // load one buffer value
/* g2l=24, load component 7 */
buffer_load_short_d16_hi v0, v[vgprGlobalReadOffsetB+0], s[sgprSrdB:sgprSrdB+3], s[sgprScalarGlobalReadOffsetB+5] offen offset:14 // load one buffer value
s_waitcnt vmcnt(0)
v_or_b32 v[vgprG2LB+24+3], v[vgprG2LB+24+3], v0    // HasEccHalf: pack
s_waitcnt vmcnt(0)                                 // 2wait for global read
// Skip force waitcnt0
s_barrier

/* local write a */

/* local write b */
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+0:vgprG2LB+0+3] offset:0 // lwoB_0_0_0_0 = (0*LSCB)*(MT1J+PAD) + (0*LSPB) = 0
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+4:vgprG2LB+4+3] offset:640 // lwoB_0_0_1_0 = (0*LSCB)*(MT1J+PAD) + (1*LSPB) = 640
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+8:vgprG2LB+8+3] offset:1280 // lwoB_0_0_2_0 = (0*LSCB)*(MT1J+PAD) + (2*LSPB) = 1280
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+12:vgprG2LB+12+3] offset:1920 // lwoB_0_0_3_0 = (0*LSCB)*(MT1J+PAD) + (3*LSPB) = 1920
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+16:vgprG2LB+16+3] offset:2560 // lwoB_0_0_4_0 = (0*LSCB)*(MT1J+PAD) + (4*LSPB) = 2560
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+20:vgprG2LB+20+3] offset:3200 // lwoB_0_0_5_0 = (0*LSCB)*(MT1J+PAD) + (5*LSPB) = 3200
ds_write_b128 v[vgprLocalWriteAddrB], v[vgprG2LB+24:vgprG2LB+24+3] offset:3840 // lwoB_0_0_6_0 = (0*LSCB)*(MT1J+PAD) + (6*LSPB) = 3840

/* Recalc local read offsets */
/* lr0I */
v_and_b32 v1, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_and_b32 v0, 15, v1                               // 1. N offset: nIdx = wtid % MI_N(16)
v_lshlrev_b32 v0, 0x6, v0                          // 1. N offset: nOffset = nIdx * nStride(64)
/* Skip. 2. block offset: bnOffset = 0 when num1DBlocks = 1 */
v_lshlrev_b32 v0, 0x2, v0                          // 4. apply VectorWidth: bnOffset = bnOffset * vw(4)
v_and_b32 v1, 63, v[vgprSerial]                    // 5. thread id in wave: wtid = tid % wavelength(64)
v_lshrrev_b32 v1, 4, v1                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
v_lshlrev_b32 v1, 0x2, v1                          // 5. K offset: lrKOffset = kIdx * mStride(4)
v_add_u32 v0, v1, v0                               // 6. offset in wave: lrOffset = bnOffset + lrKOffset
v_lshrrev_b32 v1, 6, v[vgprSerial]                 // 7. wave offset in N dimen: wtid = tid / dividedForWaveId(64)
v_and_b32 v1, 3, v1                                // 7. wave offset in M dimen: wtid0 = wtid / num1DWaves(4)
v_lshlrev_b32 v1, 0xc, v1                          // 7. wave offset in M dimen: wOffset = wtid0 * W0Stride(4096)
v_add_u32 v0, v1, v0                               // 7. final local read offset: flrOffset = lrOffset + WOffset
/* lr1J */
v_and_b32 v2, 63, v[vgprSerial]                    // 0. thread id in wave: wtid = tid % wavelength(64)
v_and_b32 v1, 15, v2                               // 1. N offset: nIdx = wtid % MI_N(16)
v_lshlrev_b32 v1, 0x6, v1                          // 1. N offset: nOffset = nIdx * nStride(64)
/* Skip. 2. block offset: bnOffset = 0 when num1DBlocks = 1 */
                                                   // 4. apply VectorWidth: bnOffset = bnOffset * vw(1) (multiplier is 1, do nothing)
v_and_b32 v2, 63, v[vgprSerial]                    // 5. thread id in wave: wtid = tid % wavelength(64)
v_lshrrev_b32 v2, 4, v2                            // 5. K offset: kIdx = wtid / (MIN(16) * MIBB(1))
v_lshlrev_b32 v2, 0x2, v2                          // 5. K offset: lrKOffset = kIdx * mStride(4)
v_add_u32 v1, v2, v1                               // 6. offset in wave: lrOffset = bnOffset + lrKOffset
v_lshrrev_b32 v2, 6, v[vgprSerial]                 // v2 = v[vgprSerial] / 64
v_lshrrev_b32 v2, 2, v2                            // LSU offset: Get LSU wave_id
s_mov_b32 s8, 64                                   // LSU offset: stride = lsuStride(64) when umlds==True
v_mul_lo_u32 v2, s8, v2                            // LSU offset: lsuoffset = wave_id*lsuStride*(MT0+PAD)
v_add_lshl_u32 v[vgprLocalReadAddrA], v2, v0, 0x1  // Final Offset: offset = (lro0+lsuoffset)*bpeDS
v_lshrrev_b32 v3, 9, v[vgprLocalReadAddrA]         // Final Offset: padding 32 per block 512
v_lshlrev_b32 v3, 0x5, v3                          // Final Offset: padding 32 per block 512
v_add_u32 v[vgprLocalReadAddrA], v3, v[vgprLocalReadAddrA] // Final Offset: add padding 32 per block 512
/* N/A */
v_lshrrev_b32 v0, 6, v[vgprSerial]                 // v0 = v[vgprSerial] / 64
v_lshrrev_b32 v0, 2, v0                            // LSU offset: Get LSU wave_id
s_mov_b32 s8, 64                                   // LSU offset: stride = lsuStride(64) when umlds==True
v_mul_lo_u32 v0, s8, v0                            // LSU offset: lsuoffset = wave_id*lsuStride*(MT1+PAD)
v_add_lshl_u32 v[vgprLocalReadAddrB], v0, v1, 0x1  // Final Offset: offset = (lro1+lsuoffset)*bpeDS
v_lshrrev_b32 v2, 7, v[vgprLocalReadAddrB]         // Final Offset: padding 32 per block 128
v_lshlrev_b32 v2, 0x5, v2                          // Final Offset: padding 32 per block 128
v_add_u32 v[vgprLocalReadAddrB], v2, v[vgprLocalReadAddrB] // Final Offset: add padding 32 per block 128
s_waitcnt lgkmcnt(0)                               // 5wait for local write
// Skip force waitcnt0
s_barrier

/* local read reset offsets a */

/* local read reset offsets b */

/* local read init pointers a */

/* localReadInitPointers */

/* local read init pointers b */

/* localReadInitPointers */

/* tail loop: macs */
label_TailLoopBeginL:

/* Tail: remove ValuA/B vgpr buffer [0...144) from pool */

/* Tail: add address/G2L vgpr [144...210) to pool */

/* local read a */

/* local read b */
ds_read_b64 v[vgprValuB_X0_I0+0:vgprValuB_X0_I0+0+1], v[vgprLocalReadAddrB] offset:0 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=0 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuB_X0_I0+2:vgprValuB_X0_I0+2+1], v[vgprLocalReadAddrB] offset:2560 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=1 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuB_X0_I0+4:vgprValuB_X0_I0+4+1], v[vgprLocalReadAddrB] offset:5120 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=2 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuB_X0_I0+6:vgprValuB_X0_I0+6+1], v[vgprLocalReadAddrB] offset:7680 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=3 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuB_X0_I0+8:vgprValuB_X0_I0+8+1], v[vgprLocalReadAddrB] offset:10240 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=4 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuB_X0_I0+10:vgprValuB_X0_I0+10+1], v[vgprLocalReadAddrB] offset:12800 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=5 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuB_X0_I0+12:vgprValuB_X0_I0+12+1], v[vgprLocalReadAddrB] offset:15360 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=6 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuB_X0_I0+14:vgprValuB_X0_I0+14+1], v[vgprLocalReadAddrB] offset:17920 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=7 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuB_X0_I0+16:vgprValuB_X0_I0+16+1], v[vgprLocalReadAddrB] offset:20480 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=8 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuB_X0_I0+18:vgprValuB_X0_I0+18+1], v[vgprLocalReadAddrB] offset:23040 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=9 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuB_X0_I0+20:vgprValuB_X0_I0+20+1], v[vgprLocalReadAddrB] offset:25600 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=10 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuB_X0_I0+22:vgprValuB_X0_I0+22+1], v[vgprLocalReadAddrB] offset:28160 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=11 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuB_X0_I0+24:vgprValuB_X0_I0+24+1], v[vgprLocalReadAddrB] offset:30720 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=12 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0
ds_read_b64 v[vgprValuB_X0_I0+26:vgprValuB_X0_I0+26+1], v[vgprLocalReadAddrB] offset:33280 // L -> Reg lro=0 swapByteOffset=0 ti=16 vIdx=13 eIdx=0 rIdx=0 oIdx=0 buffer=0 iui=0

/* local read inc a */
s_mov_b32 s8, 0x20                                 // inc

/* local read inc b */
s_mov_b32 s8, 0x20                                 // inc
v_add_co_u32 v[vgprLocalReadAddrB], vcc, s8, v[vgprLocalReadAddrB] // lrB += 32 (bpeDS)
s_waitcnt lgkmcnt(0)                               // 4wait for local read
v_and_b32 v144, 63, v[vgprSerial]                  // v144 = v[vgprSerial] % 64
v_lshrrev_b32 v144, 4, v144                        // v144 = v144 / 16
v_lshlrev_b32 v144, 0x2, v144                      // v144 = v144 * 4
v_cmp_ge_i32 s[80:81], v144, s[sgprLoopCounterL]   // check K index >= Size L
v_cndmask_b32 v[vgprValuA_X0_I0+0+0], v[vgprValuA_X0_I0+0+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuA_X0_I0+2+0], v[vgprValuA_X0_I0+2+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuA_X0_I0+4+0], v[vgprValuA_X0_I0+4+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuA_X0_I0+6+0], v[vgprValuA_X0_I0+6+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuA_X0_I0+0+1], v[vgprValuA_X0_I0+0+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuA_X0_I0+2+1], v[vgprValuA_X0_I0+2+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuA_X0_I0+4+1], v[vgprValuA_X0_I0+4+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuA_X0_I0+6+1], v[vgprValuA_X0_I0+6+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+0+0], v[vgprValuB_X0_I0+0+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+2+0], v[vgprValuB_X0_I0+2+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+4+0], v[vgprValuB_X0_I0+4+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+6+0], v[vgprValuB_X0_I0+6+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+8+0], v[vgprValuB_X0_I0+8+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+10+0], v[vgprValuB_X0_I0+10+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+12+0], v[vgprValuB_X0_I0+12+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+14+0], v[vgprValuB_X0_I0+14+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+16+0], v[vgprValuB_X0_I0+16+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+18+0], v[vgprValuB_X0_I0+18+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+20+0], v[vgprValuB_X0_I0+20+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+22+0], v[vgprValuB_X0_I0+22+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+24+0], v[vgprValuB_X0_I0+24+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+26+0], v[vgprValuB_X0_I0+26+0], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+0+1], v[vgprValuB_X0_I0+0+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+2+1], v[vgprValuB_X0_I0+2+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+4+1], v[vgprValuB_X0_I0+4+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+6+1], v[vgprValuB_X0_I0+6+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+8+1], v[vgprValuB_X0_I0+8+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+10+1], v[vgprValuB_X0_I0+10+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+12+1], v[vgprValuB_X0_I0+12+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+14+1], v[vgprValuB_X0_I0+14+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+16+1], v[vgprValuB_X0_I0+16+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+18+1], v[vgprValuB_X0_I0+18+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+20+1], v[vgprValuB_X0_I0+20+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+22+1], v[vgprValuB_X0_I0+22+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+24+1], v[vgprValuB_X0_I0+24+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_cndmask_b32 v[vgprValuB_X0_I0+26+1], v[vgprValuB_X0_I0+26+1], 0x0, s[80:81] // set 0 if K_idx >= sizeL
v_sub_u32 v144, s[sgprLoopCounterL], v144          // get distance between size and k index
v_cmp_lt_i32 s[80:81], v144, 4                     // set partial 0 if distance less than input per thread
s_and_b32 s79, s[sgprLoopCounterL], 3              // get inputs for edge thread
s_sub_u32 s79, 4, s79                              // use shift to fill 0 for outside element
s_lshl_b32 s79, s79, 4                             // use shift to fill 0 for outside element
v_lshlrev_b64 v[146:147], s79, v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1]
v_cndmask_b32 v[vgprValuA_X0_I0+0+0+0+0], v[vgprValuA_X0_I0+0+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuA_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+0+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1]
v_cndmask_b32 v[vgprValuA_X0_I0+2+0+0+0], v[vgprValuA_X0_I0+2+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuA_X0_I0+2+0+0+1], v[vgprValuA_X0_I0+2+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1]
v_cndmask_b32 v[vgprValuA_X0_I0+4+0+0+0], v[vgprValuA_X0_I0+4+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuA_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+4+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1]
v_cndmask_b32 v[vgprValuA_X0_I0+6+0+0+0], v[vgprValuA_X0_I0+6+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuA_X0_I0+6+0+0+1], v[vgprValuA_X0_I0+6+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+0+0+0+0], v[vgprValuB_X0_I0+0+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuB_X0_I0+0+0+0+1], v[vgprValuB_X0_I0+0+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuB_X0_I0+2+0+0:vgprValuB_X0_I0+2+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+2+0+0+0], v[vgprValuB_X0_I0+2+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuB_X0_I0+2+0+0+1], v[vgprValuB_X0_I0+2+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+4+0+0+0], v[vgprValuB_X0_I0+4+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuB_X0_I0+4+0+0+1], v[vgprValuB_X0_I0+4+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuB_X0_I0+6+0+0:vgprValuB_X0_I0+6+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+6+0+0+0], v[vgprValuB_X0_I0+6+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuB_X0_I0+6+0+0+1], v[vgprValuB_X0_I0+6+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+8+0+0+0], v[vgprValuB_X0_I0+8+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuB_X0_I0+8+0+0+1], v[vgprValuB_X0_I0+8+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuB_X0_I0+10+0+0:vgprValuB_X0_I0+10+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+10+0+0+0], v[vgprValuB_X0_I0+10+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuB_X0_I0+10+0+0+1], v[vgprValuB_X0_I0+10+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+12+0+0+0], v[vgprValuB_X0_I0+12+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuB_X0_I0+12+0+0+1], v[vgprValuB_X0_I0+12+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuB_X0_I0+14+0+0:vgprValuB_X0_I0+14+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+14+0+0+0], v[vgprValuB_X0_I0+14+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuB_X0_I0+14+0+0+1], v[vgprValuB_X0_I0+14+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+16+0+0+0], v[vgprValuB_X0_I0+16+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuB_X0_I0+16+0+0+1], v[vgprValuB_X0_I0+16+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuB_X0_I0+18+0+0:vgprValuB_X0_I0+18+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+18+0+0+0], v[vgprValuB_X0_I0+18+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuB_X0_I0+18+0+0+1], v[vgprValuB_X0_I0+18+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+20+0+0+0], v[vgprValuB_X0_I0+20+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuB_X0_I0+20+0+0+1], v[vgprValuB_X0_I0+20+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuB_X0_I0+22+0+0:vgprValuB_X0_I0+22+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+22+0+0+0], v[vgprValuB_X0_I0+22+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuB_X0_I0+22+0+0+1], v[vgprValuB_X0_I0+22+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+24+0+0+0], v[vgprValuB_X0_I0+24+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuB_X0_I0+24+0+0+1], v[vgprValuB_X0_I0+24+0+0+1], v147, s[80:81]
v_lshlrev_b64 v[146:147], s79, v[vgprValuB_X0_I0+26+0+0:vgprValuB_X0_I0+26+0+0+1]
v_cndmask_b32 v[vgprValuB_X0_I0+26+0+0+0], v[vgprValuB_X0_I0+26+0+0+0], v146, s[80:81]
v_cndmask_b32 v[vgprValuB_X0_I0+26+0+0+1], v[vgprValuB_X0_I0+26+0+0+1], v147, s[80:81]
s_nop 1
v_mfma_f32_16x16x16bf16_1k acc[0:3], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[0:3] // left value = acc[0+0:3+0]
v_mfma_f32_16x16x16bf16_1k acc[4:7], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[4:7] // left value = acc[4+0:7+0]
v_mfma_f32_16x16x16bf16_1k acc[8:11], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[8:11] // left value = acc[8+0:11+0]
v_mfma_f32_16x16x16bf16_1k acc[12:15], v[vgprValuB_X0_I0+0+0+0:vgprValuB_X0_I0+0+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[12:15] // left value = acc[12+0:15+0]
v_mfma_f32_16x16x16bf16_1k acc[16:19], v[vgprValuB_X0_I0+2+0+0:vgprValuB_X0_I0+2+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[16:19] // left value = acc[16+0:19+0]
v_mfma_f32_16x16x16bf16_1k acc[20:23], v[vgprValuB_X0_I0+2+0+0:vgprValuB_X0_I0+2+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[20:23] // left value = acc[20+0:23+0]
v_mfma_f32_16x16x16bf16_1k acc[24:27], v[vgprValuB_X0_I0+2+0+0:vgprValuB_X0_I0+2+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[24:27] // left value = acc[24+0:27+0]
v_mfma_f32_16x16x16bf16_1k acc[28:31], v[vgprValuB_X0_I0+2+0+0:vgprValuB_X0_I0+2+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[28:31] // left value = acc[28+0:31+0]
v_mfma_f32_16x16x16bf16_1k acc[32:35], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[32:35] // left value = acc[32+0:35+0]
v_mfma_f32_16x16x16bf16_1k acc[36:39], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[36:39] // left value = acc[36+0:39+0]
v_mfma_f32_16x16x16bf16_1k acc[40:43], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[40:43] // left value = acc[40+0:43+0]
v_mfma_f32_16x16x16bf16_1k acc[44:47], v[vgprValuB_X0_I0+4+0+0:vgprValuB_X0_I0+4+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[44:47] // left value = acc[44+0:47+0]
v_mfma_f32_16x16x16bf16_1k acc[48:51], v[vgprValuB_X0_I0+6+0+0:vgprValuB_X0_I0+6+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[48:51] // left value = acc[48+0:51+0]
v_mfma_f32_16x16x16bf16_1k acc[52:55], v[vgprValuB_X0_I0+6+0+0:vgprValuB_X0_I0+6+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[52:55] // left value = acc[52+0:55+0]
v_mfma_f32_16x16x16bf16_1k acc[56:59], v[vgprValuB_X0_I0+6+0+0:vgprValuB_X0_I0+6+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[56:59] // left value = acc[56+0:59+0]
v_mfma_f32_16x16x16bf16_1k acc[60:63], v[vgprValuB_X0_I0+6+0+0:vgprValuB_X0_I0+6+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[60:63] // left value = acc[60+0:63+0]
v_mfma_f32_16x16x16bf16_1k acc[64:67], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[64:67] // left value = acc[64+0:67+0]
v_mfma_f32_16x16x16bf16_1k acc[68:71], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[68:71] // left value = acc[68+0:71+0]
v_mfma_f32_16x16x16bf16_1k acc[72:75], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[72:75] // left value = acc[72+0:75+0]
v_mfma_f32_16x16x16bf16_1k acc[76:79], v[vgprValuB_X0_I0+8+0+0:vgprValuB_X0_I0+8+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[76:79] // left value = acc[76+0:79+0]
v_mfma_f32_16x16x16bf16_1k acc[80:83], v[vgprValuB_X0_I0+10+0+0:vgprValuB_X0_I0+10+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[80:83] // left value = acc[80+0:83+0]
v_mfma_f32_16x16x16bf16_1k acc[84:87], v[vgprValuB_X0_I0+10+0+0:vgprValuB_X0_I0+10+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[84:87] // left value = acc[84+0:87+0]
v_mfma_f32_16x16x16bf16_1k acc[88:91], v[vgprValuB_X0_I0+10+0+0:vgprValuB_X0_I0+10+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[88:91] // left value = acc[88+0:91+0]
v_mfma_f32_16x16x16bf16_1k acc[92:95], v[vgprValuB_X0_I0+10+0+0:vgprValuB_X0_I0+10+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[92:95] // left value = acc[92+0:95+0]
v_mfma_f32_16x16x16bf16_1k acc[96:99], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[96:99] // left value = acc[96+0:99+0]
v_mfma_f32_16x16x16bf16_1k acc[100:103], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[100:103] // left value = acc[100+0:103+0]
v_mfma_f32_16x16x16bf16_1k acc[104:107], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[104:107] // left value = acc[104+0:107+0]
v_mfma_f32_16x16x16bf16_1k acc[108:111], v[vgprValuB_X0_I0+12+0+0:vgprValuB_X0_I0+12+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[108:111] // left value = acc[108+0:111+0]
v_mfma_f32_16x16x16bf16_1k acc[112:115], v[vgprValuB_X0_I0+14+0+0:vgprValuB_X0_I0+14+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[112:115] // left value = acc[112+0:115+0]
v_mfma_f32_16x16x16bf16_1k acc[116:119], v[vgprValuB_X0_I0+14+0+0:vgprValuB_X0_I0+14+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[116:119] // left value = acc[116+0:119+0]
v_mfma_f32_16x16x16bf16_1k acc[120:123], v[vgprValuB_X0_I0+14+0+0:vgprValuB_X0_I0+14+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[120:123] // left value = acc[120+0:123+0]
v_mfma_f32_16x16x16bf16_1k acc[124:127], v[vgprValuB_X0_I0+14+0+0:vgprValuB_X0_I0+14+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[124:127] // left value = acc[124+0:127+0]
v_mfma_f32_16x16x16bf16_1k acc[128:131], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[128:131] // left value = acc[128+0:131+0]
v_mfma_f32_16x16x16bf16_1k acc[132:135], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[132:135] // left value = acc[132+0:135+0]
v_mfma_f32_16x16x16bf16_1k acc[136:139], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[136:139] // left value = acc[136+0:139+0]
v_mfma_f32_16x16x16bf16_1k acc[140:143], v[vgprValuB_X0_I0+16+0+0:vgprValuB_X0_I0+16+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[140:143] // left value = acc[140+0:143+0]
v_mfma_f32_16x16x16bf16_1k acc[144:147], v[vgprValuB_X0_I0+18+0+0:vgprValuB_X0_I0+18+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[144:147] // left value = acc[144+0:147+0]
v_mfma_f32_16x16x16bf16_1k acc[148:151], v[vgprValuB_X0_I0+18+0+0:vgprValuB_X0_I0+18+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[148:151] // left value = acc[148+0:151+0]
v_mfma_f32_16x16x16bf16_1k acc[152:155], v[vgprValuB_X0_I0+18+0+0:vgprValuB_X0_I0+18+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[152:155] // left value = acc[152+0:155+0]
v_mfma_f32_16x16x16bf16_1k acc[156:159], v[vgprValuB_X0_I0+18+0+0:vgprValuB_X0_I0+18+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[156:159] // left value = acc[156+0:159+0]
v_mfma_f32_16x16x16bf16_1k acc[160:163], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[160:163] // left value = acc[160+0:163+0]
v_mfma_f32_16x16x16bf16_1k acc[164:167], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[164:167] // left value = acc[164+0:167+0]
v_mfma_f32_16x16x16bf16_1k acc[168:171], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[168:171] // left value = acc[168+0:171+0]
v_mfma_f32_16x16x16bf16_1k acc[172:175], v[vgprValuB_X0_I0+20+0+0:vgprValuB_X0_I0+20+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[172:175] // left value = acc[172+0:175+0]
v_mfma_f32_16x16x16bf16_1k acc[176:179], v[vgprValuB_X0_I0+22+0+0:vgprValuB_X0_I0+22+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[176:179] // left value = acc[176+0:179+0]
v_mfma_f32_16x16x16bf16_1k acc[180:183], v[vgprValuB_X0_I0+22+0+0:vgprValuB_X0_I0+22+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[180:183] // left value = acc[180+0:183+0]
v_mfma_f32_16x16x16bf16_1k acc[184:187], v[vgprValuB_X0_I0+22+0+0:vgprValuB_X0_I0+22+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[184:187] // left value = acc[184+0:187+0]
v_mfma_f32_16x16x16bf16_1k acc[188:191], v[vgprValuB_X0_I0+22+0+0:vgprValuB_X0_I0+22+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[188:191] // left value = acc[188+0:191+0]
v_mfma_f32_16x16x16bf16_1k acc[192:195], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[192:195] // left value = acc[192+0:195+0]
v_mfma_f32_16x16x16bf16_1k acc[196:199], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[196:199] // left value = acc[196+0:199+0]
v_mfma_f32_16x16x16bf16_1k acc[200:203], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[200:203] // left value = acc[200+0:203+0]
v_mfma_f32_16x16x16bf16_1k acc[204:207], v[vgprValuB_X0_I0+24+0+0:vgprValuB_X0_I0+24+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[204:207] // left value = acc[204+0:207+0]
v_mfma_f32_16x16x16bf16_1k acc[208:211], v[vgprValuB_X0_I0+26+0+0:vgprValuB_X0_I0+26+0+0+1], v[vgprValuA_X0_I0+0+0+0:vgprValuA_X0_I0+0+0+0+1], acc[208:211] // left value = acc[208+0:211+0]
v_mfma_f32_16x16x16bf16_1k acc[212:215], v[vgprValuB_X0_I0+26+0+0:vgprValuB_X0_I0+26+0+0+1], v[vgprValuA_X0_I0+2+0+0:vgprValuA_X0_I0+2+0+0+1], acc[212:215] // left value = acc[212+0:215+0]
v_mfma_f32_16x16x16bf16_1k acc[216:219], v[vgprValuB_X0_I0+26+0+0:vgprValuB_X0_I0+26+0+0+1], v[vgprValuA_X0_I0+4+0+0:vgprValuA_X0_I0+4+0+0+1], acc[216:219] // left value = acc[216+0:219+0]
v_mfma_f32_16x16x16bf16_1k acc[220:223], v[vgprValuB_X0_I0+26+0+0:vgprValuB_X0_I0+26+0+0+1], v[vgprValuA_X0_I0+6+0+0:vgprValuA_X0_I0+6+0+0+1], acc[220:223] // left value = acc[220+0:223+0]

v_mov_b32 v[vgprValuA_X0_I0+0+0], v[vgprValuA_X0_I0+8+0]
v_mov_b32 v[vgprValuA_X0_I0+1+0], v[vgprValuA_X0_I0+9+0]
v_mov_b32 v[vgprValuA_X0_I0+2+0], v[vgprValuA_X0_I0+10+0]
v_mov_b32 v[vgprValuA_X0_I0+3+0], v[vgprValuA_X0_I0+11+0]
v_mov_b32 v[vgprValuA_X0_I0+4+0], v[vgprValuA_X0_I0+12+0]
v_mov_b32 v[vgprValuA_X0_I0+5+0], v[vgprValuA_X0_I0+13+0]
v_mov_b32 v[vgprValuA_X0_I0+6+0], v[vgprValuA_X0_I0+14+0]
v_mov_b32 v[vgprValuA_X0_I0+7+0], v[vgprValuA_X0_I0+15+0]
v_mov_b32 v[vgprValuA_X0_I0+8+0], v[vgprValuA_X2_I0+0+0]
v_mov_b32 v[vgprValuA_X0_I0+9+0], v[vgprValuA_X2_I0+1+0]
v_mov_b32 v[vgprValuA_X0_I0+10+0], v[vgprValuA_X2_I0+2+0]
v_mov_b32 v[vgprValuA_X0_I0+11+0], v[vgprValuA_X2_I0+3+0]
v_mov_b32 v[vgprValuA_X0_I0+12+0], v[vgprValuA_X2_I0+4+0]
v_mov_b32 v[vgprValuA_X0_I0+13+0], v[vgprValuA_X2_I0+5+0]
v_mov_b32 v[vgprValuA_X0_I0+14+0], v[vgprValuA_X2_I0+6+0]
v_mov_b32 v[vgprValuA_X0_I0+15+0], v[vgprValuA_X2_I0+7+0]
v_mov_b32 v[vgprValuA_X2_I0+0+0], v[vgprValuA_X2_I0+8+0]
v_mov_b32 v[vgprValuA_X2_I0+1+0], v[vgprValuA_X2_I0+9+0]
v_mov_b32 v[vgprValuA_X2_I0+2+0], v[vgprValuA_X2_I0+10+0]
v_mov_b32 v[vgprValuA_X2_I0+3+0], v[vgprValuA_X2_I0+11+0]
v_mov_b32 v[vgprValuA_X2_I0+4+0], v[vgprValuA_X2_I0+12+0]
v_mov_b32 v[vgprValuA_X2_I0+5+0], v[vgprValuA_X2_I0+13+0]
v_mov_b32 v[vgprValuA_X2_I0+6+0], v[vgprValuA_X2_I0+14+0]
v_mov_b32 v[vgprValuA_X2_I0+7+0], v[vgprValuA_X2_I0+15+0]

/* closeLoop loopL finalLoop=1 tailLoop=1 */
s_sub_i32 s[sgprLoopCounterL], s[sgprLoopCounterL], 0x10 // dec counterL (tailLoop)
s_add_u32 s[sgprOrigLoopCounter], s[sgprOrigLoopCounter], 0x10 // inc counterL
s_cmp_le_i32 s[sgprLoopCounterL], 0x0              // counterL<=0
s_cbranch_scc0 label_TailLoopBeginL                // restart LoopL
label_TailLoopEndL:
label_SkipTailLoopL:

/* Tail: remove address/G2L [144...210) from pool */
label_Summation_End_CDN0AHSW4VIUUZIY_0:
/* endSummation: add vgpr [0...210) to pool */
.set sgprWGM, UNDEF
.set sgprLoopCounterL, UNDEF
.set sgprOrigLoopCounter, UNDEF
.set sgprAddressA, UNDEF
.set sgprAddressB, UNDEF
.set sgprStridesA, UNDEF
.set sgprStridesB, UNDEF
.set sgprStaggerUIter, UNDEF
.set sgprSrdA, UNDEF
.set sgprSrdB, UNDEF
.set sgprShadowLimitA, UNDEF
.set sgprShadowLimitB, UNDEF
.set sgprWrapUA, UNDEF
.set sgprWrapUB, UNDEF
.set sgprGlobalReadIncsA, UNDEF
.set sgprGlobalReadIncsB, UNDEF
.set sgprScalarGlobalReadOffsetA, UNDEF
.set sgprScalarGlobalReadOffsetB, UNDEF
/* load store sgprs */
.set sgprAddressScaleAlphaVec, 48
.set sgprAddressBias, 50
.set sgprBiasType, 52
.set sgprBiasStride, 53
.set sgpractivationAlpha, 54
.set sgpractivationBeta, 55
.set sgprActivationType, 56
s_cmp_eq_u32 s[sgprGSU], 1                         // GSU == 1 ?
s_cbranch_scc0 label_GSU_4                         // branch if GSU != 1
/* Check if custom structure pointer is null */
s_cmp_eq_u32 s[sgprArgType], 2                     // ArgType == 2 ?
s_cbranch_scc1 label_LoadExternalEpilogueStruct_1  // branch if ArgType == 2
s_load_dwordx8 s[48:55], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x58
s_load_dword s56, s[sgprKernArgAddress:sgprKernArgAddress+1], 0x78
s_branch label_LoadExternalEpilogueStructEnd_1
label_LoadExternalEpilogueStruct_1:
s_load_dwordx4 s[48:51], s[sgprKernArgAddress:sgprKernArgAddress+1], 0x90
s_load_dwordx2 s[52:53], s[sgprKernArgAddress:sgprKernArgAddress+1], 0xa0
s_load_dwordx2 s[54:55], s[sgprKernArgAddress:sgprKernArgAddress+1], 0xb8
s_load_dword s56, s[sgprKernArgAddress:sgprKernArgAddress+1], 0xc0
label_LoadExternalEpilogueStructEnd_1:
label_GSU_4:
.set sgprSrdScaleAlphaVec, 32
.set sgprSrdBias, 40

/* Mapping of Acc register -> C Vgpr register */

/* not-LocalSplitU: global write indices */
/* computeStoreVgprs */
v_lshrrev_b32 v4, 6, v[vgprSerial]                 // v4 = v[vgprSerial] / 64
v_lshrrev_b32 v5, 2, v4                            // v5 = v4 / 4
v_mul_lo_u32 v5, 0x10, v5                          // wave coordination offset 1
v_and_b32 v1, 63, v[vgprSerial]                    // v1 = v[vgprSerial] % 64
v_lshrrev_b32 v1, 4, v1                            // v1 = v1 / 16
v_lshlrev_b32 v1, 0x2, v1                          // thread0 * continuous_output
v_add_lshl_u32 v1, v5, v1, 0                       // coordination 1 = vwB *(wave_id1 + tid1)
v_mul_lo_u32 v2, v1, s[sgprStrideC1J]              //  offset 1
v_mul_lo_u32 v3, v1, s[sgprStrideD1J]              //  offset 1
v_and_b32 v0, 3, v4                                // v0 = v4 % 4
v_mul_lo_u32 v0, 0x10, v0                          // wave coordination offset 0
v_and_b32 v5, 15, v[vgprSerial]                    // v5 = v[vgprSerial] % 16
v_add_lshl_u32 v0, v5, v0, 2                       // coordination 0 = vwA * (wave_id0 + tid0)
s_mul_i32 s8, 256, s[sgprWorkGroup0]               // wgp0 * MT0
v_add_u32 v0, s8, v0                               // coord 0 = (tid0/MI_m)*4 + waveG0*MIB_m + MT0*SG0
s_mul_i32 s8, 224, s[sgprWorkGroup1]               // wgp1 * MT1
v_add_u32 v1, s8, v1                               // coord 1 = (tid0%MI_m) + waveG1*MIB_n + MT1*SG1

/* not-LocalSplitU: global write */

/******************************************/
/* Global Write Elements                  */
/******************************************/
s_waitcnt lgkmcnt(0)                               // wait for 36 bytes of kern args.
s_cmp_eq_u32 s[sgprGSU], 1                         // GSU == 1 ?
s_cbranch_scc1 label_GSU_5                         // branch if GSU == 1
s_and_b32 s58, 255, s[sgprSizeI]                   // s58 = s[sgprSizeI] % 256
s_add_u32 s59, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s59                // wg0 >= nwg0-1 ?
s_cselect_b32 s58, s58, 0                          // set rMT0
s_cmpk_gt_u32 s58, 0x0                             // rMT0 > 0
s_cbranch_scc1 label_GW_B0_E1_M                    // jump if edges required
s_mov_b32 s61, 0x0                                 // STATIC_DIV: divisior=224
s_mul_i32 s60, 0x249, s[sgprSizeJ]                 // tmp1 = dividend * magic hi
s_lshl_b64 s[60:61], s[60:61], 0x10                // left shift 16 bits
s_mul_i32 s59, s[sgprSizeJ], 0x2493                // tmp0 = dividend * magic lo
s_add_u32 s60, s59, s60                            // add lo
s_addc_u32 s61, s61, 0x0                           // add hi
s_lshr_b64 s[60:61], s[60:61], 0x21                // tmp1 = (dividend * magic) << shift
s_mov_b32 s59, s60                                 // quotient
s_mul_i32 s60, s59, 0xe0                           // quotient*divisor
s_sub_u32 s58, s[sgprSizeJ], s60                   // rReg = dividend - quotient*divisor
s_add_u32 s59, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s59                // wg1 >= nwg1-1
s_cselect_b32 s58, s58, 0                          // set rMT1
s_cmpk_gt_u32 s58, 0x0                             // rMT1 > 0
s_cbranch_scc1 label_GW_B0_E1_N                    // jump if edges required
label_GW_B0_E0_1:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=16 */
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 biasDim=0 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4); (1,0,0,0:vw4); (1,0,1,0:vw4); (1,0,2,0:vw4); (1,0,3,0:vw4); (2,0,0,0:vw4); (2,0,1,0:vw4); (2,0,2,0:vw4); (2,0,3,0:vw4); (3,0,0,0:vw4); (3,0,1,0:vw4); (3,0,2,0:vw4); (3,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
/* (d1,vc1,d0,vc0)=(2,0,0,0) */
/* (d1,vc1,d0,vc0)=(2,1,0,0) */
/* (d1,vc1,d0,vc0)=(2,2,0,0) */
/* (d1,vc1,d0,vc0)=(2,3,0,0) */
/* (d1,vc1,d0,vc0)=(3,0,0,0) */
/* (d1,vc1,d0,vc0)=(3,1,0,0) */
/* (d1,vc1,d0,vc0)=(3,2,0,0) */
/* (d1,vc1,d0,vc0)=(3,3,0,0) */
v_add_lshl_u32 v10, v3, v0, 0x2                    // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+12], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+13], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+14], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+15], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+16], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+17], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+18], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+19], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+20], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+21], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+22], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+23], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+24], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+25], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+26], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+27], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+28], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+29], acc20          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+30], acc24          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+31], acc28          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+32], acc17          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+33], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+34], acc25          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+35], acc29          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+36], acc18          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+37], acc22          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+38], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+39], acc30          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+40], acc19          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+41], acc23          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+42], acc27          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+43], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+44], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+45], acc36          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+46], acc40          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+47], acc44          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+48], acc33          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+49], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+50], acc41          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+51], acc45          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+52], acc34          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+53], acc38          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+54], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+55], acc46          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+56], acc35          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+57], acc39          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+58], acc43          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+59], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+60], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+61], acc52          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+62], acc56          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+63], acc60          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+64], acc49          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+65], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+66], acc57          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+67], acc61          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+68], acc50          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+69], acc54          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+70], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+71], acc62          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+72], acc51          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+73], acc55          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+74], acc59          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+75], acc63          // copy acc to vreg[63]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0), (1, 0, 0, 0), (1, 0, 1, 0), (1, 0, 2, 0), (1, 0, 3, 0), (2, 0, 0, 0), (2, 0, 1, 0), (2, 0, 2, 0), (2, 0, 3, 0), (3, 0, 0, 0), (3, 0, 1, 0), (3, 0, 2, 0), (3, 0, 3, 0)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dwordx4 v[12:15], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[16:19], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[20:23], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[24:27], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_mul_i32 s12, s[sgprStrideD1J], 52                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[28:31], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[36:39], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[40:43], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_mul_i32 s12, s[sgprStrideD1J], 52                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[44:47], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[48:51], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[52:55], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[56:59], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_mul_i32 s12, s[sgprStrideD1J], 52                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[60:63], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[64:67], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[68:71], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[72:75], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 biasDim=0 */

/******************************************/
/* Global Write Batch #1 (d1,d0,vc1,vc0) = */
/*    (4,0,0,0:vw4); (4,0,1,0:vw4); (4,0,2,0:vw4); (4,0,3,0:vw4); (5,0,0,0:vw4); (5,0,1,0:vw4); (5,0,2,0:vw4); (5,0,3,0:vw4); (6,0,0,0:vw4); (6,0,1,0:vw4); (6,0,2,0:vw4); (6,0,3,0:vw4); (7,0,0,0:vw4); (7,0,1,0:vw4); (7,0,2,0:vw4); (7,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(4,0,0,0) */
/* (d1,vc1,d0,vc0)=(4,1,0,0) */
/* (d1,vc1,d0,vc0)=(4,2,0,0) */
/* (d1,vc1,d0,vc0)=(4,3,0,0) */
/* (d1,vc1,d0,vc0)=(5,0,0,0) */
/* (d1,vc1,d0,vc0)=(5,1,0,0) */
/* (d1,vc1,d0,vc0)=(5,2,0,0) */
/* (d1,vc1,d0,vc0)=(5,3,0,0) */
/* (d1,vc1,d0,vc0)=(6,0,0,0) */
/* (d1,vc1,d0,vc0)=(6,1,0,0) */
/* (d1,vc1,d0,vc0)=(6,2,0,0) */
/* (d1,vc1,d0,vc0)=(6,3,0,0) */
/* (d1,vc1,d0,vc0)=(7,0,0,0) */
/* (d1,vc1,d0,vc0)=(7,1,0,0) */
/* (d1,vc1,d0,vc0)=(7,2,0,0) */
/* (d1,vc1,d0,vc0)=(7,3,0,0) */
v_accvgpr_read_b32 v[vgprValuC+12], acc64          // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+13], acc68          // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+14], acc72          // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+15], acc76          // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+16], acc65          // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+17], acc69          // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+18], acc73          // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+19], acc77          // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+20], acc66          // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+21], acc70          // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+22], acc74          // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+23], acc78          // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+24], acc67          // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+25], acc71          // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+26], acc75          // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+27], acc79          // copy acc to vreg[79]
v_accvgpr_read_b32 v[vgprValuC+28], acc80          // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+29], acc84          // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+30], acc88          // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+31], acc92          // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+32], acc81          // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+33], acc85          // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+34], acc89          // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+35], acc93          // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+36], acc82          // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+37], acc86          // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+38], acc90          // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+39], acc94          // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+40], acc83          // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+41], acc87          // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+42], acc91          // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+43], acc95          // copy acc to vreg[95]
v_accvgpr_read_b32 v[vgprValuC+44], acc96          // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+45], acc100         // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+46], acc104         // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+47], acc108         // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+48], acc97          // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+49], acc101         // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+50], acc105         // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+51], acc109         // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+52], acc98          // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+53], acc102         // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+54], acc106         // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+55], acc110         // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+56], acc99          // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+57], acc103         // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+58], acc107         // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+59], acc111         // copy acc to vreg[111]
v_accvgpr_read_b32 v[vgprValuC+60], acc112         // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+61], acc116         // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+62], acc120         // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+63], acc124         // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+64], acc113         // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+65], acc117         // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+66], acc121         // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+67], acc125         // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+68], acc114         // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+69], acc118         // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+70], acc122         // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+71], acc126         // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+72], acc115         // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+73], acc119         // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+74], acc123         // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+75], acc127         // copy acc to vreg[127]

/* rC *= alpha batchElements=[(4, 0, 0, 0), (4, 0, 1, 0), (4, 0, 2, 0), (4, 0, 3, 0), (5, 0, 0, 0), (5, 0, 1, 0), (5, 0, 2, 0), (5, 0, 3, 0), (6, 0, 0, 0), (6, 0, 1, 0), (6, 0, 2, 0), (6, 0, 3, 0), (7, 0, 0, 0), (7, 0, 1, 0), (7, 0, 2, 0), (7, 0, 3, 0)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
s_mul_i32 s12, s[sgprStrideD1J], 52                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[12:15], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[16:19], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[20:23], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[24:27], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_mul_i32 s12, s[sgprStrideD1J], 52                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[28:31], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[36:39], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[40:43], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_mul_i32 s12, s[sgprStrideD1J], 52                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[44:47], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[48:51], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[52:55], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[56:59], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_mul_i32 s12, s[sgprStrideD1J], 52                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[60:63], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[64:67], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[68:71], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[72:75], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 biasDim=0 */

/******************************************/
/* Global Write Batch #2 (d1,d0,vc1,vc0) = */
/*    (8,0,0,0:vw4); (8,0,1,0:vw4); (8,0,2,0:vw4); (8,0,3,0:vw4); (9,0,0,0:vw4); (9,0,1,0:vw4); (9,0,2,0:vw4); (9,0,3,0:vw4); (10,0,0,0:vw4); (10,0,1,0:vw4); (10,0,2,0:vw4); (10,0,3,0:vw4); (11,0,0,0:vw4); (11,0,1,0:vw4); (11,0,2,0:vw4); (11,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(8,0,0,0) */
/* (d1,vc1,d0,vc0)=(8,1,0,0) */
/* (d1,vc1,d0,vc0)=(8,2,0,0) */
/* (d1,vc1,d0,vc0)=(8,3,0,0) */
/* (d1,vc1,d0,vc0)=(9,0,0,0) */
/* (d1,vc1,d0,vc0)=(9,1,0,0) */
/* (d1,vc1,d0,vc0)=(9,2,0,0) */
/* (d1,vc1,d0,vc0)=(9,3,0,0) */
/* (d1,vc1,d0,vc0)=(10,0,0,0) */
/* (d1,vc1,d0,vc0)=(10,1,0,0) */
/* (d1,vc1,d0,vc0)=(10,2,0,0) */
/* (d1,vc1,d0,vc0)=(10,3,0,0) */
/* (d1,vc1,d0,vc0)=(11,0,0,0) */
/* (d1,vc1,d0,vc0)=(11,1,0,0) */
/* (d1,vc1,d0,vc0)=(11,2,0,0) */
/* (d1,vc1,d0,vc0)=(11,3,0,0) */
v_accvgpr_read_b32 v[vgprValuC+12], acc128         // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+13], acc132         // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+14], acc136         // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+15], acc140         // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+16], acc129         // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+17], acc133         // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+18], acc137         // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+19], acc141         // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+20], acc130         // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+21], acc134         // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+22], acc138         // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+23], acc142         // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+24], acc131         // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+25], acc135         // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+26], acc139         // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+27], acc143         // copy acc to vreg[143]
v_accvgpr_read_b32 v[vgprValuC+28], acc144         // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+29], acc148         // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+30], acc152         // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+31], acc156         // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+32], acc145         // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+33], acc149         // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+34], acc153         // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+35], acc157         // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+36], acc146         // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+37], acc150         // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+38], acc154         // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+39], acc158         // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+40], acc147         // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+41], acc151         // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+42], acc155         // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+43], acc159         // copy acc to vreg[159]
v_accvgpr_read_b32 v[vgprValuC+44], acc160         // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+45], acc164         // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+46], acc168         // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+47], acc172         // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+48], acc161         // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+49], acc165         // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+50], acc169         // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+51], acc173         // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+52], acc162         // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+53], acc166         // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+54], acc170         // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+55], acc174         // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+56], acc163         // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+57], acc167         // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+58], acc171         // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+59], acc175         // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+60], acc176         // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+61], acc180         // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+62], acc184         // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+63], acc188         // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+64], acc177         // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+65], acc181         // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+66], acc185         // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+67], acc189         // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+68], acc178         // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+69], acc182         // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+70], acc186         // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+71], acc190         // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+72], acc179         // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+73], acc183         // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+74], acc187         // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+75], acc191         // copy acc to vreg[191]

/* rC *= alpha batchElements=[(8, 0, 0, 0), (8, 0, 1, 0), (8, 0, 2, 0), (8, 0, 3, 0), (9, 0, 0, 0), (9, 0, 1, 0), (9, 0, 2, 0), (9, 0, 3, 0), (10, 0, 0, 0), (10, 0, 1, 0), (10, 0, 2, 0), (10, 0, 3, 0), (11, 0, 0, 0), (11, 0, 1, 0), (11, 0, 2, 0), (11, 0, 3, 0)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
s_mul_i32 s12, s[sgprStrideD1J], 52                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[12:15], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[16:19], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[20:23], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[24:27], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_mul_i32 s12, s[sgprStrideD1J], 52                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[28:31], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[36:39], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[40:43], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_mul_i32 s12, s[sgprStrideD1J], 52                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[44:47], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[48:51], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[52:55], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[56:59], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_mul_i32 s12, s[sgprStrideD1J], 52                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[60:63], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[64:67], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[68:71], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[72:75], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 biasDim=0 */

/******************************************/
/* Global Write Batch #3 (d1,d0,vc1,vc0) = */
/*    (12,0,0,0:vw4); (12,0,1,0:vw4); (12,0,2,0:vw4); (12,0,3,0:vw4); (13,0,0,0:vw4); (13,0,1,0:vw4); (13,0,2,0:vw4); (13,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(12,0,0,0) */
/* (d1,vc1,d0,vc0)=(12,1,0,0) */
/* (d1,vc1,d0,vc0)=(12,2,0,0) */
/* (d1,vc1,d0,vc0)=(12,3,0,0) */
/* (d1,vc1,d0,vc0)=(13,0,0,0) */
/* (d1,vc1,d0,vc0)=(13,1,0,0) */
/* (d1,vc1,d0,vc0)=(13,2,0,0) */
/* (d1,vc1,d0,vc0)=(13,3,0,0) */
v_accvgpr_read_b32 v[vgprValuC+12], acc192         // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+13], acc196         // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+14], acc200         // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+15], acc204         // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+16], acc193         // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+17], acc197         // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+18], acc201         // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+19], acc205         // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+20], acc194         // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+21], acc198         // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+22], acc202         // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+23], acc206         // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+24], acc195         // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+25], acc199         // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+26], acc203         // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+27], acc207         // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+28], acc208         // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+29], acc212         // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+30], acc216         // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+31], acc220         // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+32], acc209         // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+33], acc213         // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+34], acc217         // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+35], acc221         // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+36], acc210         // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+37], acc214         // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+38], acc218         // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+39], acc222         // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+40], acc211         // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+41], acc215         // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+42], acc219         // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+43], acc223         // copy acc to vreg[223]

/* rC *= alpha batchElements=[(12, 0, 0, 0), (12, 0, 1, 0), (12, 0, 2, 0), (12, 0, 3, 0), (13, 0, 0, 0), (13, 0, 1, 0), (13, 0, 2, 0), (13, 0, 3, 0)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
s_mul_i32 s12, s[sgprStrideD1J], 52                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[12:15], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[16:19], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[20:23], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[24:27], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_mul_i32 s12, s[sgprStrideD1J], 52                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[28:31], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[32:35], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[36:39], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_lshl_b32 s12, s[sgprStrideD1J], 2                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s12        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx4 v[40:43], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_1                            // jump to end
label_GW_B0_E1_N:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=16 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4); (1,0,0,0:vw4); (1,0,1,0:vw4); (1,0,2,0:vw4); (1,0,3,0:vw4); (2,0,0,0:vw4); (2,0,1,0:vw4); (2,0,2,0:vw4); (2,0,3,0:vw4); (3,0,0,0:vw4); (3,0,1,0:vw4); (3,0,2,0:vw4); (3,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v82, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v82, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v11, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v82, v11, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v82, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v21, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v21, v82, v21, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v82, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v23, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v23, v82, v23, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v82, v40, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v41, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v82, v41, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v42, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v82, v42, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v43, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v82, v43, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v60, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v60, v82, v60, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v61, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v82, v61, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v62, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v82, v62, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v63, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v63, v82, v63, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v80, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v80, v82, v80, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v81, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v82, v81, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+13], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+14], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+15], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+16], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+17], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+18], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+19], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+24], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+25], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+26], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+27], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+28], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+29], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+30], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+31], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+32], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+33], acc20          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+34], acc24          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+35], acc28          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+36], acc17          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+37], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+38], acc25          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+39], acc29          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+44], acc18          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+45], acc22          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+46], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+47], acc30          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+48], acc19          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+49], acc23          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+50], acc27          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+51], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+52], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+53], acc36          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+54], acc40          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+55], acc44          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+56], acc33          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+57], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+58], acc41          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+59], acc45          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+64], acc34          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+65], acc38          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+66], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+67], acc46          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+68], acc35          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+69], acc39          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+70], acc43          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+71], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+72], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+73], acc52          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+74], acc56          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+75], acc60          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+76], acc49          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+77], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+78], acc57          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+79], acc61          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+84], acc50          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+85], acc54          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+86], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+87], acc62          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+88], acc51          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+89], acc55          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+90], acc59          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+91], acc63          // copy acc to vreg[63]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0), (1, 0, 0, 0), (1, 0, 1, 0), (1, 0, 2, 0), (1, 0, 3, 0), (2, 0, 0, 0), (2, 0, 1, 0), (2, 0, 2, 0), (2, 0, 3, 0), (3, 0, 0, 0), (3, 0, 1, 0), (3, 0, 2, 0), (3, 0, 3, 0)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dwordx4 v[12:15], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[16:19], v11, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[24:27], v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[28:31], v21, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[32:35], v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[36:39], v23, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[44:47], v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[48:51], v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[52:55], v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[56:59], v43, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[64:67], v60, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[68:71], v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[72:75], v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[76:79], v63, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[84:87], v80, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[88:91], v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (4,0,0,0:vw4); (4,0,1,0:vw4); (4,0,2,0:vw4); (4,0,3,0:vw4); (5,0,0,0:vw4); (5,0,1,0:vw4); (5,0,2,0:vw4); (5,0,3,0:vw4); (6,0,0,0:vw4); (6,0,1,0:vw4); (6,0,2,0:vw4); (6,0,3,0:vw4); (7,0,0,0:vw4); (7,0,1,0:vw4); (7,0,2,0:vw4); (7,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v82, BufferOOB
/* (d1,vc1,d0,vc0)=(4,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v82, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v11, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v82, v11, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v82, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v21, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v21, v82, v21, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v82, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v23, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v23, v82, v23, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v82, v40, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v41, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v82, v41, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v42, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v82, v42, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v43, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v82, v43, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v60, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v60, v82, v60, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v61, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v82, v61, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v62, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v82, v62, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v63, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v63, v82, v63, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v80, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v80, v82, v80, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v81, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v82, v81, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc64          // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+13], acc68          // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+14], acc72          // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+15], acc76          // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+16], acc65          // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+17], acc69          // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+18], acc73          // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+19], acc77          // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+24], acc66          // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+25], acc70          // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+26], acc74          // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+27], acc78          // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+28], acc67          // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+29], acc71          // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+30], acc75          // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+31], acc79          // copy acc to vreg[79]
v_accvgpr_read_b32 v[vgprValuC+32], acc80          // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+33], acc84          // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+34], acc88          // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+35], acc92          // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+36], acc81          // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+37], acc85          // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+38], acc89          // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+39], acc93          // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+44], acc82          // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+45], acc86          // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+46], acc90          // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+47], acc94          // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+48], acc83          // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+49], acc87          // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+50], acc91          // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+51], acc95          // copy acc to vreg[95]
v_accvgpr_read_b32 v[vgprValuC+52], acc96          // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+53], acc100         // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+54], acc104         // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+55], acc108         // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+56], acc97          // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+57], acc101         // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+58], acc105         // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+59], acc109         // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+64], acc98          // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+65], acc102         // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+66], acc106         // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+67], acc110         // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+68], acc99          // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+69], acc103         // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+70], acc107         // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+71], acc111         // copy acc to vreg[111]
v_accvgpr_read_b32 v[vgprValuC+72], acc112         // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+73], acc116         // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+74], acc120         // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+75], acc124         // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+76], acc113         // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+77], acc117         // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+78], acc121         // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+79], acc125         // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+84], acc114         // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+85], acc118         // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+86], acc122         // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+87], acc126         // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+88], acc115         // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+89], acc119         // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+90], acc123         // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+91], acc127         // copy acc to vreg[127]

/* rC *= alpha batchElements=[(4, 0, 0, 0), (4, 0, 1, 0), (4, 0, 2, 0), (4, 0, 3, 0), (5, 0, 0, 0), (5, 0, 1, 0), (5, 0, 2, 0), (5, 0, 3, 0), (6, 0, 0, 0), (6, 0, 1, 0), (6, 0, 2, 0), (6, 0, 3, 0), (7, 0, 0, 0), (7, 0, 1, 0), (7, 0, 2, 0), (7, 0, 3, 0)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dwordx4 v[12:15], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[16:19], v11, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[24:27], v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[28:31], v21, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[32:35], v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[36:39], v23, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[44:47], v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[48:51], v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[52:55], v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[56:59], v43, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[64:67], v60, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[68:71], v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[72:75], v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[76:79], v63, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[84:87], v80, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[88:91], v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #2 (d1,d0,vc1,vc0) = */
/*    (8,0,0,0:vw4); (8,0,1,0:vw4); (8,0,2,0:vw4); (8,0,3,0:vw4); (9,0,0,0:vw4); (9,0,1,0:vw4); (9,0,2,0:vw4); (9,0,3,0:vw4); (10,0,0,0:vw4); (10,0,1,0:vw4); (10,0,2,0:vw4); (10,0,3,0:vw4); (11,0,0,0:vw4); (11,0,1,0:vw4); (11,0,2,0:vw4); (11,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v82, BufferOOB
/* (d1,vc1,d0,vc0)=(8,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v82, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v11, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v82, v11, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v82, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v21, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v21, v82, v21, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v82, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v23, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v23, v82, v23, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v82, v40, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v41, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v82, v41, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v42, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v42, v82, v42, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v43, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v43, v82, v43, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v60, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v60, v82, v60, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v61, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v82, v61, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v62, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v82, v62, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v63, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v63, v82, v63, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v80, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v80, v82, v80, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v81, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v82, v81, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc128         // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+13], acc132         // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+14], acc136         // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+15], acc140         // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+16], acc129         // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+17], acc133         // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+18], acc137         // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+19], acc141         // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+24], acc130         // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+25], acc134         // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+26], acc138         // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+27], acc142         // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+28], acc131         // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+29], acc135         // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+30], acc139         // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+31], acc143         // copy acc to vreg[143]
v_accvgpr_read_b32 v[vgprValuC+32], acc144         // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+33], acc148         // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+34], acc152         // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+35], acc156         // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+36], acc145         // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+37], acc149         // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+38], acc153         // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+39], acc157         // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+44], acc146         // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+45], acc150         // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+46], acc154         // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+47], acc158         // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+48], acc147         // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+49], acc151         // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+50], acc155         // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+51], acc159         // copy acc to vreg[159]
v_accvgpr_read_b32 v[vgprValuC+52], acc160         // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+53], acc164         // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+54], acc168         // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+55], acc172         // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+56], acc161         // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+57], acc165         // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+58], acc169         // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+59], acc173         // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+64], acc162         // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+65], acc166         // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+66], acc170         // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+67], acc174         // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+68], acc163         // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+69], acc167         // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+70], acc171         // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+71], acc175         // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+72], acc176         // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+73], acc180         // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+74], acc184         // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+75], acc188         // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+76], acc177         // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+77], acc181         // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+78], acc185         // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+79], acc189         // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+84], acc178         // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+85], acc182         // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+86], acc186         // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+87], acc190         // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+88], acc179         // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+89], acc183         // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+90], acc187         // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+91], acc191         // copy acc to vreg[191]

/* rC *= alpha batchElements=[(8, 0, 0, 0), (8, 0, 1, 0), (8, 0, 2, 0), (8, 0, 3, 0), (9, 0, 0, 0), (9, 0, 1, 0), (9, 0, 2, 0), (9, 0, 3, 0), (10, 0, 0, 0), (10, 0, 1, 0), (10, 0, 2, 0), (10, 0, 3, 0), (11, 0, 0, 0), (11, 0, 1, 0), (11, 0, 2, 0), (11, 0, 3, 0)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dwordx4 v[12:15], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[16:19], v11, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[24:27], v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[28:31], v21, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[32:35], v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[36:39], v23, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[44:47], v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[48:51], v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[52:55], v42, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[56:59], v43, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[64:67], v60, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[68:71], v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[72:75], v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[76:79], v63, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[84:87], v80, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[88:91], v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #3 (d1,d0,vc1,vc0) = */
/*    (12,0,0,0:vw4); (12,0,1,0:vw4); (12,0,2,0:vw4); (12,0,3,0:vw4); (13,0,0,0:vw4); (13,0,1,0:vw4); (13,0,2,0:vw4); (13,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(12,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v11, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v11, v42, v11, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v42, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v21, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v21, v42, v21, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v42, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v23, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v23, v42, v23, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v42, v40, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v41, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v42, v41, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+12], acc192         // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+13], acc196         // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+14], acc200         // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+15], acc204         // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+16], acc193         // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+17], acc197         // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+18], acc201         // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+19], acc205         // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+24], acc194         // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+25], acc198         // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+26], acc202         // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+27], acc206         // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+28], acc195         // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+29], acc199         // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+30], acc203         // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+31], acc207         // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+32], acc208         // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+33], acc212         // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+34], acc216         // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+35], acc220         // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+36], acc209         // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+37], acc213         // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+38], acc217         // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+39], acc221         // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+44], acc210         // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+45], acc214         // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+46], acc218         // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+47], acc222         // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+48], acc211         // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+49], acc215         // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+50], acc219         // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+51], acc223         // copy acc to vreg[223]

/* rC *= alpha batchElements=[(12, 0, 0, 0), (12, 0, 1, 0), (12, 0, 2, 0), (12, 0, 3, 0), (13, 0, 0, 0), (13, 0, 1, 0), (13, 0, 2, 0), (13, 0, 3, 0)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dwordx4 v[12:15], v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[16:19], v11, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[24:27], v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[28:31], v21, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[32:35], v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[36:39], v23, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[44:47], v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dwordx4 v[48:51], v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_1                            // jump to end
label_GW_B0_E1_M:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=16 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw1); (0,0,0,1:vw1); (0,0,0,2:vw1); (0,0,0,3:vw1); (0,0,1,0:vw1); (0,0,1,1:vw1); (0,0,1,2:vw1); (0,0,1,3:vw1); (0,0,2,0:vw1); (0,0,2,1:vw1); (0,0,2,2:vw1); (0,0,2,3:vw1); (0,0,3,0:vw1); (0,0,3,1:vw1); (0,0,3,2:vw1); (0,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v42, v12, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v42, v14, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v42, v16, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v18, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v42, v18, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v42, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v42, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v24, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v42, v24, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v28, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v42, v28, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v42, v30, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v32, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v32, v42, v32, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v42, v36, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v42, v38, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v42, v40, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+13], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+15], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+17], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+19], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+21], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+23], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+25], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+27], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+29], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+31], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+33], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+35], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+37], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+39], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+41], acc15          // copy acc to vreg[15]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 0, 2), (0, 0, 0, 3), (0, 0, 1, 0), (0, 0, 1, 1), (0, 0, 1, 2), (0, 0, 1, 3), (0, 0, 2, 0), (0, 0, 2, 1), (0, 0, 2, 2), (0, 0, 2, 3), (0, 0, 3, 0), (0, 0, 3, 1), (0, 0, 3, 2), (0, 0, 3, 3)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v23, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v25, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v27, v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v29, v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v31, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v33, v32, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v35, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v37, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v39, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v41, v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (1,0,0,0:vw1); (1,0,0,1:vw1); (1,0,0,2:vw1); (1,0,0,3:vw1); (1,0,1,0:vw1); (1,0,1,1:vw1); (1,0,1,2:vw1); (1,0,1,3:vw1); (1,0,2,0:vw1); (1,0,2,1:vw1); (1,0,2,2:vw1); (1,0,2,3:vw1); (1,0,3,0:vw1); (1,0,3,1:vw1); (1,0,3,2:vw1); (1,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v42, v12, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v42, v14, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v42, v16, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v18, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v42, v18, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v42, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v42, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v24, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v42, v24, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v28, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v42, v28, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v42, v30, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v32, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v32, v42, v32, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v42, v36, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v42, v38, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v42, v40, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+13], acc20          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+15], acc24          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+17], acc28          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+19], acc17          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+21], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+23], acc25          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+25], acc29          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+27], acc18          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+29], acc22          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+31], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+33], acc30          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+35], acc19          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+37], acc23          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+39], acc27          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+41], acc31          // copy acc to vreg[31]

/* rC *= alpha batchElements=[(1, 0, 0, 0), (1, 0, 0, 1), (1, 0, 0, 2), (1, 0, 0, 3), (1, 0, 1, 0), (1, 0, 1, 1), (1, 0, 1, 2), (1, 0, 1, 3), (1, 0, 2, 0), (1, 0, 2, 1), (1, 0, 2, 2), (1, 0, 2, 3), (1, 0, 3, 0), (1, 0, 3, 1), (1, 0, 3, 2), (1, 0, 3, 3)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v23, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v25, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v27, v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v29, v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v31, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v33, v32, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v35, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v37, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v39, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v41, v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #2 (d1,d0,vc1,vc0) = */
/*    (2,0,0,0:vw1); (2,0,0,1:vw1); (2,0,0,2:vw1); (2,0,0,3:vw1); (2,0,1,0:vw1); (2,0,1,1:vw1); (2,0,1,2:vw1); (2,0,1,3:vw1); (2,0,2,0:vw1); (2,0,2,1:vw1); (2,0,2,2:vw1); (2,0,2,3:vw1); (2,0,3,0:vw1); (2,0,3,1:vw1); (2,0,3,2:vw1); (2,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(2,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v42, v12, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v42, v14, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v42, v16, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v18, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v42, v18, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v42, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v42, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v24, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v42, v24, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v28, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v42, v28, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v42, v30, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v32, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v32, v42, v32, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v42, v36, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v42, v38, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v42, v40, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+13], acc36          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+15], acc40          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+17], acc44          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+19], acc33          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+21], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+23], acc41          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+25], acc45          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+27], acc34          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+29], acc38          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+31], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+33], acc46          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+35], acc35          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+37], acc39          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+39], acc43          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+41], acc47          // copy acc to vreg[47]

/* rC *= alpha batchElements=[(2, 0, 0, 0), (2, 0, 0, 1), (2, 0, 0, 2), (2, 0, 0, 3), (2, 0, 1, 0), (2, 0, 1, 1), (2, 0, 1, 2), (2, 0, 1, 3), (2, 0, 2, 0), (2, 0, 2, 1), (2, 0, 2, 2), (2, 0, 2, 3), (2, 0, 3, 0), (2, 0, 3, 1), (2, 0, 3, 2), (2, 0, 3, 3)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v23, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v25, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v27, v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v29, v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v31, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v33, v32, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v35, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v37, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v39, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v41, v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #3 (d1,d0,vc1,vc0) = */
/*    (3,0,0,0:vw1); (3,0,0,1:vw1); (3,0,0,2:vw1); (3,0,0,3:vw1); (3,0,1,0:vw1); (3,0,1,1:vw1); (3,0,1,2:vw1); (3,0,1,3:vw1); (3,0,2,0:vw1); (3,0,2,1:vw1); (3,0,2,2:vw1); (3,0,2,3:vw1); (3,0,3,0:vw1); (3,0,3,1:vw1); (3,0,3,2:vw1); (3,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(3,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v42, v12, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v42, v14, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v42, v16, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v18, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v42, v18, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v42, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v42, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v24, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v42, v24, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v28, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v42, v28, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v42, v30, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v32, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v32, v42, v32, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v42, v36, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v42, v38, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v42, v40, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+13], acc52          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+15], acc56          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+17], acc60          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+19], acc49          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+21], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+23], acc57          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+25], acc61          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+27], acc50          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+29], acc54          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+31], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+33], acc62          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+35], acc51          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+37], acc55          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+39], acc59          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+41], acc63          // copy acc to vreg[63]

/* rC *= alpha batchElements=[(3, 0, 0, 0), (3, 0, 0, 1), (3, 0, 0, 2), (3, 0, 0, 3), (3, 0, 1, 0), (3, 0, 1, 1), (3, 0, 1, 2), (3, 0, 1, 3), (3, 0, 2, 0), (3, 0, 2, 1), (3, 0, 2, 2), (3, 0, 2, 3), (3, 0, 3, 0), (3, 0, 3, 1), (3, 0, 3, 2), (3, 0, 3, 3)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v23, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v25, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v27, v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v29, v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v31, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v33, v32, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v35, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v37, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v39, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v41, v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #4 (d1,d0,vc1,vc0) = */
/*    (4,0,0,0:vw1); (4,0,0,1:vw1); (4,0,0,2:vw1); (4,0,0,3:vw1); (4,0,1,0:vw1); (4,0,1,1:vw1); (4,0,1,2:vw1); (4,0,1,3:vw1); (4,0,2,0:vw1); (4,0,2,1:vw1); (4,0,2,2:vw1); (4,0,2,3:vw1); (4,0,3,0:vw1); (4,0,3,1:vw1); (4,0,3,2:vw1); (4,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(4,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v42, v12, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v42, v14, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v42, v16, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v18, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v42, v18, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v42, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v42, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v24, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v42, v24, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v28, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v42, v28, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v42, v30, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v32, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v32, v42, v32, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v42, v36, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v42, v38, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v42, v40, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc64          // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+13], acc68          // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+15], acc72          // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+17], acc76          // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+19], acc65          // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+21], acc69          // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+23], acc73          // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+25], acc77          // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+27], acc66          // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+29], acc70          // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+31], acc74          // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+33], acc78          // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+35], acc67          // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+37], acc71          // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+39], acc75          // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+41], acc79          // copy acc to vreg[79]

/* rC *= alpha batchElements=[(4, 0, 0, 0), (4, 0, 0, 1), (4, 0, 0, 2), (4, 0, 0, 3), (4, 0, 1, 0), (4, 0, 1, 1), (4, 0, 1, 2), (4, 0, 1, 3), (4, 0, 2, 0), (4, 0, 2, 1), (4, 0, 2, 2), (4, 0, 2, 3), (4, 0, 3, 0), (4, 0, 3, 1), (4, 0, 3, 2), (4, 0, 3, 3)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v23, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v25, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v27, v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v29, v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v31, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v33, v32, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v35, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v37, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v39, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v41, v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #5 (d1,d0,vc1,vc0) = */
/*    (5,0,0,0:vw1); (5,0,0,1:vw1); (5,0,0,2:vw1); (5,0,0,3:vw1); (5,0,1,0:vw1); (5,0,1,1:vw1); (5,0,1,2:vw1); (5,0,1,3:vw1); (5,0,2,0:vw1); (5,0,2,1:vw1); (5,0,2,2:vw1); (5,0,2,3:vw1); (5,0,3,0:vw1); (5,0,3,1:vw1); (5,0,3,2:vw1); (5,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(5,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v42, v12, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v42, v14, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v42, v16, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v18, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v42, v18, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v42, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v42, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v24, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v42, v24, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v28, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v42, v28, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v42, v30, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v32, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v32, v42, v32, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v42, v36, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v42, v38, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v42, v40, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc80          // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+13], acc84          // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+15], acc88          // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+17], acc92          // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+19], acc81          // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+21], acc85          // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+23], acc89          // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+25], acc93          // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+27], acc82          // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+29], acc86          // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+31], acc90          // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+33], acc94          // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+35], acc83          // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+37], acc87          // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+39], acc91          // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+41], acc95          // copy acc to vreg[95]

/* rC *= alpha batchElements=[(5, 0, 0, 0), (5, 0, 0, 1), (5, 0, 0, 2), (5, 0, 0, 3), (5, 0, 1, 0), (5, 0, 1, 1), (5, 0, 1, 2), (5, 0, 1, 3), (5, 0, 2, 0), (5, 0, 2, 1), (5, 0, 2, 2), (5, 0, 2, 3), (5, 0, 3, 0), (5, 0, 3, 1), (5, 0, 3, 2), (5, 0, 3, 3)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v23, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v25, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v27, v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v29, v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v31, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v33, v32, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v35, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v37, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v39, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v41, v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #6 (d1,d0,vc1,vc0) = */
/*    (6,0,0,0:vw1); (6,0,0,1:vw1); (6,0,0,2:vw1); (6,0,0,3:vw1); (6,0,1,0:vw1); (6,0,1,1:vw1); (6,0,1,2:vw1); (6,0,1,3:vw1); (6,0,2,0:vw1); (6,0,2,1:vw1); (6,0,2,2:vw1); (6,0,2,3:vw1); (6,0,3,0:vw1); (6,0,3,1:vw1); (6,0,3,2:vw1); (6,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(6,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v42, v12, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v42, v14, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v42, v16, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v18, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v42, v18, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v42, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v42, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v24, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v42, v24, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v28, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v42, v28, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v42, v30, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v32, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v32, v42, v32, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v42, v36, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v42, v38, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v42, v40, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc96          // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+13], acc100         // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+15], acc104         // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+17], acc108         // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+19], acc97          // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+21], acc101         // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+23], acc105         // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+25], acc109         // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+27], acc98          // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+29], acc102         // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+31], acc106         // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+33], acc110         // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+35], acc99          // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+37], acc103         // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+39], acc107         // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+41], acc111         // copy acc to vreg[111]

/* rC *= alpha batchElements=[(6, 0, 0, 0), (6, 0, 0, 1), (6, 0, 0, 2), (6, 0, 0, 3), (6, 0, 1, 0), (6, 0, 1, 1), (6, 0, 1, 2), (6, 0, 1, 3), (6, 0, 2, 0), (6, 0, 2, 1), (6, 0, 2, 2), (6, 0, 2, 3), (6, 0, 3, 0), (6, 0, 3, 1), (6, 0, 3, 2), (6, 0, 3, 3)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v23, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v25, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v27, v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v29, v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v31, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v33, v32, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v35, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v37, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v39, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v41, v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #7 (d1,d0,vc1,vc0) = */
/*    (7,0,0,0:vw1); (7,0,0,1:vw1); (7,0,0,2:vw1); (7,0,0,3:vw1); (7,0,1,0:vw1); (7,0,1,1:vw1); (7,0,1,2:vw1); (7,0,1,3:vw1); (7,0,2,0:vw1); (7,0,2,1:vw1); (7,0,2,2:vw1); (7,0,2,3:vw1); (7,0,3,0:vw1); (7,0,3,1:vw1); (7,0,3,2:vw1); (7,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(7,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v42, v12, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v42, v14, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v42, v16, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v18, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v42, v18, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v42, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v42, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v24, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v42, v24, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v28, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v42, v28, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v42, v30, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v32, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v32, v42, v32, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v42, v36, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v42, v38, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v42, v40, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc112         // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+13], acc116         // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+15], acc120         // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+17], acc124         // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+19], acc113         // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+21], acc117         // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+23], acc121         // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+25], acc125         // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+27], acc114         // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+29], acc118         // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+31], acc122         // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+33], acc126         // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+35], acc115         // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+37], acc119         // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+39], acc123         // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+41], acc127         // copy acc to vreg[127]

/* rC *= alpha batchElements=[(7, 0, 0, 0), (7, 0, 0, 1), (7, 0, 0, 2), (7, 0, 0, 3), (7, 0, 1, 0), (7, 0, 1, 1), (7, 0, 1, 2), (7, 0, 1, 3), (7, 0, 2, 0), (7, 0, 2, 1), (7, 0, 2, 2), (7, 0, 2, 3), (7, 0, 3, 0), (7, 0, 3, 1), (7, 0, 3, 2), (7, 0, 3, 3)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v23, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v25, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v27, v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v29, v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v31, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v33, v32, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v35, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v37, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v39, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v41, v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #8 (d1,d0,vc1,vc0) = */
/*    (8,0,0,0:vw1); (8,0,0,1:vw1); (8,0,0,2:vw1); (8,0,0,3:vw1); (8,0,1,0:vw1); (8,0,1,1:vw1); (8,0,1,2:vw1); (8,0,1,3:vw1); (8,0,2,0:vw1); (8,0,2,1:vw1); (8,0,2,2:vw1); (8,0,2,3:vw1); (8,0,3,0:vw1); (8,0,3,1:vw1); (8,0,3,2:vw1); (8,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(8,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v42, v12, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v42, v14, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v42, v16, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v18, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v42, v18, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v42, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v42, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v24, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v42, v24, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v28, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v42, v28, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v42, v30, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v32, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v32, v42, v32, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v42, v36, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v42, v38, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v42, v40, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc128         // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+13], acc132         // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+15], acc136         // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+17], acc140         // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+19], acc129         // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+21], acc133         // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+23], acc137         // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+25], acc141         // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+27], acc130         // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+29], acc134         // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+31], acc138         // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+33], acc142         // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+35], acc131         // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+37], acc135         // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+39], acc139         // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+41], acc143         // copy acc to vreg[143]

/* rC *= alpha batchElements=[(8, 0, 0, 0), (8, 0, 0, 1), (8, 0, 0, 2), (8, 0, 0, 3), (8, 0, 1, 0), (8, 0, 1, 1), (8, 0, 1, 2), (8, 0, 1, 3), (8, 0, 2, 0), (8, 0, 2, 1), (8, 0, 2, 2), (8, 0, 2, 3), (8, 0, 3, 0), (8, 0, 3, 1), (8, 0, 3, 2), (8, 0, 3, 3)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v23, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v25, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v27, v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v29, v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v31, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v33, v32, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v35, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v37, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v39, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v41, v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #9 (d1,d0,vc1,vc0) = */
/*    (9,0,0,0:vw1); (9,0,0,1:vw1); (9,0,0,2:vw1); (9,0,0,3:vw1); (9,0,1,0:vw1); (9,0,1,1:vw1); (9,0,1,2:vw1); (9,0,1,3:vw1); (9,0,2,0:vw1); (9,0,2,1:vw1); (9,0,2,2:vw1); (9,0,2,3:vw1); (9,0,3,0:vw1); (9,0,3,1:vw1); (9,0,3,2:vw1); (9,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(9,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v42, v12, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v42, v14, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v42, v16, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v18, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v42, v18, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v42, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v42, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v24, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v42, v24, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v28, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v42, v28, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v42, v30, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v32, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v32, v42, v32, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v42, v36, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v42, v38, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v42, v40, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc144         // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+13], acc148         // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+15], acc152         // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+17], acc156         // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+19], acc145         // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+21], acc149         // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+23], acc153         // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+25], acc157         // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+27], acc146         // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+29], acc150         // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+31], acc154         // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+33], acc158         // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+35], acc147         // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+37], acc151         // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+39], acc155         // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+41], acc159         // copy acc to vreg[159]

/* rC *= alpha batchElements=[(9, 0, 0, 0), (9, 0, 0, 1), (9, 0, 0, 2), (9, 0, 0, 3), (9, 0, 1, 0), (9, 0, 1, 1), (9, 0, 1, 2), (9, 0, 1, 3), (9, 0, 2, 0), (9, 0, 2, 1), (9, 0, 2, 2), (9, 0, 2, 3), (9, 0, 3, 0), (9, 0, 3, 1), (9, 0, 3, 2), (9, 0, 3, 3)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v23, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v25, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v27, v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v29, v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v31, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v33, v32, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v35, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v37, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v39, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v41, v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #10 (d1,d0,vc1,vc0) = */
/*    (10,0,0,0:vw1); (10,0,0,1:vw1); (10,0,0,2:vw1); (10,0,0,3:vw1); (10,0,1,0:vw1); (10,0,1,1:vw1); (10,0,1,2:vw1); (10,0,1,3:vw1); (10,0,2,0:vw1); (10,0,2,1:vw1); (10,0,2,2:vw1); (10,0,2,3:vw1); (10,0,3,0:vw1); (10,0,3,1:vw1); (10,0,3,2:vw1); (10,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(10,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v42, v12, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v42, v14, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v42, v16, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v18, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v42, v18, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v42, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v42, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v24, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v42, v24, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v28, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v42, v28, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v42, v30, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v32, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v32, v42, v32, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v42, v36, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v42, v38, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v42, v40, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc160         // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+13], acc164         // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+15], acc168         // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+17], acc172         // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+19], acc161         // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+21], acc165         // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+23], acc169         // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+25], acc173         // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+27], acc162         // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+29], acc166         // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+31], acc170         // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+33], acc174         // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+35], acc163         // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+37], acc167         // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+39], acc171         // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+41], acc175         // copy acc to vreg[175]

/* rC *= alpha batchElements=[(10, 0, 0, 0), (10, 0, 0, 1), (10, 0, 0, 2), (10, 0, 0, 3), (10, 0, 1, 0), (10, 0, 1, 1), (10, 0, 1, 2), (10, 0, 1, 3), (10, 0, 2, 0), (10, 0, 2, 1), (10, 0, 2, 2), (10, 0, 2, 3), (10, 0, 3, 0), (10, 0, 3, 1), (10, 0, 3, 2), (10, 0, 3, 3)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v23, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v25, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v27, v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v29, v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v31, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v33, v32, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v35, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v37, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v39, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v41, v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #11 (d1,d0,vc1,vc0) = */
/*    (11,0,0,0:vw1); (11,0,0,1:vw1); (11,0,0,2:vw1); (11,0,0,3:vw1); (11,0,1,0:vw1); (11,0,1,1:vw1); (11,0,1,2:vw1); (11,0,1,3:vw1); (11,0,2,0:vw1); (11,0,2,1:vw1); (11,0,2,2:vw1); (11,0,2,3:vw1); (11,0,3,0:vw1); (11,0,3,1:vw1); (11,0,3,2:vw1); (11,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(11,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v42, v12, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v42, v14, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v42, v16, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v18, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v42, v18, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v42, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v42, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v24, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v42, v24, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v28, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v42, v28, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v42, v30, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v32, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v32, v42, v32, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v42, v36, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v42, v38, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v42, v40, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc176         // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+13], acc180         // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+15], acc184         // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+17], acc188         // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+19], acc177         // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+21], acc181         // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+23], acc185         // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+25], acc189         // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+27], acc178         // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+29], acc182         // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+31], acc186         // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+33], acc190         // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+35], acc179         // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+37], acc183         // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+39], acc187         // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+41], acc191         // copy acc to vreg[191]

/* rC *= alpha batchElements=[(11, 0, 0, 0), (11, 0, 0, 1), (11, 0, 0, 2), (11, 0, 0, 3), (11, 0, 1, 0), (11, 0, 1, 1), (11, 0, 1, 2), (11, 0, 1, 3), (11, 0, 2, 0), (11, 0, 2, 1), (11, 0, 2, 2), (11, 0, 2, 3), (11, 0, 3, 0), (11, 0, 3, 1), (11, 0, 3, 2), (11, 0, 3, 3)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v23, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v25, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v27, v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v29, v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v31, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v33, v32, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v35, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v37, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v39, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v41, v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #12 (d1,d0,vc1,vc0) = */
/*    (12,0,0,0:vw1); (12,0,0,1:vw1); (12,0,0,2:vw1); (12,0,0,3:vw1); (12,0,1,0:vw1); (12,0,1,1:vw1); (12,0,1,2:vw1); (12,0,1,3:vw1); (12,0,2,0:vw1); (12,0,2,1:vw1); (12,0,2,2:vw1); (12,0,2,3:vw1); (12,0,3,0:vw1); (12,0,3,1:vw1); (12,0,3,2:vw1); (12,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(12,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v42, v12, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v42, v14, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v42, v16, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v18, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v42, v18, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v42, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v42, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v24, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v42, v24, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v28, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v42, v28, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v42, v30, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v32, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v32, v42, v32, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v42, v36, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v42, v38, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v42, v40, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc192         // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+13], acc196         // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+15], acc200         // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+17], acc204         // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+19], acc193         // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+21], acc197         // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+23], acc201         // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+25], acc205         // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+27], acc194         // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+29], acc198         // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+31], acc202         // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+33], acc206         // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+35], acc195         // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+37], acc199         // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+39], acc203         // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+41], acc207         // copy acc to vreg[207]

/* rC *= alpha batchElements=[(12, 0, 0, 0), (12, 0, 0, 1), (12, 0, 0, 2), (12, 0, 0, 3), (12, 0, 1, 0), (12, 0, 1, 1), (12, 0, 1, 2), (12, 0, 1, 3), (12, 0, 2, 0), (12, 0, 2, 1), (12, 0, 2, 2), (12, 0, 2, 3), (12, 0, 3, 0), (12, 0, 3, 1), (12, 0, 3, 2), (12, 0, 3, 3)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v23, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v25, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v27, v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v29, v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v31, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v33, v32, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v35, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v37, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v39, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v41, v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #13 (d1,d0,vc1,vc0) = */
/*    (13,0,0,0:vw1); (13,0,0,1:vw1); (13,0,0,2:vw1); (13,0,0,3:vw1); (13,0,1,0:vw1); (13,0,1,1:vw1); (13,0,1,2:vw1); (13,0,1,3:vw1); (13,0,2,0:vw1); (13,0,2,1:vw1); (13,0,2,2:vw1); (13,0,2,3:vw1); (13,0,3,0:vw1); (13,0,3,1:vw1); (13,0,3,2:vw1); (13,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v42, BufferOOB
/* (d1,vc1,d0,vc0)=(13,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s58, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s58                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s58, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s58                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v10, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v10, v42, v10, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v12, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v12, v42, v12, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v14, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v14, v42, v14, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v16, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v16, v42, v16, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v18, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v42, v18, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v20, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v42, v20, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v22, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v22, v42, v22, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v24, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v24, v42, v24, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v26, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v26, v42, v26, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v28, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v42, v28, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v30, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v30, v42, v30, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v32, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v32, v42, v32, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[58:59], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v34, v3, v0, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v42, v34, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v36, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v36, v42, v36, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v38, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v42, v38, s[62:63]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[58:59], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[62:63], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[62:63], s[58:59], s[62:63]             // in0 && in1
v_add_lshl_u32 v40, v3, v4, 0x2                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v40, v42, v40, s[62:63]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+11], acc208         // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+13], acc212         // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+15], acc216         // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+17], acc220         // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+19], acc209         // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+21], acc213         // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+23], acc217         // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+25], acc221         // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+27], acc210         // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+29], acc214         // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+31], acc218         // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+33], acc222         // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+35], acc211         // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+37], acc215         // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+39], acc219         // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+41], acc223         // copy acc to vreg[223]

/* rC *= alpha batchElements=[(13, 0, 0, 0), (13, 0, 0, 1), (13, 0, 0, 2), (13, 0, 0, 3), (13, 0, 1, 0), (13, 0, 1, 1), (13, 0, 1, 2), (13, 0, 1, 3), (13, 0, 2, 0), (13, 0, 2, 1), (13, 0, 2, 2), (13, 0, 2, 3), (13, 0, 3, 0), (13, 0, 3, 1), (13, 0, 3, 2), (13, 0, 3, 3)] */

/* apply mask, calc new C and issue writes */
v_mov_b32 v7, 0xffff0000                           // mask for pack two bfloat16 element to 32bit
v_mov_b32 v8, 0x7fff0000                           // fp32 Nan
v_mov_b32 v9, 0x7fff                               // rounding bias for bfloat16
buffer_store_dword v11, v10, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v13, v12, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v15, v14, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v17, v16, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v19, v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v21, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v23, v22, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v25, v24, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v27, v26, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v29, v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v31, v30, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v33, v32, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v35, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v37, v36, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v39, v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
buffer_store_dword v41, v40, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_1                            // jump to end
label_GW_End_1:
s_getpc_b64 s[58:59]                               // addr of next instr
s_add_i32 s60, label_KernelEnd, 0x4                // target branch offset
s_add_u32 s58, s58, s60                            // add target branch offset
s_addc_u32 s59, s59, 0                             // add high and carry
s_setpc_b64 s[58:59]                               // branch to label_KernelEnd
label_GSU_5:
s_mov_b32 s[sgprSrdScaleAlphaVec+0], s[sgprAddressScaleAlphaVec+0] // init SRD base address (lower)
s_mov_b32 s[sgprSrdScaleAlphaVec+1], s[sgprAddressScaleAlphaVec+1] // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdScaleAlphaVec+3], Srd127_96     // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], 0 // s[AddressScaleAlphaVec] == 0 ?
s_cbranch_scc0 label_ScaleAlphaVec_1AddrValid      // branch if s[AddressScaleAlphaVec] != 0
s_mov_b32 s[sgprSrdScaleAlphaVec+2], 0
s_branch label_ScaleAlphaVec_1AddrValid_End
label_ScaleAlphaVec_1AddrValid:
s_mov_b32 s[sgprSrdScaleAlphaVec+2], s[sgprSizeI]
label_ScaleAlphaVec_1AddrValid_End:

s_mul_i32 s[sgprSrdScaleAlphaVec+2], 0x4, s[sgprSrdScaleAlphaVec+2] // ScaleAlphaVec scaled by BPE
s_add_u32 s8, s[sgprWorkGroup2], 0x1
s_mul_i32 s8, s[sgprBiasStride], s8                // stride * (wg+1)
s_cmp_eq_u32 s8, 0x0                               // bias stride = 0?
s_cselect_b32 s8, s[sgprSizeI], s8
s_mov_b32 s[sgprSrdBias+0], s[sgprAddressBias+0]   // init SRD base address (lower)
s_mov_b32 s[sgprSrdBias+1], s[sgprAddressBias+1]   // init SRD base address (upper) + other fields
s_mov_b32 s[sgprSrdBias+3], Srd127_96              // Set bits 127_96 in post-loop SRD
s_cmp_eq_u64 s[sgprAddressBias:sgprAddressBias+1], 0 // s[AddressBias] == 0 ?
s_cbranch_scc0 label_Bias_1AddrValid               // branch if s[AddressBias] != 0
s_mov_b32 s[sgprSrdBias+2], 0
s_branch label_Bias_1AddrValid_End
label_Bias_1AddrValid:
s_mov_b32 s[sgprSrdBias+2], s8
label_Bias_1AddrValid_End:

label_Load_Biasf32_0_1:
s_cmpk_lg_u32 s[sgprBiasType], 0                   // BiasType != 0
s_cbranch_scc1 label_Load_Biasbf16_0_1             // Branch if true

/******************************************/
/* Read Bias to LDS                       */
/******************************************/
s_mul_i32 s[sgprSrdBias+2], 0x4, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s8, 256, s[sgprWorkGroup0]               // wgp0 * MT0
v_add_u32 v8, s8, v[vgprSerial]                    // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s8, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v8, s8, v8                               // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v8, 0x2, v8                          // Global bias address scaled by BPE
buffer_load_dword v4, v8, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // load bias
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local bias address scaled by BPE
s_waitcnt vmcnt(0)                                 // wait for bias load
s_barrier                                          // Wait for all wavefronts
ds_write_b32 v8, v4 offset:0                       // store bias
s_branch label_Load_Bias_End_1                     // Branch to load bias end
label_Load_Biasbf16_0_1:
s_cmpk_lg_u32 s[sgprBiasType], 7                   // BiasType != 7
s_cbranch_scc1 label_Load_Bias_End_1               // Branch if true

/******************************************/
/* Read Bias to LDS                       */
/******************************************/
s_mul_i32 s[sgprSrdBias+2], 0x2, s[sgprSrdBias+2]  // scaled by BPE
s_mul_i32 s8, 256, s[sgprWorkGroup0]               // wgp0 * MT0
v_add_u32 v8, s8, v[vgprSerial]                    // coord 0 = wgp0 * MT0 + thread offset
s_mul_i32 s8, s[sgprBiasStride], s[sgprWorkGroup2] // Stride * WG
v_add_u32 v8, s8, v8                               // coord 0 = wgp0 * MT0 + thread offset + Stride * WG
v_lshlrev_b32 v8, 0x1, v8                          // Global bias address scaled by BPE
buffer_load_short_d16 v4, v8, s[sgprSrdBias:sgprSrdBias+3], 0 offen offset:0 // load bias
v_lshlrev_b32 v8, 0x2, v[vgprSerial]               // Local bias address scaled by BPE
s_waitcnt vmcnt(0)                                 // wait for bias load
s_barrier                                          // Wait for all wavefronts
v_lshlrev_b32 v4, 16, v4                           // cvt bf16 to fp32. 
ds_write_b32 v8, v4 offset:0                       // store bias
s_branch label_Load_Bias_End_1                     // Branch to load bias end
label_Load_Bias_End_1:
s_cmpk_eq_u32 s[sgprBeta], 0x0                     // Beta == 0
s_cbranch_scc0 label_GW_Beta_2                     // Branch if Beta is not zero

s_and_b32 s60, 255, s[sgprSizeI]                   // s60 = s[sgprSizeI] % 256
s_add_u32 s61, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s61                // wg0 >= nwg0-1 ?
s_cselect_b32 s60, s60, 0                          // set rMT0
s_cmpk_gt_u32 s60, 0x0                             // rMT0 > 0
s_cbranch_scc0 label_NoBranch_8EVAWN0JLND2BV8W_0   // Only branch on scc1
// jump if edges required
s_getpc_b64 s[60:61]                               // addr of next instr
s_add_i32 s62, label_GW_B0_E1_M_1, 0x4             // target branch offset
s_add_u32 s60, s60, s62                            // add target branch offset
s_addc_u32 s61, s61, 0                             // add high and carry
s_setpc_b64 s[60:61]                               // branch to label_GW_B0_E1_M_1
label_NoBranch_8EVAWN0JLND2BV8W_0:
s_mov_b32 s63, 0x0                                 // STATIC_DIV: divisior=224
s_mul_i32 s62, 0x249, s[sgprSizeJ]                 // tmp1 = dividend * magic hi
s_lshl_b64 s[62:63], s[62:63], 0x10                // left shift 16 bits
s_mul_i32 s61, s[sgprSizeJ], 0x2493                // tmp0 = dividend * magic lo
s_add_u32 s62, s61, s62                            // add lo
s_addc_u32 s63, s63, 0x0                           // add hi
s_lshr_b64 s[62:63], s[62:63], 0x21                // tmp1 = (dividend * magic) << shift
s_mov_b32 s61, s62                                 // quotient
s_mul_i32 s62, s61, 0xe0                           // quotient*divisor
s_sub_u32 s60, s[sgprSizeJ], s62                   // rReg = dividend - quotient*divisor
s_add_u32 s61, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s61                // wg1 >= nwg1-1
s_cselect_b32 s60, s60, 0                          // set rMT1
s_cmpk_gt_u32 s60, 0x0                             // rMT1 > 0
s_cbranch_scc0 label_NoBranch_N2EU1NHTIMLNBK0W_0   // Only branch on scc1
// jump if edges required
s_getpc_b64 s[60:61]                               // addr of next instr
s_add_i32 s62, label_GW_B0_E1_N_1, 0x4             // target branch offset
s_add_u32 s60, s60, s62                            // add target branch offset
s_addc_u32 s61, s61, 0                             // add high and carry
s_setpc_b64 s[60:61]                               // branch to label_GW_B0_E1_N_1
label_NoBranch_N2EU1NHTIMLNBK0W_0:
label_GW_B0_E0_2:
s_cmpk_eq_u32 s[sgprActivationType], 1             // activationType == 1
s_cbranch_scc1 label_To_Activation_Abs_VW4_1_beta_0_edge_0 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 2             // activationType == 2
s_cbranch_scc1 label_To_Activation_Clippedrelu_VW4_1_beta_0_edge_0 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 3             // activationType == 3
s_cbranch_scc1 label_To_Activation_Gelu_VW4_1_beta_0_edge_0 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 4             // activationType == 4
s_cbranch_scc1 label_To_Activation_Leakyrelu_VW4_1_beta_0_edge_0 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 5             // activationType == 5
s_cbranch_scc1 label_To_Activation_Relu_VW4_1_beta_0_edge_0 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 6             // activationType == 6
s_cbranch_scc1 label_To_Activation_Sigmoid_VW4_1_beta_0_edge_0 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 7             // activationType == 7
s_cbranch_scc1 label_To_Activation_Tanh_VW4_1_beta_0_edge_0 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 9             // activationType == 9
s_cbranch_scc1 label_To_Activation_Geluscaling_VW4_1_beta_0_edge_0 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 10            // activationType == 10
s_cbranch_scc1 label_To_Activation_Silu_VW4_1_beta_0_edge_0 // Branch if true
label_To_Activation_None_VW4_1_beta_0_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_None_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_6
label_To_Activation_Abs_VW4_1_beta_0_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Abs_VW4, 0x4        // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_6
label_To_Activation_Clippedrelu_VW4_1_beta_0_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Clippedrelu_VW4, 0x4 // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_6
label_To_Activation_Gelu_VW4_1_beta_0_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Gelu_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_6
label_To_Activation_Leakyrelu_VW4_1_beta_0_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Leakyrelu_VW4, 0x4  // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_6
label_To_Activation_Relu_VW4_1_beta_0_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Relu_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_6
label_To_Activation_Sigmoid_VW4_1_beta_0_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Sigmoid_VW4, 0x4    // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_6
label_To_Activation_Tanh_VW4_1_beta_0_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Tanh_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_6
label_To_Activation_Geluscaling_VW4_1_beta_0_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Geluscaling_VW4, 0x4 // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_6
label_To_Activation_Silu_VW4_1_beta_0_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Silu_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_6
label_ActivationSetPCAddrEnd_6:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=16 */
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 biasDim=0 */

/******************************************/
/* Global Write Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4); (1,0,0,0:vw4); (1,0,1,0:vw4); (1,0,2,0:vw4); (1,0,3,0:vw4); (2,0,0,0:vw4); (2,0,1,0:vw4); (2,0,2,0:vw4); (2,0,3,0:vw4); (3,0,0,0:vw4); (3,0,1,0:vw4); (3,0,2,0:vw4); (3,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s60
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[20:23], v15 offset:0                // load bias
v_lshlrev_b32 v16, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[24:27], v16, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
/* (d1,vc1,d0,vc0)=(2,0,0,0) */
/* (d1,vc1,d0,vc0)=(2,1,0,0) */
/* (d1,vc1,d0,vc0)=(2,2,0,0) */
/* (d1,vc1,d0,vc0)=(2,3,0,0) */
/* (d1,vc1,d0,vc0)=(3,0,0,0) */
/* (d1,vc1,d0,vc0)=(3,1,0,0) */
/* (d1,vc1,d0,vc0)=(3,2,0,0) */
/* (d1,vc1,d0,vc0)=(3,3,0,0) */
v_add_lshl_u32 v13, v3, v0, 0x1                    // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+28], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+29], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+30], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+31], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+36], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+37], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+38], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+39], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+40], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+41], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+42], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+43], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+44], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+45], acc20          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+46], acc24          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+47], acc28          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+48], acc17          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+49], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+50], acc25          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+51], acc29          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+52], acc18          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+53], acc22          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+54], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+55], acc30          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+56], acc19          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+57], acc23          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+58], acc27          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+59], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+60], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+61], acc36          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+62], acc40          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+63], acc44          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+64], acc33          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+65], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+66], acc41          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+67], acc45          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+68], acc34          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+69], acc38          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+70], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+71], acc46          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+72], acc35          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+73], acc39          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+74], acc43          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+75], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+76], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+77], acc52          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+78], acc56          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+79], acc60          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+80], acc49          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+81], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+82], acc57          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+83], acc61          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+84], acc50          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+85], acc54          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+86], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+87], acc62          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+88], acc51          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+89], acc55          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+90], acc59          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+91], acc63          // copy acc to vreg[63]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0), (1, 0, 0, 0), (1, 0, 1, 0), (1, 0, 2, 0), (1, 0, 3, 0), (2, 0, 0, 0), (2, 0, 1, 0), (2, 0, 2, 0), (2, 0, 3, 0), (3, 0, 0, 0), (3, 0, 1, 0), (3, 0, 2, 0), (3, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v32, v4
v_mov_b32 v33, v5
v_mov_b32 v34, v6
v_mov_b32 v35, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+32], v[vgprValuC+32] // check Nan
v_bfe_u32 v9, v[vgprValuC+32], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+32], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+32], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+32], 16, v[vgprValuC+32] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_and_or_b32 v32, v[vgprValuC+33], v10, v[vgprValuC+32] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+34], v[vgprValuC+34] // check Nan
v_bfe_u32 v9, v[vgprValuC+34], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+34], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+34], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+34], 16, v[vgprValuC+34] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+35], v[vgprValuC+35] // check Nan
v_bfe_u32 v9, v[vgprValuC+35], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+35], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+35], v9, v11, s[60:61]
v_and_or_b32 v33, v[vgprValuC+35], v10, v[vgprValuC+34] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[24:25], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[26:27], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_mov_b32 v41, v5
v_mov_b32 v42, v6
v_mov_b32 v43, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+40], 16, v[vgprValuC+40] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+41], v[vgprValuC+41] // check Nan
v_bfe_u32 v9, v[vgprValuC+41], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+41], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+41], v9, v11, s[60:61]
v_and_or_b32 v40, v[vgprValuC+41], v10, v[vgprValuC+40] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+42], v[vgprValuC+42] // check Nan
v_bfe_u32 v9, v[vgprValuC+42], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+42], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+42], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+42], 16, v[vgprValuC+42] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+43], v[vgprValuC+43] // check Nan
v_bfe_u32 v9, v[vgprValuC+43], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+43], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+43], v9, v11, s[60:61]
v_and_or_b32 v41, v[vgprValuC+43], v10, v[vgprValuC+42] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[24:25], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[26:27], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_mov_b32 v45, v5
v_mov_b32 v46, v6
v_mov_b32 v47, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+44], 16, v[vgprValuC+44] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_and_or_b32 v44, v[vgprValuC+45], v10, v[vgprValuC+44] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+46], v[vgprValuC+46] // check Nan
v_bfe_u32 v9, v[vgprValuC+46], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+46], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+46], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+46], 16, v[vgprValuC+46] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+47], v[vgprValuC+47] // check Nan
v_bfe_u32 v9, v[vgprValuC+47], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+47], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+47], v9, v11, s[60:61]
v_and_or_b32 v45, v[vgprValuC+47], v10, v[vgprValuC+46] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_mov_b32 v49, v5
v_mov_b32 v50, v6
v_mov_b32 v51, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+48], 16, v[vgprValuC+48] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+49], v[vgprValuC+49] // check Nan
v_bfe_u32 v9, v[vgprValuC+49], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+49], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+49], v9, v11, s[60:61]
v_and_or_b32 v48, v[vgprValuC+49], v10, v[vgprValuC+48] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+50], 16, v[vgprValuC+50] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+51], v[vgprValuC+51] // check Nan
v_bfe_u32 v9, v[vgprValuC+51], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+51], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+51], v9, v11, s[60:61]
v_and_or_b32 v49, v[vgprValuC+51], v10, v[vgprValuC+50] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[24:25], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[26:27], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_mov_b32 v53, v5
v_mov_b32 v54, v6
v_mov_b32 v55, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+52], 16, v[vgprValuC+52] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+53], v[vgprValuC+53] // check Nan
v_bfe_u32 v9, v[vgprValuC+53], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+53], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+53], v9, v11, s[60:61]
v_and_or_b32 v52, v[vgprValuC+53], v10, v[vgprValuC+52] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+54], v[vgprValuC+54] // check Nan
v_bfe_u32 v9, v[vgprValuC+54], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+54], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+54], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+54], 16, v[vgprValuC+54] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_and_or_b32 v53, v[vgprValuC+55], v10, v[vgprValuC+54] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[52:53], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[24:25], v[vgprValuC+56:vgprValuC+56+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[26:27], v[vgprValuC+58:vgprValuC+58+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+58:vgprValuC+58+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_mov_b32 v57, v5
v_mov_b32 v58, v6
v_mov_b32 v59, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+56], 16, v[vgprValuC+56] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+57], v[vgprValuC+57] // check Nan
v_bfe_u32 v9, v[vgprValuC+57], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+57], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+57], v9, v11, s[60:61]
v_and_or_b32 v56, v[vgprValuC+57], v10, v[vgprValuC+56] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+58], v[vgprValuC+58] // check Nan
v_bfe_u32 v9, v[vgprValuC+58], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+58], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+58], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+58], 16, v[vgprValuC+58] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+59], v[vgprValuC+59] // check Nan
v_bfe_u32 v9, v[vgprValuC+59], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+59], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+59], v9, v11, s[60:61]
v_and_or_b32 v57, v[vgprValuC+59], v10, v[vgprValuC+58] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[56:57], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[24:25], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[26:27], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_mov_b32 v61, v5
v_mov_b32 v62, v6
v_mov_b32 v63, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+60], 16, v[vgprValuC+60] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+61], v[vgprValuC+61] // check Nan
v_bfe_u32 v9, v[vgprValuC+61], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+61], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+61], v9, v11, s[60:61]
v_and_or_b32 v60, v[vgprValuC+61], v10, v[vgprValuC+60] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+62], v[vgprValuC+62] // check Nan
v_bfe_u32 v9, v[vgprValuC+62], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+62], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+62], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+62], 16, v[vgprValuC+62] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+63], v[vgprValuC+63] // check Nan
v_bfe_u32 v9, v[vgprValuC+63], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+63], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+63], v9, v11, s[60:61]
v_and_or_b32 v61, v[vgprValuC+63], v10, v[vgprValuC+62] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_mov_b32 v65, v5
v_mov_b32 v66, v6
v_mov_b32 v67, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+64], 16, v[vgprValuC+64] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_and_or_b32 v64, v[vgprValuC+65], v10, v[vgprValuC+64] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+66], v[vgprValuC+66] // check Nan
v_bfe_u32 v9, v[vgprValuC+66], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+66], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+66], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+66], 16, v[vgprValuC+66] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+67], v[vgprValuC+67] // check Nan
v_bfe_u32 v9, v[vgprValuC+67], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+67], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+67], v9, v11, s[60:61]
v_and_or_b32 v65, v[vgprValuC+67], v10, v[vgprValuC+66] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[64:65], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[24:25], v[vgprValuC+68:vgprValuC+68+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[26:27], v[vgprValuC+70:vgprValuC+70+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+70:vgprValuC+70+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_mov_b32 v69, v5
v_mov_b32 v70, v6
v_mov_b32 v71, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+68], 16, v[vgprValuC+68] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+69], v[vgprValuC+69] // check Nan
v_bfe_u32 v9, v[vgprValuC+69], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+69], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+69], v9, v11, s[60:61]
v_and_or_b32 v68, v[vgprValuC+69], v10, v[vgprValuC+68] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+70], 16, v[vgprValuC+70] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+71], v[vgprValuC+71] // check Nan
v_bfe_u32 v9, v[vgprValuC+71], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+71], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+71], v9, v11, s[60:61]
v_and_or_b32 v69, v[vgprValuC+71], v10, v[vgprValuC+70] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+74:vgprValuC+74+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_mov_b32 v73, v5
v_mov_b32 v74, v6
v_mov_b32 v75, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+72], 16, v[vgprValuC+72] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+73], v[vgprValuC+73] // check Nan
v_bfe_u32 v9, v[vgprValuC+73], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+73], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+73], v9, v11, s[60:61]
v_and_or_b32 v72, v[vgprValuC+73], v10, v[vgprValuC+72] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+74], v[vgprValuC+74] // check Nan
v_bfe_u32 v9, v[vgprValuC+74], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+74], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+74], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+74], 16, v[vgprValuC+74] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_and_or_b32 v73, v[vgprValuC+75], v10, v[vgprValuC+74] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[72:73], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[24:25], v[vgprValuC+76:vgprValuC+76+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[26:27], v[vgprValuC+78:vgprValuC+78+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+78:vgprValuC+78+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_mov_b32 v77, v5
v_mov_b32 v78, v6
v_mov_b32 v79, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+76], 16, v[vgprValuC+76] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+77], v[vgprValuC+77] // check Nan
v_bfe_u32 v9, v[vgprValuC+77], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+77], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+77], v9, v11, s[60:61]
v_and_or_b32 v76, v[vgprValuC+77], v10, v[vgprValuC+76] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+78], v[vgprValuC+78] // check Nan
v_bfe_u32 v9, v[vgprValuC+78], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+78], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+78], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+78], 16, v[vgprValuC+78] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+79], v[vgprValuC+79] // check Nan
v_bfe_u32 v9, v[vgprValuC+79], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+79], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+79], v9, v11, s[60:61]
v_and_or_b32 v77, v[vgprValuC+79], v10, v[vgprValuC+78] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_mov_b32 v81, v5
v_mov_b32 v82, v6
v_mov_b32 v83, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+80], 16, v[vgprValuC+80] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+81], v[vgprValuC+81] // check Nan
v_bfe_u32 v9, v[vgprValuC+81], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+81], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+81], v9, v11, s[60:61]
v_and_or_b32 v80, v[vgprValuC+81], v10, v[vgprValuC+80] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+82], v[vgprValuC+82] // check Nan
v_bfe_u32 v9, v[vgprValuC+82], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+82], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+82], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+82], 16, v[vgprValuC+82] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+83], v[vgprValuC+83] // check Nan
v_bfe_u32 v9, v[vgprValuC+83], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+83], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+83], v9, v11, s[60:61]
v_and_or_b32 v81, v[vgprValuC+83], v10, v[vgprValuC+82] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[80:81], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[24:25], v[vgprValuC+84:vgprValuC+84+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[26:27], v[vgprValuC+86:vgprValuC+86+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+86:vgprValuC+86+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_mov_b32 v85, v5
v_mov_b32 v86, v6
v_mov_b32 v87, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+84], 16, v[vgprValuC+84] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_and_or_b32 v84, v[vgprValuC+85], v10, v[vgprValuC+84] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+86], v[vgprValuC+86] // check Nan
v_bfe_u32 v9, v[vgprValuC+86], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+86], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+86], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+86], 16, v[vgprValuC+86] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+87], v[vgprValuC+87] // check Nan
v_bfe_u32 v9, v[vgprValuC+87], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+87], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+87], v9, v11, s[60:61]
v_and_or_b32 v85, v[vgprValuC+87], v10, v[vgprValuC+86] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[84:85], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[24:25], v[vgprValuC+88:vgprValuC+88+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[26:27], v[vgprValuC+90:vgprValuC+90+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+90:vgprValuC+90+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v88, v4
v_mov_b32 v89, v5
v_mov_b32 v90, v6
v_mov_b32 v91, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+88], v[vgprValuC+88] // check Nan
v_bfe_u32 v9, v[vgprValuC+88], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+88], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+88], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+88], 16, v[vgprValuC+88] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+89], v[vgprValuC+89] // check Nan
v_bfe_u32 v9, v[vgprValuC+89], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+89], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+89], v9, v11, s[60:61]
v_and_or_b32 v88, v[vgprValuC+89], v10, v[vgprValuC+88] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+90], 16, v[vgprValuC+90] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+91], v[vgprValuC+91] // check Nan
v_bfe_u32 v9, v[vgprValuC+91], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+91], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+91], v9, v11, s[60:61]
v_and_or_b32 v89, v[vgprValuC+91], v10, v[vgprValuC+90] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[88:89], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 biasDim=0 */

/******************************************/
/* Global Write Batch #1 (d1,d0,vc1,vc0) = */
/*    (4,0,0,0:vw4); (4,0,1,0:vw4); (4,0,2,0:vw4); (4,0,3,0:vw4); (5,0,0,0:vw4); (5,0,1,0:vw4); (5,0,2,0:vw4); (5,0,3,0:vw4); (6,0,0,0:vw4); (6,0,1,0:vw4); (6,0,2,0:vw4); (6,0,3,0:vw4); (7,0,0,0:vw4); (7,0,1,0:vw4); (7,0,2,0:vw4); (7,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(4,0,0,0) */
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s60
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
ds_read_b128 v[20:23], v15 offset:0                // load bias
buffer_load_dwordx4 v[24:27], v16, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(4,1,0,0) */
/* (d1,vc1,d0,vc0)=(4,2,0,0) */
/* (d1,vc1,d0,vc0)=(4,3,0,0) */
/* (d1,vc1,d0,vc0)=(5,0,0,0) */
/* (d1,vc1,d0,vc0)=(5,1,0,0) */
/* (d1,vc1,d0,vc0)=(5,2,0,0) */
/* (d1,vc1,d0,vc0)=(5,3,0,0) */
/* (d1,vc1,d0,vc0)=(6,0,0,0) */
/* (d1,vc1,d0,vc0)=(6,1,0,0) */
/* (d1,vc1,d0,vc0)=(6,2,0,0) */
/* (d1,vc1,d0,vc0)=(6,3,0,0) */
/* (d1,vc1,d0,vc0)=(7,0,0,0) */
/* (d1,vc1,d0,vc0)=(7,1,0,0) */
/* (d1,vc1,d0,vc0)=(7,2,0,0) */
/* (d1,vc1,d0,vc0)=(7,3,0,0) */
v_accvgpr_read_b32 v[vgprValuC+28], acc64          // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+29], acc68          // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+30], acc72          // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+31], acc76          // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+32], acc65          // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+33], acc69          // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+34], acc73          // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+35], acc77          // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+36], acc66          // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+37], acc70          // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+38], acc74          // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+39], acc78          // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+40], acc67          // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+41], acc71          // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+42], acc75          // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+43], acc79          // copy acc to vreg[79]
v_accvgpr_read_b32 v[vgprValuC+44], acc80          // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+45], acc84          // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+46], acc88          // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+47], acc92          // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+48], acc81          // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+49], acc85          // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+50], acc89          // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+51], acc93          // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+52], acc82          // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+53], acc86          // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+54], acc90          // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+55], acc94          // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+56], acc83          // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+57], acc87          // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+58], acc91          // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+59], acc95          // copy acc to vreg[95]
v_accvgpr_read_b32 v[vgprValuC+60], acc96          // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+61], acc100         // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+62], acc104         // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+63], acc108         // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+64], acc97          // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+65], acc101         // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+66], acc105         // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+67], acc109         // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+68], acc98          // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+69], acc102         // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+70], acc106         // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+71], acc110         // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+72], acc99          // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+73], acc103         // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+74], acc107         // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+75], acc111         // copy acc to vreg[111]
v_accvgpr_read_b32 v[vgprValuC+76], acc112         // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+77], acc116         // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+78], acc120         // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+79], acc124         // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+80], acc113         // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+81], acc117         // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+82], acc121         // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+83], acc125         // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+84], acc114         // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+85], acc118         // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+86], acc122         // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+87], acc126         // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+88], acc115         // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+89], acc119         // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+90], acc123         // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+91], acc127         // copy acc to vreg[127]

/* rC *= alpha batchElements=[(4, 0, 0, 0), (4, 0, 1, 0), (4, 0, 2, 0), (4, 0, 3, 0), (5, 0, 0, 0), (5, 0, 1, 0), (5, 0, 2, 0), (5, 0, 3, 0), (6, 0, 0, 0), (6, 0, 1, 0), (6, 0, 2, 0), (6, 0, 3, 0), (7, 0, 0, 0), (7, 0, 1, 0), (7, 0, 2, 0), (7, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v32, v4
v_mov_b32 v33, v5
v_mov_b32 v34, v6
v_mov_b32 v35, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+32], v[vgprValuC+32] // check Nan
v_bfe_u32 v9, v[vgprValuC+32], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+32], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+32], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+32], 16, v[vgprValuC+32] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_and_or_b32 v32, v[vgprValuC+33], v10, v[vgprValuC+32] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+34], v[vgprValuC+34] // check Nan
v_bfe_u32 v9, v[vgprValuC+34], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+34], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+34], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+34], 16, v[vgprValuC+34] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+35], v[vgprValuC+35] // check Nan
v_bfe_u32 v9, v[vgprValuC+35], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+35], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+35], v9, v11, s[60:61]
v_and_or_b32 v33, v[vgprValuC+35], v10, v[vgprValuC+34] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[24:25], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[26:27], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_mov_b32 v41, v5
v_mov_b32 v42, v6
v_mov_b32 v43, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+40], 16, v[vgprValuC+40] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+41], v[vgprValuC+41] // check Nan
v_bfe_u32 v9, v[vgprValuC+41], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+41], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+41], v9, v11, s[60:61]
v_and_or_b32 v40, v[vgprValuC+41], v10, v[vgprValuC+40] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+42], v[vgprValuC+42] // check Nan
v_bfe_u32 v9, v[vgprValuC+42], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+42], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+42], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+42], 16, v[vgprValuC+42] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+43], v[vgprValuC+43] // check Nan
v_bfe_u32 v9, v[vgprValuC+43], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+43], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+43], v9, v11, s[60:61]
v_and_or_b32 v41, v[vgprValuC+43], v10, v[vgprValuC+42] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[24:25], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[26:27], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_mov_b32 v45, v5
v_mov_b32 v46, v6
v_mov_b32 v47, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+44], 16, v[vgprValuC+44] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_and_or_b32 v44, v[vgprValuC+45], v10, v[vgprValuC+44] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+46], v[vgprValuC+46] // check Nan
v_bfe_u32 v9, v[vgprValuC+46], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+46], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+46], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+46], 16, v[vgprValuC+46] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+47], v[vgprValuC+47] // check Nan
v_bfe_u32 v9, v[vgprValuC+47], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+47], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+47], v9, v11, s[60:61]
v_and_or_b32 v45, v[vgprValuC+47], v10, v[vgprValuC+46] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_mov_b32 v49, v5
v_mov_b32 v50, v6
v_mov_b32 v51, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+48], 16, v[vgprValuC+48] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+49], v[vgprValuC+49] // check Nan
v_bfe_u32 v9, v[vgprValuC+49], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+49], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+49], v9, v11, s[60:61]
v_and_or_b32 v48, v[vgprValuC+49], v10, v[vgprValuC+48] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+50], 16, v[vgprValuC+50] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+51], v[vgprValuC+51] // check Nan
v_bfe_u32 v9, v[vgprValuC+51], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+51], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+51], v9, v11, s[60:61]
v_and_or_b32 v49, v[vgprValuC+51], v10, v[vgprValuC+50] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[24:25], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[26:27], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_mov_b32 v53, v5
v_mov_b32 v54, v6
v_mov_b32 v55, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+52], 16, v[vgprValuC+52] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+53], v[vgprValuC+53] // check Nan
v_bfe_u32 v9, v[vgprValuC+53], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+53], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+53], v9, v11, s[60:61]
v_and_or_b32 v52, v[vgprValuC+53], v10, v[vgprValuC+52] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+54], v[vgprValuC+54] // check Nan
v_bfe_u32 v9, v[vgprValuC+54], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+54], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+54], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+54], 16, v[vgprValuC+54] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_and_or_b32 v53, v[vgprValuC+55], v10, v[vgprValuC+54] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[52:53], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[24:25], v[vgprValuC+56:vgprValuC+56+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[26:27], v[vgprValuC+58:vgprValuC+58+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+58:vgprValuC+58+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_mov_b32 v57, v5
v_mov_b32 v58, v6
v_mov_b32 v59, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+56], 16, v[vgprValuC+56] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+57], v[vgprValuC+57] // check Nan
v_bfe_u32 v9, v[vgprValuC+57], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+57], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+57], v9, v11, s[60:61]
v_and_or_b32 v56, v[vgprValuC+57], v10, v[vgprValuC+56] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+58], v[vgprValuC+58] // check Nan
v_bfe_u32 v9, v[vgprValuC+58], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+58], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+58], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+58], 16, v[vgprValuC+58] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+59], v[vgprValuC+59] // check Nan
v_bfe_u32 v9, v[vgprValuC+59], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+59], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+59], v9, v11, s[60:61]
v_and_or_b32 v57, v[vgprValuC+59], v10, v[vgprValuC+58] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[56:57], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[24:25], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[26:27], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_mov_b32 v61, v5
v_mov_b32 v62, v6
v_mov_b32 v63, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+60], 16, v[vgprValuC+60] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+61], v[vgprValuC+61] // check Nan
v_bfe_u32 v9, v[vgprValuC+61], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+61], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+61], v9, v11, s[60:61]
v_and_or_b32 v60, v[vgprValuC+61], v10, v[vgprValuC+60] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+62], v[vgprValuC+62] // check Nan
v_bfe_u32 v9, v[vgprValuC+62], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+62], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+62], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+62], 16, v[vgprValuC+62] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+63], v[vgprValuC+63] // check Nan
v_bfe_u32 v9, v[vgprValuC+63], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+63], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+63], v9, v11, s[60:61]
v_and_or_b32 v61, v[vgprValuC+63], v10, v[vgprValuC+62] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_mov_b32 v65, v5
v_mov_b32 v66, v6
v_mov_b32 v67, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+64], 16, v[vgprValuC+64] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_and_or_b32 v64, v[vgprValuC+65], v10, v[vgprValuC+64] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+66], v[vgprValuC+66] // check Nan
v_bfe_u32 v9, v[vgprValuC+66], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+66], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+66], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+66], 16, v[vgprValuC+66] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+67], v[vgprValuC+67] // check Nan
v_bfe_u32 v9, v[vgprValuC+67], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+67], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+67], v9, v11, s[60:61]
v_and_or_b32 v65, v[vgprValuC+67], v10, v[vgprValuC+66] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[64:65], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[24:25], v[vgprValuC+68:vgprValuC+68+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[26:27], v[vgprValuC+70:vgprValuC+70+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+70:vgprValuC+70+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_mov_b32 v69, v5
v_mov_b32 v70, v6
v_mov_b32 v71, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+68], 16, v[vgprValuC+68] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+69], v[vgprValuC+69] // check Nan
v_bfe_u32 v9, v[vgprValuC+69], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+69], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+69], v9, v11, s[60:61]
v_and_or_b32 v68, v[vgprValuC+69], v10, v[vgprValuC+68] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+70], 16, v[vgprValuC+70] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+71], v[vgprValuC+71] // check Nan
v_bfe_u32 v9, v[vgprValuC+71], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+71], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+71], v9, v11, s[60:61]
v_and_or_b32 v69, v[vgprValuC+71], v10, v[vgprValuC+70] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+74:vgprValuC+74+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_mov_b32 v73, v5
v_mov_b32 v74, v6
v_mov_b32 v75, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+72], 16, v[vgprValuC+72] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+73], v[vgprValuC+73] // check Nan
v_bfe_u32 v9, v[vgprValuC+73], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+73], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+73], v9, v11, s[60:61]
v_and_or_b32 v72, v[vgprValuC+73], v10, v[vgprValuC+72] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+74], v[vgprValuC+74] // check Nan
v_bfe_u32 v9, v[vgprValuC+74], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+74], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+74], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+74], 16, v[vgprValuC+74] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_and_or_b32 v73, v[vgprValuC+75], v10, v[vgprValuC+74] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[72:73], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[24:25], v[vgprValuC+76:vgprValuC+76+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[26:27], v[vgprValuC+78:vgprValuC+78+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+78:vgprValuC+78+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_mov_b32 v77, v5
v_mov_b32 v78, v6
v_mov_b32 v79, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+76], 16, v[vgprValuC+76] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+77], v[vgprValuC+77] // check Nan
v_bfe_u32 v9, v[vgprValuC+77], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+77], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+77], v9, v11, s[60:61]
v_and_or_b32 v76, v[vgprValuC+77], v10, v[vgprValuC+76] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+78], v[vgprValuC+78] // check Nan
v_bfe_u32 v9, v[vgprValuC+78], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+78], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+78], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+78], 16, v[vgprValuC+78] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+79], v[vgprValuC+79] // check Nan
v_bfe_u32 v9, v[vgprValuC+79], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+79], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+79], v9, v11, s[60:61]
v_and_or_b32 v77, v[vgprValuC+79], v10, v[vgprValuC+78] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_mov_b32 v81, v5
v_mov_b32 v82, v6
v_mov_b32 v83, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+80], 16, v[vgprValuC+80] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+81], v[vgprValuC+81] // check Nan
v_bfe_u32 v9, v[vgprValuC+81], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+81], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+81], v9, v11, s[60:61]
v_and_or_b32 v80, v[vgprValuC+81], v10, v[vgprValuC+80] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+82], v[vgprValuC+82] // check Nan
v_bfe_u32 v9, v[vgprValuC+82], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+82], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+82], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+82], 16, v[vgprValuC+82] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+83], v[vgprValuC+83] // check Nan
v_bfe_u32 v9, v[vgprValuC+83], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+83], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+83], v9, v11, s[60:61]
v_and_or_b32 v81, v[vgprValuC+83], v10, v[vgprValuC+82] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[80:81], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[24:25], v[vgprValuC+84:vgprValuC+84+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[26:27], v[vgprValuC+86:vgprValuC+86+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+86:vgprValuC+86+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_mov_b32 v85, v5
v_mov_b32 v86, v6
v_mov_b32 v87, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+84], 16, v[vgprValuC+84] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_and_or_b32 v84, v[vgprValuC+85], v10, v[vgprValuC+84] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+86], v[vgprValuC+86] // check Nan
v_bfe_u32 v9, v[vgprValuC+86], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+86], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+86], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+86], 16, v[vgprValuC+86] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+87], v[vgprValuC+87] // check Nan
v_bfe_u32 v9, v[vgprValuC+87], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+87], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+87], v9, v11, s[60:61]
v_and_or_b32 v85, v[vgprValuC+87], v10, v[vgprValuC+86] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[84:85], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[24:25], v[vgprValuC+88:vgprValuC+88+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[26:27], v[vgprValuC+90:vgprValuC+90+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+90:vgprValuC+90+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v88, v4
v_mov_b32 v89, v5
v_mov_b32 v90, v6
v_mov_b32 v91, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+88], v[vgprValuC+88] // check Nan
v_bfe_u32 v9, v[vgprValuC+88], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+88], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+88], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+88], 16, v[vgprValuC+88] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+89], v[vgprValuC+89] // check Nan
v_bfe_u32 v9, v[vgprValuC+89], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+89], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+89], v9, v11, s[60:61]
v_and_or_b32 v88, v[vgprValuC+89], v10, v[vgprValuC+88] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+90], 16, v[vgprValuC+90] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+91], v[vgprValuC+91] // check Nan
v_bfe_u32 v9, v[vgprValuC+91], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+91], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+91], v9, v11, s[60:61]
v_and_or_b32 v89, v[vgprValuC+91], v10, v[vgprValuC+90] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[88:89], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 biasDim=0 */

/******************************************/
/* Global Write Batch #2 (d1,d0,vc1,vc0) = */
/*    (8,0,0,0:vw4); (8,0,1,0:vw4); (8,0,2,0:vw4); (8,0,3,0:vw4); (9,0,0,0:vw4); (9,0,1,0:vw4); (9,0,2,0:vw4); (9,0,3,0:vw4); (10,0,0,0:vw4); (10,0,1,0:vw4); (10,0,2,0:vw4); (10,0,3,0:vw4); (11,0,0,0:vw4); (11,0,1,0:vw4); (11,0,2,0:vw4); (11,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(8,0,0,0) */
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s60
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
ds_read_b128 v[20:23], v15 offset:0                // load bias
buffer_load_dwordx4 v[24:27], v16, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(8,1,0,0) */
/* (d1,vc1,d0,vc0)=(8,2,0,0) */
/* (d1,vc1,d0,vc0)=(8,3,0,0) */
/* (d1,vc1,d0,vc0)=(9,0,0,0) */
/* (d1,vc1,d0,vc0)=(9,1,0,0) */
/* (d1,vc1,d0,vc0)=(9,2,0,0) */
/* (d1,vc1,d0,vc0)=(9,3,0,0) */
/* (d1,vc1,d0,vc0)=(10,0,0,0) */
/* (d1,vc1,d0,vc0)=(10,1,0,0) */
/* (d1,vc1,d0,vc0)=(10,2,0,0) */
/* (d1,vc1,d0,vc0)=(10,3,0,0) */
/* (d1,vc1,d0,vc0)=(11,0,0,0) */
/* (d1,vc1,d0,vc0)=(11,1,0,0) */
/* (d1,vc1,d0,vc0)=(11,2,0,0) */
/* (d1,vc1,d0,vc0)=(11,3,0,0) */
v_accvgpr_read_b32 v[vgprValuC+28], acc128         // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+29], acc132         // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+30], acc136         // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+31], acc140         // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+32], acc129         // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+33], acc133         // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+34], acc137         // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+35], acc141         // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+36], acc130         // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+37], acc134         // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+38], acc138         // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+39], acc142         // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+40], acc131         // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+41], acc135         // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+42], acc139         // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+43], acc143         // copy acc to vreg[143]
v_accvgpr_read_b32 v[vgprValuC+44], acc144         // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+45], acc148         // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+46], acc152         // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+47], acc156         // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+48], acc145         // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+49], acc149         // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+50], acc153         // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+51], acc157         // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+52], acc146         // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+53], acc150         // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+54], acc154         // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+55], acc158         // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+56], acc147         // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+57], acc151         // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+58], acc155         // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+59], acc159         // copy acc to vreg[159]
v_accvgpr_read_b32 v[vgprValuC+60], acc160         // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+61], acc164         // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+62], acc168         // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+63], acc172         // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+64], acc161         // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+65], acc165         // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+66], acc169         // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+67], acc173         // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+68], acc162         // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+69], acc166         // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+70], acc170         // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+71], acc174         // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+72], acc163         // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+73], acc167         // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+74], acc171         // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+75], acc175         // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+76], acc176         // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+77], acc180         // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+78], acc184         // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+79], acc188         // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+80], acc177         // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+81], acc181         // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+82], acc185         // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+83], acc189         // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+84], acc178         // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+85], acc182         // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+86], acc186         // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+87], acc190         // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+88], acc179         // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+89], acc183         // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+90], acc187         // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+91], acc191         // copy acc to vreg[191]

/* rC *= alpha batchElements=[(8, 0, 0, 0), (8, 0, 1, 0), (8, 0, 2, 0), (8, 0, 3, 0), (9, 0, 0, 0), (9, 0, 1, 0), (9, 0, 2, 0), (9, 0, 3, 0), (10, 0, 0, 0), (10, 0, 1, 0), (10, 0, 2, 0), (10, 0, 3, 0), (11, 0, 0, 0), (11, 0, 1, 0), (11, 0, 2, 0), (11, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v32, v4
v_mov_b32 v33, v5
v_mov_b32 v34, v6
v_mov_b32 v35, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+32], v[vgprValuC+32] // check Nan
v_bfe_u32 v9, v[vgprValuC+32], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+32], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+32], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+32], 16, v[vgprValuC+32] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_and_or_b32 v32, v[vgprValuC+33], v10, v[vgprValuC+32] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+34], v[vgprValuC+34] // check Nan
v_bfe_u32 v9, v[vgprValuC+34], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+34], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+34], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+34], 16, v[vgprValuC+34] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+35], v[vgprValuC+35] // check Nan
v_bfe_u32 v9, v[vgprValuC+35], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+35], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+35], v9, v11, s[60:61]
v_and_or_b32 v33, v[vgprValuC+35], v10, v[vgprValuC+34] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[24:25], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[26:27], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_mov_b32 v41, v5
v_mov_b32 v42, v6
v_mov_b32 v43, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+40], 16, v[vgprValuC+40] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+41], v[vgprValuC+41] // check Nan
v_bfe_u32 v9, v[vgprValuC+41], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+41], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+41], v9, v11, s[60:61]
v_and_or_b32 v40, v[vgprValuC+41], v10, v[vgprValuC+40] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+42], v[vgprValuC+42] // check Nan
v_bfe_u32 v9, v[vgprValuC+42], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+42], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+42], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+42], 16, v[vgprValuC+42] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+43], v[vgprValuC+43] // check Nan
v_bfe_u32 v9, v[vgprValuC+43], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+43], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+43], v9, v11, s[60:61]
v_and_or_b32 v41, v[vgprValuC+43], v10, v[vgprValuC+42] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[24:25], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[26:27], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_mov_b32 v45, v5
v_mov_b32 v46, v6
v_mov_b32 v47, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+44], 16, v[vgprValuC+44] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_and_or_b32 v44, v[vgprValuC+45], v10, v[vgprValuC+44] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+46], v[vgprValuC+46] // check Nan
v_bfe_u32 v9, v[vgprValuC+46], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+46], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+46], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+46], 16, v[vgprValuC+46] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+47], v[vgprValuC+47] // check Nan
v_bfe_u32 v9, v[vgprValuC+47], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+47], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+47], v9, v11, s[60:61]
v_and_or_b32 v45, v[vgprValuC+47], v10, v[vgprValuC+46] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_mov_b32 v49, v5
v_mov_b32 v50, v6
v_mov_b32 v51, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+48], 16, v[vgprValuC+48] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+49], v[vgprValuC+49] // check Nan
v_bfe_u32 v9, v[vgprValuC+49], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+49], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+49], v9, v11, s[60:61]
v_and_or_b32 v48, v[vgprValuC+49], v10, v[vgprValuC+48] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+50], 16, v[vgprValuC+50] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+51], v[vgprValuC+51] // check Nan
v_bfe_u32 v9, v[vgprValuC+51], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+51], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+51], v9, v11, s[60:61]
v_and_or_b32 v49, v[vgprValuC+51], v10, v[vgprValuC+50] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[24:25], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[26:27], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_mov_b32 v53, v5
v_mov_b32 v54, v6
v_mov_b32 v55, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+52], 16, v[vgprValuC+52] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+53], v[vgprValuC+53] // check Nan
v_bfe_u32 v9, v[vgprValuC+53], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+53], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+53], v9, v11, s[60:61]
v_and_or_b32 v52, v[vgprValuC+53], v10, v[vgprValuC+52] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+54], v[vgprValuC+54] // check Nan
v_bfe_u32 v9, v[vgprValuC+54], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+54], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+54], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+54], 16, v[vgprValuC+54] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_and_or_b32 v53, v[vgprValuC+55], v10, v[vgprValuC+54] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[52:53], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[24:25], v[vgprValuC+56:vgprValuC+56+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[26:27], v[vgprValuC+58:vgprValuC+58+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+58:vgprValuC+58+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_mov_b32 v57, v5
v_mov_b32 v58, v6
v_mov_b32 v59, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+56], 16, v[vgprValuC+56] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+57], v[vgprValuC+57] // check Nan
v_bfe_u32 v9, v[vgprValuC+57], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+57], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+57], v9, v11, s[60:61]
v_and_or_b32 v56, v[vgprValuC+57], v10, v[vgprValuC+56] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+58], v[vgprValuC+58] // check Nan
v_bfe_u32 v9, v[vgprValuC+58], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+58], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+58], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+58], 16, v[vgprValuC+58] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+59], v[vgprValuC+59] // check Nan
v_bfe_u32 v9, v[vgprValuC+59], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+59], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+59], v9, v11, s[60:61]
v_and_or_b32 v57, v[vgprValuC+59], v10, v[vgprValuC+58] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[56:57], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[24:25], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[26:27], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_mov_b32 v61, v5
v_mov_b32 v62, v6
v_mov_b32 v63, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+60], 16, v[vgprValuC+60] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+61], v[vgprValuC+61] // check Nan
v_bfe_u32 v9, v[vgprValuC+61], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+61], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+61], v9, v11, s[60:61]
v_and_or_b32 v60, v[vgprValuC+61], v10, v[vgprValuC+60] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+62], v[vgprValuC+62] // check Nan
v_bfe_u32 v9, v[vgprValuC+62], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+62], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+62], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+62], 16, v[vgprValuC+62] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+63], v[vgprValuC+63] // check Nan
v_bfe_u32 v9, v[vgprValuC+63], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+63], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+63], v9, v11, s[60:61]
v_and_or_b32 v61, v[vgprValuC+63], v10, v[vgprValuC+62] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_mov_b32 v65, v5
v_mov_b32 v66, v6
v_mov_b32 v67, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+64], 16, v[vgprValuC+64] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_and_or_b32 v64, v[vgprValuC+65], v10, v[vgprValuC+64] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+66], v[vgprValuC+66] // check Nan
v_bfe_u32 v9, v[vgprValuC+66], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+66], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+66], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+66], 16, v[vgprValuC+66] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+67], v[vgprValuC+67] // check Nan
v_bfe_u32 v9, v[vgprValuC+67], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+67], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+67], v9, v11, s[60:61]
v_and_or_b32 v65, v[vgprValuC+67], v10, v[vgprValuC+66] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[64:65], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[24:25], v[vgprValuC+68:vgprValuC+68+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[26:27], v[vgprValuC+70:vgprValuC+70+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+70:vgprValuC+70+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_mov_b32 v69, v5
v_mov_b32 v70, v6
v_mov_b32 v71, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+68], 16, v[vgprValuC+68] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+69], v[vgprValuC+69] // check Nan
v_bfe_u32 v9, v[vgprValuC+69], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+69], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+69], v9, v11, s[60:61]
v_and_or_b32 v68, v[vgprValuC+69], v10, v[vgprValuC+68] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+70], 16, v[vgprValuC+70] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+71], v[vgprValuC+71] // check Nan
v_bfe_u32 v9, v[vgprValuC+71], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+71], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+71], v9, v11, s[60:61]
v_and_or_b32 v69, v[vgprValuC+71], v10, v[vgprValuC+70] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[68:69], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+74:vgprValuC+74+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_mov_b32 v73, v5
v_mov_b32 v74, v6
v_mov_b32 v75, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+72], 16, v[vgprValuC+72] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+73], v[vgprValuC+73] // check Nan
v_bfe_u32 v9, v[vgprValuC+73], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+73], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+73], v9, v11, s[60:61]
v_and_or_b32 v72, v[vgprValuC+73], v10, v[vgprValuC+72] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+74], v[vgprValuC+74] // check Nan
v_bfe_u32 v9, v[vgprValuC+74], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+74], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+74], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+74], 16, v[vgprValuC+74] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_and_or_b32 v73, v[vgprValuC+75], v10, v[vgprValuC+74] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[72:73], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[24:25], v[vgprValuC+76:vgprValuC+76+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[26:27], v[vgprValuC+78:vgprValuC+78+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+78:vgprValuC+78+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_mov_b32 v77, v5
v_mov_b32 v78, v6
v_mov_b32 v79, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+76], 16, v[vgprValuC+76] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+77], v[vgprValuC+77] // check Nan
v_bfe_u32 v9, v[vgprValuC+77], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+77], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+77], v9, v11, s[60:61]
v_and_or_b32 v76, v[vgprValuC+77], v10, v[vgprValuC+76] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+78], v[vgprValuC+78] // check Nan
v_bfe_u32 v9, v[vgprValuC+78], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+78], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+78], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+78], 16, v[vgprValuC+78] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+79], v[vgprValuC+79] // check Nan
v_bfe_u32 v9, v[vgprValuC+79], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+79], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+79], v9, v11, s[60:61]
v_and_or_b32 v77, v[vgprValuC+79], v10, v[vgprValuC+78] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_mov_b32 v81, v5
v_mov_b32 v82, v6
v_mov_b32 v83, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+80], 16, v[vgprValuC+80] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+81], v[vgprValuC+81] // check Nan
v_bfe_u32 v9, v[vgprValuC+81], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+81], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+81], v9, v11, s[60:61]
v_and_or_b32 v80, v[vgprValuC+81], v10, v[vgprValuC+80] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+82], v[vgprValuC+82] // check Nan
v_bfe_u32 v9, v[vgprValuC+82], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+82], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+82], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+82], 16, v[vgprValuC+82] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+83], v[vgprValuC+83] // check Nan
v_bfe_u32 v9, v[vgprValuC+83], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+83], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+83], v9, v11, s[60:61]
v_and_or_b32 v81, v[vgprValuC+83], v10, v[vgprValuC+82] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[80:81], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[24:25], v[vgprValuC+84:vgprValuC+84+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[26:27], v[vgprValuC+86:vgprValuC+86+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+86:vgprValuC+86+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_mov_b32 v85, v5
v_mov_b32 v86, v6
v_mov_b32 v87, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+84], 16, v[vgprValuC+84] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_and_or_b32 v84, v[vgprValuC+85], v10, v[vgprValuC+84] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+86], v[vgprValuC+86] // check Nan
v_bfe_u32 v9, v[vgprValuC+86], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+86], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+86], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+86], 16, v[vgprValuC+86] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+87], v[vgprValuC+87] // check Nan
v_bfe_u32 v9, v[vgprValuC+87], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+87], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+87], v9, v11, s[60:61]
v_and_or_b32 v85, v[vgprValuC+87], v10, v[vgprValuC+86] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[84:85], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[24:25], v[vgprValuC+88:vgprValuC+88+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[26:27], v[vgprValuC+90:vgprValuC+90+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+90:vgprValuC+90+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v88, v4
v_mov_b32 v89, v5
v_mov_b32 v90, v6
v_mov_b32 v91, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+88], v[vgprValuC+88] // check Nan
v_bfe_u32 v9, v[vgprValuC+88], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+88], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+88], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+88], 16, v[vgprValuC+88] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+89], v[vgprValuC+89] // check Nan
v_bfe_u32 v9, v[vgprValuC+89], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+89], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+89], v9, v11, s[60:61]
v_and_or_b32 v88, v[vgprValuC+89], v10, v[vgprValuC+88] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+90], 16, v[vgprValuC+90] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+91], v[vgprValuC+91] // check Nan
v_bfe_u32 v9, v[vgprValuC+91], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+91], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+91], v9, v11, s[60:61]
v_and_or_b32 v89, v[vgprValuC+91], v10, v[vgprValuC+90] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[88:89], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 biasDim=0 */

/******************************************/
/* Global Write Batch #3 (d1,d0,vc1,vc0) = */
/*    (12,0,0,0:vw4); (12,0,1,0:vw4); (12,0,2,0:vw4); (12,0,3,0:vw4); (13,0,0,0:vw4); (13,0,1,0:vw4); (13,0,2,0:vw4); (13,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(12,0,0,0) */
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s60
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
ds_read_b128 v[20:23], v15 offset:0                // load bias
buffer_load_dwordx4 v[24:27], v16, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(12,1,0,0) */
/* (d1,vc1,d0,vc0)=(12,2,0,0) */
/* (d1,vc1,d0,vc0)=(12,3,0,0) */
/* (d1,vc1,d0,vc0)=(13,0,0,0) */
/* (d1,vc1,d0,vc0)=(13,1,0,0) */
/* (d1,vc1,d0,vc0)=(13,2,0,0) */
/* (d1,vc1,d0,vc0)=(13,3,0,0) */
v_accvgpr_read_b32 v[vgprValuC+28], acc192         // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+29], acc196         // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+30], acc200         // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+31], acc204         // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+32], acc193         // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+33], acc197         // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+34], acc201         // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+35], acc205         // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+36], acc194         // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+37], acc198         // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+38], acc202         // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+39], acc206         // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+40], acc195         // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+41], acc199         // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+42], acc203         // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+43], acc207         // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+44], acc208         // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+45], acc212         // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+46], acc216         // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+47], acc220         // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+48], acc209         // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+49], acc213         // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+50], acc217         // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+51], acc221         // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+52], acc210         // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+53], acc214         // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+54], acc218         // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+55], acc222         // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+56], acc211         // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+57], acc215         // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+58], acc219         // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+59], acc223         // copy acc to vreg[223]

/* rC *= alpha batchElements=[(12, 0, 0, 0), (12, 0, 1, 0), (12, 0, 2, 0), (12, 0, 3, 0), (13, 0, 0, 0), (13, 0, 1, 0), (13, 0, 2, 0), (13, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16

s_waitcnt 0                                        // vmcnt(0) = 1 - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[24:25], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[26:27], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v32, v4
v_mov_b32 v33, v5
v_mov_b32 v34, v6
v_mov_b32 v35, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+32], v[vgprValuC+32] // check Nan
v_bfe_u32 v9, v[vgprValuC+32], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+32], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+32], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+32], 16, v[vgprValuC+32] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_and_or_b32 v32, v[vgprValuC+33], v10, v[vgprValuC+32] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+34], v[vgprValuC+34] // check Nan
v_bfe_u32 v9, v[vgprValuC+34], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+34], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+34], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+34], 16, v[vgprValuC+34] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+35], v[vgprValuC+35] // check Nan
v_bfe_u32 v9, v[vgprValuC+35], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+35], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+35], v9, v11, s[60:61]
v_and_or_b32 v33, v[vgprValuC+35], v10, v[vgprValuC+34] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[32:33], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[24:25], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[26:27], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_mov_b32 v41, v5
v_mov_b32 v42, v6
v_mov_b32 v43, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+40], 16, v[vgprValuC+40] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+41], v[vgprValuC+41] // check Nan
v_bfe_u32 v9, v[vgprValuC+41], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+41], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+41], v9, v11, s[60:61]
v_and_or_b32 v40, v[vgprValuC+41], v10, v[vgprValuC+40] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+42], v[vgprValuC+42] // check Nan
v_bfe_u32 v9, v[vgprValuC+42], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+42], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+42], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+42], 16, v[vgprValuC+42] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+43], v[vgprValuC+43] // check Nan
v_bfe_u32 v9, v[vgprValuC+43], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+43], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+43], v9, v11, s[60:61]
v_and_or_b32 v41, v[vgprValuC+43], v10, v[vgprValuC+42] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[24:25], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[26:27], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_mov_b32 v45, v5
v_mov_b32 v46, v6
v_mov_b32 v47, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+44], 16, v[vgprValuC+44] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_and_or_b32 v44, v[vgprValuC+45], v10, v[vgprValuC+44] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+46], v[vgprValuC+46] // check Nan
v_bfe_u32 v9, v[vgprValuC+46], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+46], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+46], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+46], 16, v[vgprValuC+46] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+47], v[vgprValuC+47] // check Nan
v_bfe_u32 v9, v[vgprValuC+47], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+47], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+47], v9, v11, s[60:61]
v_and_or_b32 v45, v[vgprValuC+47], v10, v[vgprValuC+46] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[44:45], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_mov_b32 v49, v5
v_mov_b32 v50, v6
v_mov_b32 v51, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+48], 16, v[vgprValuC+48] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+49], v[vgprValuC+49] // check Nan
v_bfe_u32 v9, v[vgprValuC+49], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+49], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+49], v9, v11, s[60:61]
v_and_or_b32 v48, v[vgprValuC+49], v10, v[vgprValuC+48] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+50], 16, v[vgprValuC+50] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+51], v[vgprValuC+51] // check Nan
v_bfe_u32 v9, v[vgprValuC+51], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+51], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+51], v9, v11, s[60:61]
v_and_or_b32 v49, v[vgprValuC+51], v10, v[vgprValuC+50] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[24:25], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[26:27], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_mov_b32 v53, v5
v_mov_b32 v54, v6
v_mov_b32 v55, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+52], 16, v[vgprValuC+52] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+53], v[vgprValuC+53] // check Nan
v_bfe_u32 v9, v[vgprValuC+53], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+53], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+53], v9, v11, s[60:61]
v_and_or_b32 v52, v[vgprValuC+53], v10, v[vgprValuC+52] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+54], v[vgprValuC+54] // check Nan
v_bfe_u32 v9, v[vgprValuC+54], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+54], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+54], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+54], 16, v[vgprValuC+54] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_and_or_b32 v53, v[vgprValuC+55], v10, v[vgprValuC+54] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[52:53], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[24:25], v[vgprValuC+56:vgprValuC+56+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[26:27], v[vgprValuC+58:vgprValuC+58+1] // *= scaleAlphaVecVMulPK(24)(2)
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+58:vgprValuC+58+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_mov_b32 v57, v5
v_mov_b32 v58, v6
v_mov_b32 v59, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+56], 16, v[vgprValuC+56] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+57], v[vgprValuC+57] // check Nan
v_bfe_u32 v9, v[vgprValuC+57], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+57], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+57], v9, v11, s[60:61]
v_and_or_b32 v56, v[vgprValuC+57], v10, v[vgprValuC+56] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+58], v[vgprValuC+58] // check Nan
v_bfe_u32 v9, v[vgprValuC+58], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+58], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+58], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+58], 16, v[vgprValuC+58] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+59], v[vgprValuC+59] // check Nan
v_bfe_u32 v9, v[vgprValuC+59], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+59], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+59], v9, v11, s[60:61]
v_and_or_b32 v57, v[vgprValuC+59], v10, v[vgprValuC+58] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[56:57], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
// jump to end
s_getpc_b64 s[60:61]                               // addr of next instr
s_add_i32 s62, label_GW_End_2, 0x4                 // target branch offset
s_add_u32 s60, s60, s62                            // add target branch offset
s_addc_u32 s61, s61, 0                             // add high and carry
s_setpc_b64 s[60:61]                               // branch to label_GW_End_2
label_GW_B0_E1_N_1:
s_cmpk_eq_u32 s[sgprActivationType], 1             // activationType == 1
s_cbranch_scc1 label_To_Activation_Abs_VW4_1_beta_0_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 2             // activationType == 2
s_cbranch_scc1 label_To_Activation_Clippedrelu_VW4_1_beta_0_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 3             // activationType == 3
s_cbranch_scc1 label_To_Activation_Gelu_VW4_1_beta_0_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 4             // activationType == 4
s_cbranch_scc1 label_To_Activation_Leakyrelu_VW4_1_beta_0_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 5             // activationType == 5
s_cbranch_scc1 label_To_Activation_Relu_VW4_1_beta_0_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 6             // activationType == 6
s_cbranch_scc1 label_To_Activation_Sigmoid_VW4_1_beta_0_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 7             // activationType == 7
s_cbranch_scc1 label_To_Activation_Tanh_VW4_1_beta_0_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 9             // activationType == 9
s_cbranch_scc1 label_To_Activation_Geluscaling_VW4_1_beta_0_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 10            // activationType == 10
s_cbranch_scc1 label_To_Activation_Silu_VW4_1_beta_0_edge_1 // Branch if true
label_To_Activation_None_VW4_1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_None_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_5
label_To_Activation_Abs_VW4_1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Abs_VW4, 0x4        // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_5
label_To_Activation_Clippedrelu_VW4_1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Clippedrelu_VW4, 0x4 // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_5
label_To_Activation_Gelu_VW4_1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Gelu_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_5
label_To_Activation_Leakyrelu_VW4_1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Leakyrelu_VW4, 0x4  // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_5
label_To_Activation_Relu_VW4_1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Relu_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_5
label_To_Activation_Sigmoid_VW4_1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Sigmoid_VW4, 0x4    // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_5
label_To_Activation_Tanh_VW4_1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Tanh_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_5
label_To_Activation_Geluscaling_VW4_1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Geluscaling_VW4, 0x4 // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_5
label_To_Activation_Silu_VW4_1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Silu_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_5
label_ActivationSetPCAddrEnd_5:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=14 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4); (1,0,0,0:vw4); (1,0,1,0:vw4); (1,0,2,0:vw4); (1,0,3,0:vw4); (2,0,0,0:vw4); (2,0,1,0:vw4); (2,0,2,0:vw4); (2,0,3,0:vw4); (3,0,0,0:vw4); (3,0,1,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v115, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v115, v14, s[64:65]             // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[16:19], v14 offset:0                // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v115, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v29, v0, s60
v_lshlrev_b32 v29, 0x2, v29                        // Bias address scaled by BPE
v_cndmask_b32 v29, v115, v29, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v30, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v28, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v115, v28, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v36, v0, s60
v_lshlrev_b32 v36, 0x2, v36                        // Bias address scaled by BPE
v_cndmask_b32 v36, v115, v36, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v37, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v31, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v115, v31, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v39, v0, s60
v_lshlrev_b32 v39, 0x2, v39                        // Bias address scaled by BPE
v_cndmask_b32 v39, v115, v39, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v44, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v38, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v115, v38, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v0, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v115, v46, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v115, v45, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v0, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v115, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v115, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v64, v0, s60
v_lshlrev_b32 v64, 0x2, v64                        // Bias address scaled by BPE
v_cndmask_b32 v64, v115, v64, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v65, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v59, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v59, v115, v59, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v0, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v115, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v72, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v115, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v0, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v115, v74, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v115, v73, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v85, v0, s60
v_lshlrev_b32 v85, 0x2, v85                        // Bias address scaled by BPE
v_cndmask_b32 v85, v115, v85, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v86, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v84, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v115, v84, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v0, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v115, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v87, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v115, v87, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v95, v0, s60
v_lshlrev_b32 v95, 0x2, v95                        // Bias address scaled by BPE
v_cndmask_b32 v95, v115, v95, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v100, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v94, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v94, v115, v94, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v102, v0, s60
v_lshlrev_b32 v102, 0x2, v102                      // Bias address scaled by BPE
v_cndmask_b32 v102, v115, v102, s[64:65]           // LDBias clip if OOB. offset
v_lshlrev_b32 v103, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v101, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v101, v115, v101, s[64:65]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v113, v0, s60
v_lshlrev_b32 v113, 0x2, v113                      // Bias address scaled by BPE
v_cndmask_b32 v113, v115, v113, s[64:65]           // LDBias clip if OOB. offset
v_lshlrev_b32 v114, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v112, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v112, v115, v112, s[64:65]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+25], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+26], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+27], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+32], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+33], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+34], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+35], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+48], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+49], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+50], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+51], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+52], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+53], acc20          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+54], acc24          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+55], acc28          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+60], acc17          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+61], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+62], acc25          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+63], acc29          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+68], acc18          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+69], acc22          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+70], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+71], acc30          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+76], acc19          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+77], acc23          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+78], acc27          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+79], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+80], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+81], acc36          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+82], acc40          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+83], acc44          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+88], acc33          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+89], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+90], acc41          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+91], acc45          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+96], acc34          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+97], acc38          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+98], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+99], acc46          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+104], acc35         // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+105], acc39         // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+106], acc43         // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+107], acc47         // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+108], acc48         // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+109], acc52         // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+110], acc56         // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+111], acc60         // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+116], acc49         // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+117], acc53         // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+118], acc57         // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+119], acc61         // copy acc to vreg[55]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0), (1, 0, 0, 0), (1, 0, 1, 0), (1, 0, 2, 0), (1, 0, 3, 0), (2, 0, 0, 0), (2, 0, 1, 0), (2, 0, 2, 0), (2, 0, 3, 0), (3, 0, 0, 0), (3, 0, 1, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+97], s[sgprAlpha], v[vgprValuC+97] // *= alpha
v_mul_f32 v[vgprValuC+98], s[sgprAlpha], v[vgprValuC+98] // *= alpha
v_mul_f32 v[vgprValuC+99], s[sgprAlpha], v[vgprValuC+99] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_mov_b32 v25, v5
v_mov_b32 v26, v6
v_mov_b32 v27, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+24], 16, v[vgprValuC+24] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+25], v[vgprValuC+25] // check Nan
v_bfe_u32 v9, v[vgprValuC+25], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+25], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+25], v9, v11, s[60:61]
v_and_or_b32 v24, v[vgprValuC+25], v10, v[vgprValuC+24] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+26], 16, v[vgprValuC+26] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+27], v[vgprValuC+27] // check Nan
v_bfe_u32 v9, v[vgprValuC+27], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+27], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+27], v9, v11, s[60:61]
v_and_or_b32 v25, v[vgprValuC+27], v10, v[vgprValuC+26] // pack two bf16 to dword
buffer_store_dwordx2 v[24:25], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v32, v4
v_mov_b32 v33, v5
v_mov_b32 v34, v6
v_mov_b32 v35, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+32], v[vgprValuC+32] // check Nan
v_bfe_u32 v9, v[vgprValuC+32], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+32], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+32], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+32], 16, v[vgprValuC+32] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_and_or_b32 v32, v[vgprValuC+33], v10, v[vgprValuC+32] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+34], v[vgprValuC+34] // check Nan
v_bfe_u32 v9, v[vgprValuC+34], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+34], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+34], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+34], 16, v[vgprValuC+34] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+35], v[vgprValuC+35] // check Nan
v_bfe_u32 v9, v[vgprValuC+35], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+35], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+35], v9, v11, s[60:61]
v_and_or_b32 v33, v[vgprValuC+35], v10, v[vgprValuC+34] // pack two bf16 to dword
buffer_store_dwordx2 v[32:33], v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_mov_b32 v41, v5
v_mov_b32 v42, v6
v_mov_b32 v43, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+40], 16, v[vgprValuC+40] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+41], v[vgprValuC+41] // check Nan
v_bfe_u32 v9, v[vgprValuC+41], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+41], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+41], v9, v11, s[60:61]
v_and_or_b32 v40, v[vgprValuC+41], v10, v[vgprValuC+40] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+42], v[vgprValuC+42] // check Nan
v_bfe_u32 v9, v[vgprValuC+42], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+42], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+42], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+42], 16, v[vgprValuC+42] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+43], v[vgprValuC+43] // check Nan
v_bfe_u32 v9, v[vgprValuC+43], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+43], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+43], v9, v11, s[60:61]
v_and_or_b32 v41, v[vgprValuC+43], v10, v[vgprValuC+42] // pack two bf16 to dword
buffer_store_dwordx2 v[40:41], v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_mov_b32 v49, v5
v_mov_b32 v50, v6
v_mov_b32 v51, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+48], 16, v[vgprValuC+48] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+49], v[vgprValuC+49] // check Nan
v_bfe_u32 v9, v[vgprValuC+49], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+49], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+49], v9, v11, s[60:61]
v_and_or_b32 v48, v[vgprValuC+49], v10, v[vgprValuC+48] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+50], 16, v[vgprValuC+50] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+51], v[vgprValuC+51] // check Nan
v_bfe_u32 v9, v[vgprValuC+51], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+51], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+51], v9, v11, s[60:61]
v_and_or_b32 v49, v[vgprValuC+51], v10, v[vgprValuC+50] // pack two bf16 to dword
buffer_store_dwordx2 v[48:49], v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_mov_b32 v53, v5
v_mov_b32 v54, v6
v_mov_b32 v55, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+52], 16, v[vgprValuC+52] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+53], v[vgprValuC+53] // check Nan
v_bfe_u32 v9, v[vgprValuC+53], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+53], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+53], v9, v11, s[60:61]
v_and_or_b32 v52, v[vgprValuC+53], v10, v[vgprValuC+52] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+54], v[vgprValuC+54] // check Nan
v_bfe_u32 v9, v[vgprValuC+54], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+54], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+54], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+54], 16, v[vgprValuC+54] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_and_or_b32 v53, v[vgprValuC+55], v10, v[vgprValuC+54] // pack two bf16 to dword
buffer_store_dwordx2 v[52:53], v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_mov_b32 v61, v5
v_mov_b32 v62, v6
v_mov_b32 v63, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+60], 16, v[vgprValuC+60] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+61], v[vgprValuC+61] // check Nan
v_bfe_u32 v9, v[vgprValuC+61], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+61], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+61], v9, v11, s[60:61]
v_and_or_b32 v60, v[vgprValuC+61], v10, v[vgprValuC+60] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+62], v[vgprValuC+62] // check Nan
v_bfe_u32 v9, v[vgprValuC+62], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+62], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+62], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+62], 16, v[vgprValuC+62] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+63], v[vgprValuC+63] // check Nan
v_bfe_u32 v9, v[vgprValuC+63], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+63], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+63], v9, v11, s[60:61]
v_and_or_b32 v61, v[vgprValuC+63], v10, v[vgprValuC+62] // pack two bf16 to dword
buffer_store_dwordx2 v[60:61], v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[20:21], v[vgprValuC+68:vgprValuC+68+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[22:23], v[vgprValuC+70:vgprValuC+70+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+70:vgprValuC+70+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_mov_b32 v69, v5
v_mov_b32 v70, v6
v_mov_b32 v71, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+68], 16, v[vgprValuC+68] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+69], v[vgprValuC+69] // check Nan
v_bfe_u32 v9, v[vgprValuC+69], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+69], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+69], v9, v11, s[60:61]
v_and_or_b32 v68, v[vgprValuC+69], v10, v[vgprValuC+68] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+70], 16, v[vgprValuC+70] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+71], v[vgprValuC+71] // check Nan
v_bfe_u32 v9, v[vgprValuC+71], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+71], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+71], v9, v11, s[60:61]
v_and_or_b32 v69, v[vgprValuC+71], v10, v[vgprValuC+70] // pack two bf16 to dword
buffer_store_dwordx2 v[68:69], v59, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[20:21], v[vgprValuC+76:vgprValuC+76+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[22:23], v[vgprValuC+78:vgprValuC+78+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_mov_b32 v77, v5
v_mov_b32 v78, v6
v_mov_b32 v79, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+76], 16, v[vgprValuC+76] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+77], v[vgprValuC+77] // check Nan
v_bfe_u32 v9, v[vgprValuC+77], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+77], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+77], v9, v11, s[60:61]
v_and_or_b32 v76, v[vgprValuC+77], v10, v[vgprValuC+76] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+78], v[vgprValuC+78] // check Nan
v_bfe_u32 v9, v[vgprValuC+78], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+78], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+78], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+78], 16, v[vgprValuC+78] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+79], v[vgprValuC+79] // check Nan
v_bfe_u32 v9, v[vgprValuC+79], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+79], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+79], v9, v11, s[60:61]
v_and_or_b32 v77, v[vgprValuC+79], v10, v[vgprValuC+78] // pack two bf16 to dword
buffer_store_dwordx2 v[76:77], v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+82:vgprValuC+82+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_mov_b32 v81, v5
v_mov_b32 v82, v6
v_mov_b32 v83, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+80], 16, v[vgprValuC+80] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+81], v[vgprValuC+81] // check Nan
v_bfe_u32 v9, v[vgprValuC+81], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+81], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+81], v9, v11, s[60:61]
v_and_or_b32 v80, v[vgprValuC+81], v10, v[vgprValuC+80] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+82], v[vgprValuC+82] // check Nan
v_bfe_u32 v9, v[vgprValuC+82], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+82], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+82], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+82], 16, v[vgprValuC+82] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+83], v[vgprValuC+83] // check Nan
v_bfe_u32 v9, v[vgprValuC+83], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+83], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+83], v9, v11, s[60:61]
v_and_or_b32 v81, v[vgprValuC+83], v10, v[vgprValuC+82] // pack two bf16 to dword
buffer_store_dwordx2 v[80:81], v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[20:21], v[vgprValuC+88:vgprValuC+88+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[22:23], v[vgprValuC+90:vgprValuC+90+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+90:vgprValuC+90+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v88, v4
v_mov_b32 v89, v5
v_mov_b32 v90, v6
v_mov_b32 v91, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+88], v[vgprValuC+88] // check Nan
v_bfe_u32 v9, v[vgprValuC+88], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+88], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+88], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+88], 16, v[vgprValuC+88] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+89], v[vgprValuC+89] // check Nan
v_bfe_u32 v9, v[vgprValuC+89], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+89], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+89], v9, v11, s[60:61]
v_and_or_b32 v88, v[vgprValuC+89], v10, v[vgprValuC+88] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+90], 16, v[vgprValuC+90] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+91], v[vgprValuC+91] // check Nan
v_bfe_u32 v9, v[vgprValuC+91], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+91], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+91], v9, v11, s[60:61]
v_and_or_b32 v89, v[vgprValuC+91], v10, v[vgprValuC+90] // pack two bf16 to dword
buffer_store_dwordx2 v[88:89], v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+96:vgprValuC+96+1], v[20:21], v[vgprValuC+96:vgprValuC+96+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+98:vgprValuC+98+1], v[22:23], v[vgprValuC+98:vgprValuC+98+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+96:vgprValuC+96+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+98:vgprValuC+98+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v96, v4
v_mov_b32 v97, v5
v_mov_b32 v98, v6
v_mov_b32 v99, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+96], v[vgprValuC+96] // check Nan
v_bfe_u32 v9, v[vgprValuC+96], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+96], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+96], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+96], 16, v[vgprValuC+96] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+97], v[vgprValuC+97] // check Nan
v_bfe_u32 v9, v[vgprValuC+97], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+97], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+97], v9, v11, s[60:61]
v_and_or_b32 v96, v[vgprValuC+97], v10, v[vgprValuC+96] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+98], v[vgprValuC+98] // check Nan
v_bfe_u32 v9, v[vgprValuC+98], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+98], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+98], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+98], 16, v[vgprValuC+98] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+99], v[vgprValuC+99] // check Nan
v_bfe_u32 v9, v[vgprValuC+99], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+99], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+99], v9, v11, s[60:61]
v_and_or_b32 v97, v[vgprValuC+99], v10, v[vgprValuC+98] // pack two bf16 to dword
buffer_store_dwordx2 v[96:97], v87, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[20:21], v[vgprValuC+104:vgprValuC+104+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[22:23], v[vgprValuC+106:vgprValuC+106+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+106:vgprValuC+106+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v104, v4
v_mov_b32 v105, v5
v_mov_b32 v106, v6
v_mov_b32 v107, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+104], v[vgprValuC+104] // check Nan
v_bfe_u32 v9, v[vgprValuC+104], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+104], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+104], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+104], 16, v[vgprValuC+104] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+105], v[vgprValuC+105] // check Nan
v_bfe_u32 v9, v[vgprValuC+105], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+105], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+105], v9, v11, s[60:61]
v_and_or_b32 v104, v[vgprValuC+105], v10, v[vgprValuC+104] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+106], v[vgprValuC+106] // check Nan
v_bfe_u32 v9, v[vgprValuC+106], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+106], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+106], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+106], 16, v[vgprValuC+106] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+107], v[vgprValuC+107] // check Nan
v_bfe_u32 v9, v[vgprValuC+107], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+107], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+107], v9, v11, s[60:61]
v_and_or_b32 v105, v[vgprValuC+107], v10, v[vgprValuC+106] // pack two bf16 to dword
buffer_store_dwordx2 v[104:105], v94, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[20:21], v[vgprValuC+108:vgprValuC+108+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[22:23], v[vgprValuC+110:vgprValuC+110+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+110:vgprValuC+110+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v108, v4
v_mov_b32 v109, v5
v_mov_b32 v110, v6
v_mov_b32 v111, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+108], v[vgprValuC+108] // check Nan
v_bfe_u32 v9, v[vgprValuC+108], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+108], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+108], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+108], 16, v[vgprValuC+108] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+109], v[vgprValuC+109] // check Nan
v_bfe_u32 v9, v[vgprValuC+109], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+109], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+109], v9, v11, s[60:61]
v_and_or_b32 v108, v[vgprValuC+109], v10, v[vgprValuC+108] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+110], v[vgprValuC+110] // check Nan
v_bfe_u32 v9, v[vgprValuC+110], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+110], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+110], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+110], 16, v[vgprValuC+110] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+111], v[vgprValuC+111] // check Nan
v_bfe_u32 v9, v[vgprValuC+111], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+111], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+111], v9, v11, s[60:61]
v_and_or_b32 v109, v[vgprValuC+111], v10, v[vgprValuC+110] // pack two bf16 to dword
buffer_store_dwordx2 v[108:109], v101, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[20:21], v[vgprValuC+116:vgprValuC+116+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[22:23], v[vgprValuC+118:vgprValuC+118+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+118:vgprValuC+118+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v116, v4
v_mov_b32 v117, v5
v_mov_b32 v118, v6
v_mov_b32 v119, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+116], v[vgprValuC+116] // check Nan
v_bfe_u32 v9, v[vgprValuC+116], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+116], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+116], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+116], 16, v[vgprValuC+116] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+117], v[vgprValuC+117] // check Nan
v_bfe_u32 v9, v[vgprValuC+117], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+117], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+117], v9, v11, s[60:61]
v_and_or_b32 v116, v[vgprValuC+117], v10, v[vgprValuC+116] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+118], v[vgprValuC+118] // check Nan
v_bfe_u32 v9, v[vgprValuC+118], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+118], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+118], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+118], 16, v[vgprValuC+118] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+119], v[vgprValuC+119] // check Nan
v_bfe_u32 v9, v[vgprValuC+119], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+119], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+119], v9, v11, s[60:61]
v_and_or_b32 v117, v[vgprValuC+119], v10, v[vgprValuC+118] // pack two bf16 to dword
buffer_store_dwordx2 v[116:117], v112, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (3,0,2,0:vw4); (3,0,3,0:vw4); (4,0,0,0:vw4); (4,0,1,0:vw4); (4,0,2,0:vw4); (4,0,3,0:vw4); (5,0,0,0:vw4); (5,0,1,0:vw4); (5,0,2,0:vw4); (5,0,3,0:vw4); (6,0,0,0:vw4); (6,0,1,0:vw4); (6,0,2,0:vw4); (6,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v115, BufferOOB
/* (d1,vc1,d0,vc0)=(3,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v115, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b128 v[16:19], v14 offset:0                // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v115, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v29, v0, s60
v_lshlrev_b32 v29, 0x2, v29                        // Bias address scaled by BPE
v_cndmask_b32 v29, v115, v29, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v30, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v28, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v115, v28, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v36, v0, s60
v_lshlrev_b32 v36, 0x2, v36                        // Bias address scaled by BPE
v_cndmask_b32 v36, v115, v36, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v37, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v31, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v115, v31, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v39, v0, s60
v_lshlrev_b32 v39, 0x2, v39                        // Bias address scaled by BPE
v_cndmask_b32 v39, v115, v39, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v44, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v38, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v115, v38, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v0, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v115, v46, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v115, v45, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v0, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v115, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v115, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v64, v0, s60
v_lshlrev_b32 v64, 0x2, v64                        // Bias address scaled by BPE
v_cndmask_b32 v64, v115, v64, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v65, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v59, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v59, v115, v59, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v0, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v115, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v72, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v115, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v0, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v115, v74, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v115, v73, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v85, v0, s60
v_lshlrev_b32 v85, 0x2, v85                        // Bias address scaled by BPE
v_cndmask_b32 v85, v115, v85, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v86, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v84, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v115, v84, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v0, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v115, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v87, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v115, v87, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v95, v0, s60
v_lshlrev_b32 v95, 0x2, v95                        // Bias address scaled by BPE
v_cndmask_b32 v95, v115, v95, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v100, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v94, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v94, v115, v94, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v102, v0, s60
v_lshlrev_b32 v102, 0x2, v102                      // Bias address scaled by BPE
v_cndmask_b32 v102, v115, v102, s[64:65]           // LDBias clip if OOB. offset
v_lshlrev_b32 v103, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v101, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v101, v115, v101, s[64:65]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v113, v0, s60
v_lshlrev_b32 v113, 0x2, v113                      // Bias address scaled by BPE
v_cndmask_b32 v113, v115, v113, s[64:65]           // LDBias clip if OOB. offset
v_lshlrev_b32 v114, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v112, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v112, v115, v112, s[64:65]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc50          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+25], acc54          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+26], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+27], acc62          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+32], acc51          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+33], acc55          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+34], acc59          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+35], acc63          // copy acc to vreg[63]
v_accvgpr_read_b32 v[vgprValuC+40], acc64          // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+41], acc68          // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+42], acc72          // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+43], acc76          // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+48], acc65          // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+49], acc69          // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+50], acc73          // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+51], acc77          // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+52], acc66          // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+53], acc70          // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+54], acc74          // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+55], acc78          // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+60], acc67          // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+61], acc71          // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+62], acc75          // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+63], acc79          // copy acc to vreg[79]
v_accvgpr_read_b32 v[vgprValuC+68], acc80          // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+69], acc84          // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+70], acc88          // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+71], acc92          // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+76], acc81          // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+77], acc85          // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+78], acc89          // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+79], acc93          // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+80], acc82          // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+81], acc86          // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+82], acc90          // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+83], acc94          // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+88], acc83          // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+89], acc87          // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+90], acc91          // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+91], acc95          // copy acc to vreg[95]
v_accvgpr_read_b32 v[vgprValuC+96], acc96          // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+97], acc100         // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+98], acc104         // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+99], acc108         // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+104], acc97         // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+105], acc101        // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+106], acc105        // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+107], acc109        // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+108], acc98         // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+109], acc102        // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+110], acc106        // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+111], acc110        // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+116], acc99         // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+117], acc103        // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+118], acc107        // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+119], acc111        // copy acc to vreg[111]

/* rC *= alpha batchElements=[(3, 0, 2, 0), (3, 0, 3, 0), (4, 0, 0, 0), (4, 0, 1, 0), (4, 0, 2, 0), (4, 0, 3, 0), (5, 0, 0, 0), (5, 0, 1, 0), (5, 0, 2, 0), (5, 0, 3, 0), (6, 0, 0, 0), (6, 0, 1, 0), (6, 0, 2, 0), (6, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+97], s[sgprAlpha], v[vgprValuC+97] // *= alpha
v_mul_f32 v[vgprValuC+98], s[sgprAlpha], v[vgprValuC+98] // *= alpha
v_mul_f32 v[vgprValuC+99], s[sgprAlpha], v[vgprValuC+99] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_mov_b32 v25, v5
v_mov_b32 v26, v6
v_mov_b32 v27, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+24], 16, v[vgprValuC+24] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+25], v[vgprValuC+25] // check Nan
v_bfe_u32 v9, v[vgprValuC+25], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+25], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+25], v9, v11, s[60:61]
v_and_or_b32 v24, v[vgprValuC+25], v10, v[vgprValuC+24] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+26], 16, v[vgprValuC+26] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+27], v[vgprValuC+27] // check Nan
v_bfe_u32 v9, v[vgprValuC+27], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+27], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+27], v9, v11, s[60:61]
v_and_or_b32 v25, v[vgprValuC+27], v10, v[vgprValuC+26] // pack two bf16 to dword
buffer_store_dwordx2 v[24:25], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v32, v4
v_mov_b32 v33, v5
v_mov_b32 v34, v6
v_mov_b32 v35, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+32], v[vgprValuC+32] // check Nan
v_bfe_u32 v9, v[vgprValuC+32], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+32], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+32], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+32], 16, v[vgprValuC+32] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_and_or_b32 v32, v[vgprValuC+33], v10, v[vgprValuC+32] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+34], v[vgprValuC+34] // check Nan
v_bfe_u32 v9, v[vgprValuC+34], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+34], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+34], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+34], 16, v[vgprValuC+34] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+35], v[vgprValuC+35] // check Nan
v_bfe_u32 v9, v[vgprValuC+35], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+35], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+35], v9, v11, s[60:61]
v_and_or_b32 v33, v[vgprValuC+35], v10, v[vgprValuC+34] // pack two bf16 to dword
buffer_store_dwordx2 v[32:33], v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_mov_b32 v41, v5
v_mov_b32 v42, v6
v_mov_b32 v43, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+40], 16, v[vgprValuC+40] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+41], v[vgprValuC+41] // check Nan
v_bfe_u32 v9, v[vgprValuC+41], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+41], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+41], v9, v11, s[60:61]
v_and_or_b32 v40, v[vgprValuC+41], v10, v[vgprValuC+40] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+42], v[vgprValuC+42] // check Nan
v_bfe_u32 v9, v[vgprValuC+42], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+42], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+42], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+42], 16, v[vgprValuC+42] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+43], v[vgprValuC+43] // check Nan
v_bfe_u32 v9, v[vgprValuC+43], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+43], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+43], v9, v11, s[60:61]
v_and_or_b32 v41, v[vgprValuC+43], v10, v[vgprValuC+42] // pack two bf16 to dword
buffer_store_dwordx2 v[40:41], v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_mov_b32 v49, v5
v_mov_b32 v50, v6
v_mov_b32 v51, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+48], 16, v[vgprValuC+48] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+49], v[vgprValuC+49] // check Nan
v_bfe_u32 v9, v[vgprValuC+49], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+49], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+49], v9, v11, s[60:61]
v_and_or_b32 v48, v[vgprValuC+49], v10, v[vgprValuC+48] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+50], 16, v[vgprValuC+50] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+51], v[vgprValuC+51] // check Nan
v_bfe_u32 v9, v[vgprValuC+51], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+51], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+51], v9, v11, s[60:61]
v_and_or_b32 v49, v[vgprValuC+51], v10, v[vgprValuC+50] // pack two bf16 to dword
buffer_store_dwordx2 v[48:49], v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_mov_b32 v53, v5
v_mov_b32 v54, v6
v_mov_b32 v55, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+52], 16, v[vgprValuC+52] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+53], v[vgprValuC+53] // check Nan
v_bfe_u32 v9, v[vgprValuC+53], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+53], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+53], v9, v11, s[60:61]
v_and_or_b32 v52, v[vgprValuC+53], v10, v[vgprValuC+52] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+54], v[vgprValuC+54] // check Nan
v_bfe_u32 v9, v[vgprValuC+54], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+54], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+54], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+54], 16, v[vgprValuC+54] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_and_or_b32 v53, v[vgprValuC+55], v10, v[vgprValuC+54] // pack two bf16 to dword
buffer_store_dwordx2 v[52:53], v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_mov_b32 v61, v5
v_mov_b32 v62, v6
v_mov_b32 v63, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+60], 16, v[vgprValuC+60] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+61], v[vgprValuC+61] // check Nan
v_bfe_u32 v9, v[vgprValuC+61], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+61], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+61], v9, v11, s[60:61]
v_and_or_b32 v60, v[vgprValuC+61], v10, v[vgprValuC+60] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+62], v[vgprValuC+62] // check Nan
v_bfe_u32 v9, v[vgprValuC+62], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+62], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+62], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+62], 16, v[vgprValuC+62] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+63], v[vgprValuC+63] // check Nan
v_bfe_u32 v9, v[vgprValuC+63], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+63], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+63], v9, v11, s[60:61]
v_and_or_b32 v61, v[vgprValuC+63], v10, v[vgprValuC+62] // pack two bf16 to dword
buffer_store_dwordx2 v[60:61], v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[20:21], v[vgprValuC+68:vgprValuC+68+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[22:23], v[vgprValuC+70:vgprValuC+70+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+70:vgprValuC+70+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_mov_b32 v69, v5
v_mov_b32 v70, v6
v_mov_b32 v71, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+68], 16, v[vgprValuC+68] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+69], v[vgprValuC+69] // check Nan
v_bfe_u32 v9, v[vgprValuC+69], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+69], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+69], v9, v11, s[60:61]
v_and_or_b32 v68, v[vgprValuC+69], v10, v[vgprValuC+68] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+70], 16, v[vgprValuC+70] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+71], v[vgprValuC+71] // check Nan
v_bfe_u32 v9, v[vgprValuC+71], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+71], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+71], v9, v11, s[60:61]
v_and_or_b32 v69, v[vgprValuC+71], v10, v[vgprValuC+70] // pack two bf16 to dword
buffer_store_dwordx2 v[68:69], v59, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[20:21], v[vgprValuC+76:vgprValuC+76+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[22:23], v[vgprValuC+78:vgprValuC+78+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_mov_b32 v77, v5
v_mov_b32 v78, v6
v_mov_b32 v79, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+76], 16, v[vgprValuC+76] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+77], v[vgprValuC+77] // check Nan
v_bfe_u32 v9, v[vgprValuC+77], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+77], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+77], v9, v11, s[60:61]
v_and_or_b32 v76, v[vgprValuC+77], v10, v[vgprValuC+76] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+78], v[vgprValuC+78] // check Nan
v_bfe_u32 v9, v[vgprValuC+78], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+78], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+78], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+78], 16, v[vgprValuC+78] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+79], v[vgprValuC+79] // check Nan
v_bfe_u32 v9, v[vgprValuC+79], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+79], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+79], v9, v11, s[60:61]
v_and_or_b32 v77, v[vgprValuC+79], v10, v[vgprValuC+78] // pack two bf16 to dword
buffer_store_dwordx2 v[76:77], v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+82:vgprValuC+82+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_mov_b32 v81, v5
v_mov_b32 v82, v6
v_mov_b32 v83, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+80], 16, v[vgprValuC+80] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+81], v[vgprValuC+81] // check Nan
v_bfe_u32 v9, v[vgprValuC+81], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+81], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+81], v9, v11, s[60:61]
v_and_or_b32 v80, v[vgprValuC+81], v10, v[vgprValuC+80] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+82], v[vgprValuC+82] // check Nan
v_bfe_u32 v9, v[vgprValuC+82], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+82], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+82], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+82], 16, v[vgprValuC+82] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+83], v[vgprValuC+83] // check Nan
v_bfe_u32 v9, v[vgprValuC+83], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+83], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+83], v9, v11, s[60:61]
v_and_or_b32 v81, v[vgprValuC+83], v10, v[vgprValuC+82] // pack two bf16 to dword
buffer_store_dwordx2 v[80:81], v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[20:21], v[vgprValuC+88:vgprValuC+88+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[22:23], v[vgprValuC+90:vgprValuC+90+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+90:vgprValuC+90+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v88, v4
v_mov_b32 v89, v5
v_mov_b32 v90, v6
v_mov_b32 v91, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+88], v[vgprValuC+88] // check Nan
v_bfe_u32 v9, v[vgprValuC+88], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+88], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+88], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+88], 16, v[vgprValuC+88] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+89], v[vgprValuC+89] // check Nan
v_bfe_u32 v9, v[vgprValuC+89], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+89], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+89], v9, v11, s[60:61]
v_and_or_b32 v88, v[vgprValuC+89], v10, v[vgprValuC+88] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+90], 16, v[vgprValuC+90] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+91], v[vgprValuC+91] // check Nan
v_bfe_u32 v9, v[vgprValuC+91], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+91], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+91], v9, v11, s[60:61]
v_and_or_b32 v89, v[vgprValuC+91], v10, v[vgprValuC+90] // pack two bf16 to dword
buffer_store_dwordx2 v[88:89], v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+96:vgprValuC+96+1], v[20:21], v[vgprValuC+96:vgprValuC+96+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+98:vgprValuC+98+1], v[22:23], v[vgprValuC+98:vgprValuC+98+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+96:vgprValuC+96+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+98:vgprValuC+98+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v96, v4
v_mov_b32 v97, v5
v_mov_b32 v98, v6
v_mov_b32 v99, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+96], v[vgprValuC+96] // check Nan
v_bfe_u32 v9, v[vgprValuC+96], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+96], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+96], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+96], 16, v[vgprValuC+96] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+97], v[vgprValuC+97] // check Nan
v_bfe_u32 v9, v[vgprValuC+97], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+97], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+97], v9, v11, s[60:61]
v_and_or_b32 v96, v[vgprValuC+97], v10, v[vgprValuC+96] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+98], v[vgprValuC+98] // check Nan
v_bfe_u32 v9, v[vgprValuC+98], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+98], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+98], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+98], 16, v[vgprValuC+98] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+99], v[vgprValuC+99] // check Nan
v_bfe_u32 v9, v[vgprValuC+99], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+99], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+99], v9, v11, s[60:61]
v_and_or_b32 v97, v[vgprValuC+99], v10, v[vgprValuC+98] // pack two bf16 to dword
buffer_store_dwordx2 v[96:97], v87, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[20:21], v[vgprValuC+104:vgprValuC+104+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[22:23], v[vgprValuC+106:vgprValuC+106+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+106:vgprValuC+106+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v104, v4
v_mov_b32 v105, v5
v_mov_b32 v106, v6
v_mov_b32 v107, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+104], v[vgprValuC+104] // check Nan
v_bfe_u32 v9, v[vgprValuC+104], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+104], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+104], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+104], 16, v[vgprValuC+104] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+105], v[vgprValuC+105] // check Nan
v_bfe_u32 v9, v[vgprValuC+105], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+105], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+105], v9, v11, s[60:61]
v_and_or_b32 v104, v[vgprValuC+105], v10, v[vgprValuC+104] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+106], v[vgprValuC+106] // check Nan
v_bfe_u32 v9, v[vgprValuC+106], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+106], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+106], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+106], 16, v[vgprValuC+106] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+107], v[vgprValuC+107] // check Nan
v_bfe_u32 v9, v[vgprValuC+107], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+107], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+107], v9, v11, s[60:61]
v_and_or_b32 v105, v[vgprValuC+107], v10, v[vgprValuC+106] // pack two bf16 to dword
buffer_store_dwordx2 v[104:105], v94, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[20:21], v[vgprValuC+108:vgprValuC+108+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[22:23], v[vgprValuC+110:vgprValuC+110+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+110:vgprValuC+110+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v108, v4
v_mov_b32 v109, v5
v_mov_b32 v110, v6
v_mov_b32 v111, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+108], v[vgprValuC+108] // check Nan
v_bfe_u32 v9, v[vgprValuC+108], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+108], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+108], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+108], 16, v[vgprValuC+108] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+109], v[vgprValuC+109] // check Nan
v_bfe_u32 v9, v[vgprValuC+109], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+109], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+109], v9, v11, s[60:61]
v_and_or_b32 v108, v[vgprValuC+109], v10, v[vgprValuC+108] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+110], v[vgprValuC+110] // check Nan
v_bfe_u32 v9, v[vgprValuC+110], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+110], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+110], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+110], 16, v[vgprValuC+110] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+111], v[vgprValuC+111] // check Nan
v_bfe_u32 v9, v[vgprValuC+111], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+111], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+111], v9, v11, s[60:61]
v_and_or_b32 v109, v[vgprValuC+111], v10, v[vgprValuC+110] // pack two bf16 to dword
buffer_store_dwordx2 v[108:109], v101, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[20:21], v[vgprValuC+116:vgprValuC+116+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[22:23], v[vgprValuC+118:vgprValuC+118+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+118:vgprValuC+118+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v116, v4
v_mov_b32 v117, v5
v_mov_b32 v118, v6
v_mov_b32 v119, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+116], v[vgprValuC+116] // check Nan
v_bfe_u32 v9, v[vgprValuC+116], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+116], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+116], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+116], 16, v[vgprValuC+116] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+117], v[vgprValuC+117] // check Nan
v_bfe_u32 v9, v[vgprValuC+117], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+117], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+117], v9, v11, s[60:61]
v_and_or_b32 v116, v[vgprValuC+117], v10, v[vgprValuC+116] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+118], v[vgprValuC+118] // check Nan
v_bfe_u32 v9, v[vgprValuC+118], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+118], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+118], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+118], 16, v[vgprValuC+118] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+119], v[vgprValuC+119] // check Nan
v_bfe_u32 v9, v[vgprValuC+119], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+119], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+119], v9, v11, s[60:61]
v_and_or_b32 v117, v[vgprValuC+119], v10, v[vgprValuC+118] // pack two bf16 to dword
buffer_store_dwordx2 v[116:117], v112, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #2 (d1,d0,vc1,vc0) = */
/*    (7,0,0,0:vw4); (7,0,1,0:vw4); (7,0,2,0:vw4); (7,0,3,0:vw4); (8,0,0,0:vw4); (8,0,1,0:vw4); (8,0,2,0:vw4); (8,0,3,0:vw4); (9,0,0,0:vw4); (9,0,1,0:vw4); (9,0,2,0:vw4); (9,0,3,0:vw4); (10,0,0,0:vw4); (10,0,1,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v115, BufferOOB
/* (d1,vc1,d0,vc0)=(7,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v115, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b128 v[16:19], v14 offset:0                // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v115, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v29, v0, s60
v_lshlrev_b32 v29, 0x2, v29                        // Bias address scaled by BPE
v_cndmask_b32 v29, v115, v29, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v30, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v28, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v115, v28, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v36, v0, s60
v_lshlrev_b32 v36, 0x2, v36                        // Bias address scaled by BPE
v_cndmask_b32 v36, v115, v36, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v37, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v31, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v115, v31, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v39, v0, s60
v_lshlrev_b32 v39, 0x2, v39                        // Bias address scaled by BPE
v_cndmask_b32 v39, v115, v39, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v44, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v38, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v115, v38, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v0, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v115, v46, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v115, v45, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v0, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v115, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v115, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v64, v0, s60
v_lshlrev_b32 v64, 0x2, v64                        // Bias address scaled by BPE
v_cndmask_b32 v64, v115, v64, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v65, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v59, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v59, v115, v59, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v0, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v115, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v72, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v115, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v0, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v115, v74, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v115, v73, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v85, v0, s60
v_lshlrev_b32 v85, 0x2, v85                        // Bias address scaled by BPE
v_cndmask_b32 v85, v115, v85, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v86, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v84, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v115, v84, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v0, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v115, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v87, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v115, v87, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v95, v0, s60
v_lshlrev_b32 v95, 0x2, v95                        // Bias address scaled by BPE
v_cndmask_b32 v95, v115, v95, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v100, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v94, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v94, v115, v94, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v102, v0, s60
v_lshlrev_b32 v102, 0x2, v102                      // Bias address scaled by BPE
v_cndmask_b32 v102, v115, v102, s[64:65]           // LDBias clip if OOB. offset
v_lshlrev_b32 v103, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v101, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v101, v115, v101, s[64:65]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v113, v0, s60
v_lshlrev_b32 v113, 0x2, v113                      // Bias address scaled by BPE
v_cndmask_b32 v113, v115, v113, s[64:65]           // LDBias clip if OOB. offset
v_lshlrev_b32 v114, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v112, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v112, v115, v112, s[64:65]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc112         // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+25], acc116         // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+26], acc120         // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+27], acc124         // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+32], acc113         // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+33], acc117         // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+34], acc121         // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+35], acc125         // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+40], acc114         // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+41], acc118         // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+42], acc122         // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+43], acc126         // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+48], acc115         // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+49], acc119         // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+50], acc123         // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+51], acc127         // copy acc to vreg[127]
v_accvgpr_read_b32 v[vgprValuC+52], acc128         // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+53], acc132         // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+54], acc136         // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+55], acc140         // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+60], acc129         // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+61], acc133         // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+62], acc137         // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+63], acc141         // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+68], acc130         // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+69], acc134         // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+70], acc138         // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+71], acc142         // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+76], acc131         // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+77], acc135         // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+78], acc139         // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+79], acc143         // copy acc to vreg[143]
v_accvgpr_read_b32 v[vgprValuC+80], acc144         // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+81], acc148         // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+82], acc152         // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+83], acc156         // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+88], acc145         // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+89], acc149         // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+90], acc153         // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+91], acc157         // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+96], acc146         // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+97], acc150         // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+98], acc154         // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+99], acc158         // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+104], acc147        // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+105], acc151        // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+106], acc155        // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+107], acc159        // copy acc to vreg[159]
v_accvgpr_read_b32 v[vgprValuC+108], acc160        // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+109], acc164        // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+110], acc168        // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+111], acc172        // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+116], acc161        // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+117], acc165        // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+118], acc169        // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+119], acc173        // copy acc to vreg[167]

/* rC *= alpha batchElements=[(7, 0, 0, 0), (7, 0, 1, 0), (7, 0, 2, 0), (7, 0, 3, 0), (8, 0, 0, 0), (8, 0, 1, 0), (8, 0, 2, 0), (8, 0, 3, 0), (9, 0, 0, 0), (9, 0, 1, 0), (9, 0, 2, 0), (9, 0, 3, 0), (10, 0, 0, 0), (10, 0, 1, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+97], s[sgprAlpha], v[vgprValuC+97] // *= alpha
v_mul_f32 v[vgprValuC+98], s[sgprAlpha], v[vgprValuC+98] // *= alpha
v_mul_f32 v[vgprValuC+99], s[sgprAlpha], v[vgprValuC+99] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_mov_b32 v25, v5
v_mov_b32 v26, v6
v_mov_b32 v27, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+24], 16, v[vgprValuC+24] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+25], v[vgprValuC+25] // check Nan
v_bfe_u32 v9, v[vgprValuC+25], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+25], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+25], v9, v11, s[60:61]
v_and_or_b32 v24, v[vgprValuC+25], v10, v[vgprValuC+24] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+26], 16, v[vgprValuC+26] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+27], v[vgprValuC+27] // check Nan
v_bfe_u32 v9, v[vgprValuC+27], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+27], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+27], v9, v11, s[60:61]
v_and_or_b32 v25, v[vgprValuC+27], v10, v[vgprValuC+26] // pack two bf16 to dword
buffer_store_dwordx2 v[24:25], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v32, v4
v_mov_b32 v33, v5
v_mov_b32 v34, v6
v_mov_b32 v35, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+32], v[vgprValuC+32] // check Nan
v_bfe_u32 v9, v[vgprValuC+32], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+32], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+32], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+32], 16, v[vgprValuC+32] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_and_or_b32 v32, v[vgprValuC+33], v10, v[vgprValuC+32] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+34], v[vgprValuC+34] // check Nan
v_bfe_u32 v9, v[vgprValuC+34], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+34], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+34], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+34], 16, v[vgprValuC+34] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+35], v[vgprValuC+35] // check Nan
v_bfe_u32 v9, v[vgprValuC+35], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+35], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+35], v9, v11, s[60:61]
v_and_or_b32 v33, v[vgprValuC+35], v10, v[vgprValuC+34] // pack two bf16 to dword
buffer_store_dwordx2 v[32:33], v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_mov_b32 v41, v5
v_mov_b32 v42, v6
v_mov_b32 v43, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+40], 16, v[vgprValuC+40] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+41], v[vgprValuC+41] // check Nan
v_bfe_u32 v9, v[vgprValuC+41], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+41], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+41], v9, v11, s[60:61]
v_and_or_b32 v40, v[vgprValuC+41], v10, v[vgprValuC+40] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+42], v[vgprValuC+42] // check Nan
v_bfe_u32 v9, v[vgprValuC+42], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+42], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+42], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+42], 16, v[vgprValuC+42] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+43], v[vgprValuC+43] // check Nan
v_bfe_u32 v9, v[vgprValuC+43], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+43], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+43], v9, v11, s[60:61]
v_and_or_b32 v41, v[vgprValuC+43], v10, v[vgprValuC+42] // pack two bf16 to dword
buffer_store_dwordx2 v[40:41], v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_mov_b32 v49, v5
v_mov_b32 v50, v6
v_mov_b32 v51, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+48], 16, v[vgprValuC+48] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+49], v[vgprValuC+49] // check Nan
v_bfe_u32 v9, v[vgprValuC+49], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+49], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+49], v9, v11, s[60:61]
v_and_or_b32 v48, v[vgprValuC+49], v10, v[vgprValuC+48] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+50], 16, v[vgprValuC+50] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+51], v[vgprValuC+51] // check Nan
v_bfe_u32 v9, v[vgprValuC+51], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+51], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+51], v9, v11, s[60:61]
v_and_or_b32 v49, v[vgprValuC+51], v10, v[vgprValuC+50] // pack two bf16 to dword
buffer_store_dwordx2 v[48:49], v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_mov_b32 v53, v5
v_mov_b32 v54, v6
v_mov_b32 v55, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+52], 16, v[vgprValuC+52] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+53], v[vgprValuC+53] // check Nan
v_bfe_u32 v9, v[vgprValuC+53], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+53], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+53], v9, v11, s[60:61]
v_and_or_b32 v52, v[vgprValuC+53], v10, v[vgprValuC+52] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+54], v[vgprValuC+54] // check Nan
v_bfe_u32 v9, v[vgprValuC+54], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+54], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+54], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+54], 16, v[vgprValuC+54] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_and_or_b32 v53, v[vgprValuC+55], v10, v[vgprValuC+54] // pack two bf16 to dword
buffer_store_dwordx2 v[52:53], v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_mov_b32 v61, v5
v_mov_b32 v62, v6
v_mov_b32 v63, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+60], 16, v[vgprValuC+60] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+61], v[vgprValuC+61] // check Nan
v_bfe_u32 v9, v[vgprValuC+61], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+61], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+61], v9, v11, s[60:61]
v_and_or_b32 v60, v[vgprValuC+61], v10, v[vgprValuC+60] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+62], v[vgprValuC+62] // check Nan
v_bfe_u32 v9, v[vgprValuC+62], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+62], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+62], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+62], 16, v[vgprValuC+62] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+63], v[vgprValuC+63] // check Nan
v_bfe_u32 v9, v[vgprValuC+63], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+63], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+63], v9, v11, s[60:61]
v_and_or_b32 v61, v[vgprValuC+63], v10, v[vgprValuC+62] // pack two bf16 to dword
buffer_store_dwordx2 v[60:61], v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[20:21], v[vgprValuC+68:vgprValuC+68+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[22:23], v[vgprValuC+70:vgprValuC+70+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+70:vgprValuC+70+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_mov_b32 v69, v5
v_mov_b32 v70, v6
v_mov_b32 v71, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+68], 16, v[vgprValuC+68] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+69], v[vgprValuC+69] // check Nan
v_bfe_u32 v9, v[vgprValuC+69], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+69], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+69], v9, v11, s[60:61]
v_and_or_b32 v68, v[vgprValuC+69], v10, v[vgprValuC+68] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+70], 16, v[vgprValuC+70] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+71], v[vgprValuC+71] // check Nan
v_bfe_u32 v9, v[vgprValuC+71], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+71], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+71], v9, v11, s[60:61]
v_and_or_b32 v69, v[vgprValuC+71], v10, v[vgprValuC+70] // pack two bf16 to dword
buffer_store_dwordx2 v[68:69], v59, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[20:21], v[vgprValuC+76:vgprValuC+76+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[22:23], v[vgprValuC+78:vgprValuC+78+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_mov_b32 v77, v5
v_mov_b32 v78, v6
v_mov_b32 v79, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+76], 16, v[vgprValuC+76] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+77], v[vgprValuC+77] // check Nan
v_bfe_u32 v9, v[vgprValuC+77], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+77], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+77], v9, v11, s[60:61]
v_and_or_b32 v76, v[vgprValuC+77], v10, v[vgprValuC+76] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+78], v[vgprValuC+78] // check Nan
v_bfe_u32 v9, v[vgprValuC+78], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+78], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+78], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+78], 16, v[vgprValuC+78] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+79], v[vgprValuC+79] // check Nan
v_bfe_u32 v9, v[vgprValuC+79], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+79], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+79], v9, v11, s[60:61]
v_and_or_b32 v77, v[vgprValuC+79], v10, v[vgprValuC+78] // pack two bf16 to dword
buffer_store_dwordx2 v[76:77], v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+82:vgprValuC+82+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_mov_b32 v81, v5
v_mov_b32 v82, v6
v_mov_b32 v83, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+80], 16, v[vgprValuC+80] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+81], v[vgprValuC+81] // check Nan
v_bfe_u32 v9, v[vgprValuC+81], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+81], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+81], v9, v11, s[60:61]
v_and_or_b32 v80, v[vgprValuC+81], v10, v[vgprValuC+80] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+82], v[vgprValuC+82] // check Nan
v_bfe_u32 v9, v[vgprValuC+82], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+82], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+82], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+82], 16, v[vgprValuC+82] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+83], v[vgprValuC+83] // check Nan
v_bfe_u32 v9, v[vgprValuC+83], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+83], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+83], v9, v11, s[60:61]
v_and_or_b32 v81, v[vgprValuC+83], v10, v[vgprValuC+82] // pack two bf16 to dword
buffer_store_dwordx2 v[80:81], v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[20:21], v[vgprValuC+88:vgprValuC+88+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[22:23], v[vgprValuC+90:vgprValuC+90+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+90:vgprValuC+90+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v88, v4
v_mov_b32 v89, v5
v_mov_b32 v90, v6
v_mov_b32 v91, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+88], v[vgprValuC+88] // check Nan
v_bfe_u32 v9, v[vgprValuC+88], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+88], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+88], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+88], 16, v[vgprValuC+88] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+89], v[vgprValuC+89] // check Nan
v_bfe_u32 v9, v[vgprValuC+89], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+89], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+89], v9, v11, s[60:61]
v_and_or_b32 v88, v[vgprValuC+89], v10, v[vgprValuC+88] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+90], 16, v[vgprValuC+90] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+91], v[vgprValuC+91] // check Nan
v_bfe_u32 v9, v[vgprValuC+91], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+91], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+91], v9, v11, s[60:61]
v_and_or_b32 v89, v[vgprValuC+91], v10, v[vgprValuC+90] // pack two bf16 to dword
buffer_store_dwordx2 v[88:89], v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+96:vgprValuC+96+1], v[20:21], v[vgprValuC+96:vgprValuC+96+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+98:vgprValuC+98+1], v[22:23], v[vgprValuC+98:vgprValuC+98+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+96:vgprValuC+96+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+98:vgprValuC+98+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v96, v4
v_mov_b32 v97, v5
v_mov_b32 v98, v6
v_mov_b32 v99, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+96], v[vgprValuC+96] // check Nan
v_bfe_u32 v9, v[vgprValuC+96], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+96], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+96], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+96], 16, v[vgprValuC+96] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+97], v[vgprValuC+97] // check Nan
v_bfe_u32 v9, v[vgprValuC+97], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+97], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+97], v9, v11, s[60:61]
v_and_or_b32 v96, v[vgprValuC+97], v10, v[vgprValuC+96] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+98], v[vgprValuC+98] // check Nan
v_bfe_u32 v9, v[vgprValuC+98], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+98], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+98], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+98], 16, v[vgprValuC+98] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+99], v[vgprValuC+99] // check Nan
v_bfe_u32 v9, v[vgprValuC+99], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+99], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+99], v9, v11, s[60:61]
v_and_or_b32 v97, v[vgprValuC+99], v10, v[vgprValuC+98] // pack two bf16 to dword
buffer_store_dwordx2 v[96:97], v87, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[20:21], v[vgprValuC+104:vgprValuC+104+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[22:23], v[vgprValuC+106:vgprValuC+106+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+106:vgprValuC+106+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v104, v4
v_mov_b32 v105, v5
v_mov_b32 v106, v6
v_mov_b32 v107, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+104], v[vgprValuC+104] // check Nan
v_bfe_u32 v9, v[vgprValuC+104], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+104], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+104], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+104], 16, v[vgprValuC+104] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+105], v[vgprValuC+105] // check Nan
v_bfe_u32 v9, v[vgprValuC+105], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+105], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+105], v9, v11, s[60:61]
v_and_or_b32 v104, v[vgprValuC+105], v10, v[vgprValuC+104] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+106], v[vgprValuC+106] // check Nan
v_bfe_u32 v9, v[vgprValuC+106], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+106], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+106], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+106], 16, v[vgprValuC+106] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+107], v[vgprValuC+107] // check Nan
v_bfe_u32 v9, v[vgprValuC+107], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+107], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+107], v9, v11, s[60:61]
v_and_or_b32 v105, v[vgprValuC+107], v10, v[vgprValuC+106] // pack two bf16 to dword
buffer_store_dwordx2 v[104:105], v94, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[20:21], v[vgprValuC+108:vgprValuC+108+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[22:23], v[vgprValuC+110:vgprValuC+110+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+110:vgprValuC+110+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v108, v4
v_mov_b32 v109, v5
v_mov_b32 v110, v6
v_mov_b32 v111, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+108], v[vgprValuC+108] // check Nan
v_bfe_u32 v9, v[vgprValuC+108], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+108], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+108], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+108], 16, v[vgprValuC+108] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+109], v[vgprValuC+109] // check Nan
v_bfe_u32 v9, v[vgprValuC+109], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+109], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+109], v9, v11, s[60:61]
v_and_or_b32 v108, v[vgprValuC+109], v10, v[vgprValuC+108] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+110], v[vgprValuC+110] // check Nan
v_bfe_u32 v9, v[vgprValuC+110], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+110], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+110], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+110], 16, v[vgprValuC+110] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+111], v[vgprValuC+111] // check Nan
v_bfe_u32 v9, v[vgprValuC+111], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+111], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+111], v9, v11, s[60:61]
v_and_or_b32 v109, v[vgprValuC+111], v10, v[vgprValuC+110] // pack two bf16 to dword
buffer_store_dwordx2 v[108:109], v101, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[20:21], v[vgprValuC+116:vgprValuC+116+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[22:23], v[vgprValuC+118:vgprValuC+118+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+118:vgprValuC+118+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v116, v4
v_mov_b32 v117, v5
v_mov_b32 v118, v6
v_mov_b32 v119, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+116], v[vgprValuC+116] // check Nan
v_bfe_u32 v9, v[vgprValuC+116], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+116], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+116], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+116], 16, v[vgprValuC+116] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+117], v[vgprValuC+117] // check Nan
v_bfe_u32 v9, v[vgprValuC+117], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+117], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+117], v9, v11, s[60:61]
v_and_or_b32 v116, v[vgprValuC+117], v10, v[vgprValuC+116] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+118], v[vgprValuC+118] // check Nan
v_bfe_u32 v9, v[vgprValuC+118], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+118], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+118], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+118], 16, v[vgprValuC+118] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+119], v[vgprValuC+119] // check Nan
v_bfe_u32 v9, v[vgprValuC+119], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+119], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+119], v9, v11, s[60:61]
v_and_or_b32 v117, v[vgprValuC+119], v10, v[vgprValuC+118] // pack two bf16 to dword
buffer_store_dwordx2 v[116:117], v112, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #3 (d1,d0,vc1,vc0) = */
/*    (10,0,2,0:vw4); (10,0,3,0:vw4); (11,0,0,0:vw4); (11,0,1,0:vw4); (11,0,2,0:vw4); (11,0,3,0:vw4); (12,0,0,0:vw4); (12,0,1,0:vw4); (12,0,2,0:vw4); (12,0,3,0:vw4); (13,0,0,0:vw4); (13,0,1,0:vw4); (13,0,2,0:vw4); (13,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v115, BufferOOB
/* (d1,vc1,d0,vc0)=(10,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v115, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b128 v[16:19], v14 offset:0                // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[20:23], v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v115, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v29, v0, s60
v_lshlrev_b32 v29, 0x2, v29                        // Bias address scaled by BPE
v_cndmask_b32 v29, v115, v29, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v30, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v28, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v28, v115, v28, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v36, v0, s60
v_lshlrev_b32 v36, 0x2, v36                        // Bias address scaled by BPE
v_cndmask_b32 v36, v115, v36, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v37, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v31, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v115, v31, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v39, v0, s60
v_lshlrev_b32 v39, 0x2, v39                        // Bias address scaled by BPE
v_cndmask_b32 v39, v115, v39, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v44, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v38, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v38, v115, v38, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v0, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v115, v46, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v115, v45, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v0, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v115, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v115, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v64, v0, s60
v_lshlrev_b32 v64, 0x2, v64                        // Bias address scaled by BPE
v_cndmask_b32 v64, v115, v64, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v65, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v59, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v59, v115, v59, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v0, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v115, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v72, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v115, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v0, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v115, v74, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v115, v73, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v85, v0, s60
v_lshlrev_b32 v85, 0x2, v85                        // Bias address scaled by BPE
v_cndmask_b32 v85, v115, v85, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v86, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v84, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v115, v84, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v0, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v115, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v87, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v115, v87, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v95, v0, s60
v_lshlrev_b32 v95, 0x2, v95                        // Bias address scaled by BPE
v_cndmask_b32 v95, v115, v95, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v100, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v94, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v94, v115, v94, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v102, v0, s60
v_lshlrev_b32 v102, 0x2, v102                      // Bias address scaled by BPE
v_cndmask_b32 v102, v115, v102, s[64:65]           // LDBias clip if OOB. offset
v_lshlrev_b32 v103, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v101, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v101, v115, v101, s[64:65]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v113, v0, s60
v_lshlrev_b32 v113, 0x2, v113                      // Bias address scaled by BPE
v_cndmask_b32 v113, v115, v113, s[64:65]           // LDBias clip if OOB. offset
v_lshlrev_b32 v114, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v112, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v112, v115, v112, s[64:65]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+24], acc162         // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+25], acc166         // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+26], acc170         // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+27], acc174         // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+32], acc163         // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+33], acc167         // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+34], acc171         // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+35], acc175         // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+40], acc176         // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+41], acc180         // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+42], acc184         // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+43], acc188         // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+48], acc177         // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+49], acc181         // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+50], acc185         // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+51], acc189         // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+52], acc178         // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+53], acc182         // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+54], acc186         // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+55], acc190         // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+60], acc179         // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+61], acc183         // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+62], acc187         // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+63], acc191         // copy acc to vreg[191]
v_accvgpr_read_b32 v[vgprValuC+68], acc192         // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+69], acc196         // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+70], acc200         // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+71], acc204         // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+76], acc193         // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+77], acc197         // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+78], acc201         // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+79], acc205         // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+80], acc194         // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+81], acc198         // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+82], acc202         // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+83], acc206         // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+88], acc195         // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+89], acc199         // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+90], acc203         // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+91], acc207         // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+96], acc208         // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+97], acc212         // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+98], acc216         // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+99], acc220         // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+104], acc209        // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+105], acc213        // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+106], acc217        // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+107], acc221        // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+108], acc210        // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+109], acc214        // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+110], acc218        // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+111], acc222        // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+116], acc211        // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+117], acc215        // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+118], acc219        // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+119], acc223        // copy acc to vreg[223]

/* rC *= alpha batchElements=[(10, 0, 2, 0), (10, 0, 3, 0), (11, 0, 0, 0), (11, 0, 1, 0), (11, 0, 2, 0), (11, 0, 3, 0), (12, 0, 0, 0), (12, 0, 1, 0), (12, 0, 2, 0), (12, 0, 3, 0), (13, 0, 0, 0), (13, 0, 1, 0), (13, 0, 2, 0), (13, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+25], s[sgprAlpha], v[vgprValuC+25] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+27], s[sgprAlpha], v[vgprValuC+27] // *= alpha
v_mul_f32 v[vgprValuC+32], s[sgprAlpha], v[vgprValuC+32] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+34], s[sgprAlpha], v[vgprValuC+34] // *= alpha
v_mul_f32 v[vgprValuC+35], s[sgprAlpha], v[vgprValuC+35] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+69], s[sgprAlpha], v[vgprValuC+69] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+71], s[sgprAlpha], v[vgprValuC+71] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+97], s[sgprAlpha], v[vgprValuC+97] // *= alpha
v_mul_f32 v[vgprValuC+98], s[sgprAlpha], v[vgprValuC+98] // *= alpha
v_mul_f32 v[vgprValuC+99], s[sgprAlpha], v[vgprValuC+99] // *= alpha
v_mul_f32 v[vgprValuC+104], s[sgprAlpha], v[vgprValuC+104] // *= alpha
v_mul_f32 v[vgprValuC+105], s[sgprAlpha], v[vgprValuC+105] // *= alpha
v_mul_f32 v[vgprValuC+106], s[sgprAlpha], v[vgprValuC+106] // *= alpha
v_mul_f32 v[vgprValuC+107], s[sgprAlpha], v[vgprValuC+107] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+24:vgprValuC+24+1], v[20:21], v[vgprValuC+24:vgprValuC+24+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+26:vgprValuC+26+1], v[22:23], v[vgprValuC+26:vgprValuC+26+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+24:vgprValuC+24+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+26:vgprValuC+26+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_mov_b32 v25, v5
v_mov_b32 v26, v6
v_mov_b32 v27, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+24], 16, v[vgprValuC+24] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+25], v[vgprValuC+25] // check Nan
v_bfe_u32 v9, v[vgprValuC+25], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+25], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+25], v9, v11, s[60:61]
v_and_or_b32 v24, v[vgprValuC+25], v10, v[vgprValuC+24] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+26], 16, v[vgprValuC+26] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+27], v[vgprValuC+27] // check Nan
v_bfe_u32 v9, v[vgprValuC+27], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+27], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+27], v9, v11, s[60:61]
v_and_or_b32 v25, v[vgprValuC+27], v10, v[vgprValuC+26] // pack two bf16 to dword
buffer_store_dwordx2 v[24:25], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+32:vgprValuC+32+1], v[20:21], v[vgprValuC+32:vgprValuC+32+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+34:vgprValuC+34+1], v[22:23], v[vgprValuC+34:vgprValuC+34+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+32:vgprValuC+32+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+34:vgprValuC+34+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v32, v4
v_mov_b32 v33, v5
v_mov_b32 v34, v6
v_mov_b32 v35, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+32], v[vgprValuC+32] // check Nan
v_bfe_u32 v9, v[vgprValuC+32], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+32], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+32], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+32], 16, v[vgprValuC+32] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_and_or_b32 v32, v[vgprValuC+33], v10, v[vgprValuC+32] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+34], v[vgprValuC+34] // check Nan
v_bfe_u32 v9, v[vgprValuC+34], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+34], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+34], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+34], 16, v[vgprValuC+34] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+35], v[vgprValuC+35] // check Nan
v_bfe_u32 v9, v[vgprValuC+35], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+35], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+35], v9, v11, s[60:61]
v_and_or_b32 v33, v[vgprValuC+35], v10, v[vgprValuC+34] // pack two bf16 to dword
buffer_store_dwordx2 v[32:33], v28, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+42:vgprValuC+42+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_mov_b32 v41, v5
v_mov_b32 v42, v6
v_mov_b32 v43, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+40], 16, v[vgprValuC+40] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+41], v[vgprValuC+41] // check Nan
v_bfe_u32 v9, v[vgprValuC+41], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+41], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+41], v9, v11, s[60:61]
v_and_or_b32 v40, v[vgprValuC+41], v10, v[vgprValuC+40] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+42], v[vgprValuC+42] // check Nan
v_bfe_u32 v9, v[vgprValuC+42], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+42], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+42], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+42], 16, v[vgprValuC+42] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+43], v[vgprValuC+43] // check Nan
v_bfe_u32 v9, v[vgprValuC+43], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+43], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+43], v9, v11, s[60:61]
v_and_or_b32 v41, v[vgprValuC+43], v10, v[vgprValuC+42] // pack two bf16 to dword
buffer_store_dwordx2 v[40:41], v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+50:vgprValuC+50+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_mov_b32 v49, v5
v_mov_b32 v50, v6
v_mov_b32 v51, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+48], 16, v[vgprValuC+48] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+49], v[vgprValuC+49] // check Nan
v_bfe_u32 v9, v[vgprValuC+49], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+49], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+49], v9, v11, s[60:61]
v_and_or_b32 v48, v[vgprValuC+49], v10, v[vgprValuC+48] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+50], 16, v[vgprValuC+50] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+51], v[vgprValuC+51] // check Nan
v_bfe_u32 v9, v[vgprValuC+51], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+51], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+51], v9, v11, s[60:61]
v_and_or_b32 v49, v[vgprValuC+51], v10, v[vgprValuC+50] // pack two bf16 to dword
buffer_store_dwordx2 v[48:49], v38, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+54:vgprValuC+54+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_mov_b32 v53, v5
v_mov_b32 v54, v6
v_mov_b32 v55, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+52], 16, v[vgprValuC+52] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+53], v[vgprValuC+53] // check Nan
v_bfe_u32 v9, v[vgprValuC+53], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+53], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+53], v9, v11, s[60:61]
v_and_or_b32 v52, v[vgprValuC+53], v10, v[vgprValuC+52] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+54], v[vgprValuC+54] // check Nan
v_bfe_u32 v9, v[vgprValuC+54], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+54], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+54], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+54], 16, v[vgprValuC+54] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_and_or_b32 v53, v[vgprValuC+55], v10, v[vgprValuC+54] // pack two bf16 to dword
buffer_store_dwordx2 v[52:53], v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+62:vgprValuC+62+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_mov_b32 v61, v5
v_mov_b32 v62, v6
v_mov_b32 v63, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+60], 16, v[vgprValuC+60] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+61], v[vgprValuC+61] // check Nan
v_bfe_u32 v9, v[vgprValuC+61], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+61], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+61], v9, v11, s[60:61]
v_and_or_b32 v60, v[vgprValuC+61], v10, v[vgprValuC+60] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+62], v[vgprValuC+62] // check Nan
v_bfe_u32 v9, v[vgprValuC+62], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+62], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+62], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+62], 16, v[vgprValuC+62] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+63], v[vgprValuC+63] // check Nan
v_bfe_u32 v9, v[vgprValuC+63], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+63], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+63], v9, v11, s[60:61]
v_and_or_b32 v61, v[vgprValuC+63], v10, v[vgprValuC+62] // pack two bf16 to dword
buffer_store_dwordx2 v[60:61], v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+68:vgprValuC+68+1], v[20:21], v[vgprValuC+68:vgprValuC+68+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+70:vgprValuC+70+1], v[22:23], v[vgprValuC+70:vgprValuC+70+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+68:vgprValuC+68+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+70:vgprValuC+70+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_mov_b32 v69, v5
v_mov_b32 v70, v6
v_mov_b32 v71, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+68], 16, v[vgprValuC+68] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+69], v[vgprValuC+69] // check Nan
v_bfe_u32 v9, v[vgprValuC+69], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+69], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+69], v9, v11, s[60:61]
v_and_or_b32 v68, v[vgprValuC+69], v10, v[vgprValuC+68] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+70], 16, v[vgprValuC+70] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+71], v[vgprValuC+71] // check Nan
v_bfe_u32 v9, v[vgprValuC+71], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+71], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+71], v9, v11, s[60:61]
v_and_or_b32 v69, v[vgprValuC+71], v10, v[vgprValuC+70] // pack two bf16 to dword
buffer_store_dwordx2 v[68:69], v59, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[20:21], v[vgprValuC+76:vgprValuC+76+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[22:23], v[vgprValuC+78:vgprValuC+78+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+78:vgprValuC+78+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_mov_b32 v77, v5
v_mov_b32 v78, v6
v_mov_b32 v79, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+76], 16, v[vgprValuC+76] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+77], v[vgprValuC+77] // check Nan
v_bfe_u32 v9, v[vgprValuC+77], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+77], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+77], v9, v11, s[60:61]
v_and_or_b32 v76, v[vgprValuC+77], v10, v[vgprValuC+76] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+78], v[vgprValuC+78] // check Nan
v_bfe_u32 v9, v[vgprValuC+78], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+78], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+78], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+78], 16, v[vgprValuC+78] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+79], v[vgprValuC+79] // check Nan
v_bfe_u32 v9, v[vgprValuC+79], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+79], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+79], v9, v11, s[60:61]
v_and_or_b32 v77, v[vgprValuC+79], v10, v[vgprValuC+78] // pack two bf16 to dword
buffer_store_dwordx2 v[76:77], v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+82:vgprValuC+82+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_mov_b32 v81, v5
v_mov_b32 v82, v6
v_mov_b32 v83, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+80], 16, v[vgprValuC+80] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+81], v[vgprValuC+81] // check Nan
v_bfe_u32 v9, v[vgprValuC+81], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+81], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+81], v9, v11, s[60:61]
v_and_or_b32 v80, v[vgprValuC+81], v10, v[vgprValuC+80] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+82], v[vgprValuC+82] // check Nan
v_bfe_u32 v9, v[vgprValuC+82], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+82], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+82], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+82], 16, v[vgprValuC+82] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+83], v[vgprValuC+83] // check Nan
v_bfe_u32 v9, v[vgprValuC+83], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+83], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+83], v9, v11, s[60:61]
v_and_or_b32 v81, v[vgprValuC+83], v10, v[vgprValuC+82] // pack two bf16 to dword
buffer_store_dwordx2 v[80:81], v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[20:21], v[vgprValuC+88:vgprValuC+88+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[22:23], v[vgprValuC+90:vgprValuC+90+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+90:vgprValuC+90+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v88, v4
v_mov_b32 v89, v5
v_mov_b32 v90, v6
v_mov_b32 v91, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+88], v[vgprValuC+88] // check Nan
v_bfe_u32 v9, v[vgprValuC+88], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+88], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+88], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+88], 16, v[vgprValuC+88] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+89], v[vgprValuC+89] // check Nan
v_bfe_u32 v9, v[vgprValuC+89], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+89], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+89], v9, v11, s[60:61]
v_and_or_b32 v88, v[vgprValuC+89], v10, v[vgprValuC+88] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+90], 16, v[vgprValuC+90] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+91], v[vgprValuC+91] // check Nan
v_bfe_u32 v9, v[vgprValuC+91], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+91], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+91], v9, v11, s[60:61]
v_and_or_b32 v89, v[vgprValuC+91], v10, v[vgprValuC+90] // pack two bf16 to dword
buffer_store_dwordx2 v[88:89], v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+96:vgprValuC+96+1], v[20:21], v[vgprValuC+96:vgprValuC+96+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+98:vgprValuC+98+1], v[22:23], v[vgprValuC+98:vgprValuC+98+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+96:vgprValuC+96+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+98:vgprValuC+98+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v96, v4
v_mov_b32 v97, v5
v_mov_b32 v98, v6
v_mov_b32 v99, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+96], v[vgprValuC+96] // check Nan
v_bfe_u32 v9, v[vgprValuC+96], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+96], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+96], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+96], 16, v[vgprValuC+96] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+97], v[vgprValuC+97] // check Nan
v_bfe_u32 v9, v[vgprValuC+97], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+97], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+97], v9, v11, s[60:61]
v_and_or_b32 v96, v[vgprValuC+97], v10, v[vgprValuC+96] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+98], v[vgprValuC+98] // check Nan
v_bfe_u32 v9, v[vgprValuC+98], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+98], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+98], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+98], 16, v[vgprValuC+98] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+99], v[vgprValuC+99] // check Nan
v_bfe_u32 v9, v[vgprValuC+99], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+99], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+99], v9, v11, s[60:61]
v_and_or_b32 v97, v[vgprValuC+99], v10, v[vgprValuC+98] // pack two bf16 to dword
buffer_store_dwordx2 v[96:97], v87, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+104:vgprValuC+104+1], v[20:21], v[vgprValuC+104:vgprValuC+104+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+106:vgprValuC+106+1], v[22:23], v[vgprValuC+106:vgprValuC+106+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+104:vgprValuC+104+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+106:vgprValuC+106+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v104, v4
v_mov_b32 v105, v5
v_mov_b32 v106, v6
v_mov_b32 v107, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+104], v[vgprValuC+104] // check Nan
v_bfe_u32 v9, v[vgprValuC+104], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+104], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+104], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+104], 16, v[vgprValuC+104] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+105], v[vgprValuC+105] // check Nan
v_bfe_u32 v9, v[vgprValuC+105], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+105], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+105], v9, v11, s[60:61]
v_and_or_b32 v104, v[vgprValuC+105], v10, v[vgprValuC+104] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+106], v[vgprValuC+106] // check Nan
v_bfe_u32 v9, v[vgprValuC+106], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+106], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+106], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+106], 16, v[vgprValuC+106] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+107], v[vgprValuC+107] // check Nan
v_bfe_u32 v9, v[vgprValuC+107], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+107], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+107], v9, v11, s[60:61]
v_and_or_b32 v105, v[vgprValuC+107], v10, v[vgprValuC+106] // pack two bf16 to dword
buffer_store_dwordx2 v[104:105], v94, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[20:21], v[vgprValuC+108:vgprValuC+108+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[22:23], v[vgprValuC+110:vgprValuC+110+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+110:vgprValuC+110+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v108, v4
v_mov_b32 v109, v5
v_mov_b32 v110, v6
v_mov_b32 v111, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+108], v[vgprValuC+108] // check Nan
v_bfe_u32 v9, v[vgprValuC+108], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+108], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+108], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+108], 16, v[vgprValuC+108] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+109], v[vgprValuC+109] // check Nan
v_bfe_u32 v9, v[vgprValuC+109], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+109], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+109], v9, v11, s[60:61]
v_and_or_b32 v108, v[vgprValuC+109], v10, v[vgprValuC+108] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+110], v[vgprValuC+110] // check Nan
v_bfe_u32 v9, v[vgprValuC+110], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+110], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+110], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+110], 16, v[vgprValuC+110] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+111], v[vgprValuC+111] // check Nan
v_bfe_u32 v9, v[vgprValuC+111], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+111], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+111], v9, v11, s[60:61]
v_and_or_b32 v109, v[vgprValuC+111], v10, v[vgprValuC+110] // pack two bf16 to dword
buffer_store_dwordx2 v[108:109], v101, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v20, 1.0, v20, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v21, 1.0, v21, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[20:21], v[vgprValuC+116:vgprValuC+116+1] // *= scaleAlphaVecVMulPK(20)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v22, 1.0, v22, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[22:23], v[vgprValuC+118:vgprValuC+118+1] // *= scaleAlphaVecVMulPK(20)(2)
v_pk_add_f32 v[4:5], v[16:17], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[6:7], v[18:19], v[vgprValuC+118:vgprValuC+118+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v116, v4
v_mov_b32 v117, v5
v_mov_b32 v118, v6
v_mov_b32 v119, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+116], v[vgprValuC+116] // check Nan
v_bfe_u32 v9, v[vgprValuC+116], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+116], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+116], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+116], 16, v[vgprValuC+116] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+117], v[vgprValuC+117] // check Nan
v_bfe_u32 v9, v[vgprValuC+117], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+117], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+117], v9, v11, s[60:61]
v_and_or_b32 v116, v[vgprValuC+117], v10, v[vgprValuC+116] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+118], v[vgprValuC+118] // check Nan
v_bfe_u32 v9, v[vgprValuC+118], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+118], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+118], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+118], 16, v[vgprValuC+118] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+119], v[vgprValuC+119] // check Nan
v_bfe_u32 v9, v[vgprValuC+119], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+119], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+119], v9, v11, s[60:61]
v_and_or_b32 v117, v[vgprValuC+119], v10, v[vgprValuC+118] // pack two bf16 to dword
buffer_store_dwordx2 v[116:117], v112, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
// jump to end
s_getpc_b64 s[60:61]                               // addr of next instr
s_add_i32 s62, label_GW_End_2, 0x4                 // target branch offset
s_add_u32 s60, s60, s62                            // add target branch offset
s_addc_u32 s61, s61, 0                             // add high and carry
s_setpc_b64 s[60:61]                               // branch to label_GW_End_2
label_GW_B0_E1_M_1:
s_cmpk_eq_u32 s[sgprActivationType], 1             // activationType == 1
s_cbranch_scc1 label_To_Activation_Abs_VW1_beta_0_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 2             // activationType == 2
s_cbranch_scc1 label_To_Activation_Clippedrelu_VW1_beta_0_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 3             // activationType == 3
s_cbranch_scc1 label_To_Activation_Gelu_VW1_beta_0_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 4             // activationType == 4
s_cbranch_scc1 label_To_Activation_Leakyrelu_VW1_beta_0_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 5             // activationType == 5
s_cbranch_scc1 label_To_Activation_Relu_VW1_beta_0_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 6             // activationType == 6
s_cbranch_scc1 label_To_Activation_Sigmoid_VW1_beta_0_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 7             // activationType == 7
s_cbranch_scc1 label_To_Activation_Tanh_VW1_beta_0_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 9             // activationType == 9
s_cbranch_scc1 label_To_Activation_Geluscaling_VW1_beta_0_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 10            // activationType == 10
s_cbranch_scc1 label_To_Activation_Silu_VW1_beta_0_edge_1 // Branch if true
label_To_Activation_None_VW1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_None_VW1, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_4
label_To_Activation_Abs_VW1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Abs_VW1, 0x4        // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_4
label_To_Activation_Clippedrelu_VW1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Clippedrelu_VW1, 0x4 // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_4
label_To_Activation_Gelu_VW1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Gelu_VW1, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_4
label_To_Activation_Leakyrelu_VW1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Leakyrelu_VW1, 0x4  // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_4
label_To_Activation_Relu_VW1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Relu_VW1, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_4
label_To_Activation_Sigmoid_VW1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Sigmoid_VW1, 0x4    // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_4
label_To_Activation_Tanh_VW1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Tanh_VW1, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_4
label_To_Activation_Geluscaling_VW1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Geluscaling_VW1, 0x4 // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_4
label_To_Activation_Silu_VW1_beta_0_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Silu_VW1, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_4
label_ActivationSetPCAddrEnd_4:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=16 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw1); (0,0,0,1:vw1); (0,0,0,2:vw1); (0,0,0,3:vw1); (0,0,1,0:vw1); (0,0,1,1:vw1); (0,0,1,2:vw1); (0,0,1,3:vw1); (0,0,2,0:vw1); (0,0,2,1:vw1); (0,0,2,2:vw1); (0,0,2,3:vw1); (0,0,3,0:vw1); (0,0,3,1:vw1); (0,0,3,2:vw1); (0,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v85, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v85, v14, s[64:65]              // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b32 v16, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v17, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v85, v13, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v20, v4, s60
v_lshlrev_b32 v20, 0x2, v20                        // Bias address scaled by BPE
v_cndmask_b32 v20, v85, v20, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v22, v20 offset:0                      // load bias
v_lshlrev_b32 v21, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v23, v21, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v19, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v19, v85, v19, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v26, v4, s60
v_lshlrev_b32 v26, 0x2, v26                        // Bias address scaled by BPE
v_cndmask_b32 v26, v85, v26, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v28, v26 offset:0                      // load bias
v_lshlrev_b32 v27, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v29, v27, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v25, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v25, v85, v25, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s60
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v85, v32, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v34, v32 offset:0                      // load bias
v_lshlrev_b32 v33, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v35, v33, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v85, v31, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v38, v0, s60
v_lshlrev_b32 v38, 0x2, v38                        // Bias address scaled by BPE
v_cndmask_b32 v38, v85, v38, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v39, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v37, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v37, v85, v37, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v85, v42, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v85, v41, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v85, v46, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v85, v45, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v50, v4, s60
v_lshlrev_b32 v50, 0x2, v50                        // Bias address scaled by BPE
v_cndmask_b32 v50, v85, v50, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v51, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v49, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v49, v85, v49, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v85, v54, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v53, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v53, v85, v53, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v58, v4, s60
v_lshlrev_b32 v58, 0x2, v58                        // Bias address scaled by BPE
v_cndmask_b32 v58, v85, v58, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v59, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v57, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v57, v85, v57, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v4, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v85, v62, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v85, v61, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v66, v4, s60
v_lshlrev_b32 v66, 0x2, v66                        // Bias address scaled by BPE
v_cndmask_b32 v66, v85, v66, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v67, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v65, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v65, v85, v65, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v0, s60
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_cndmask_b32 v70, v85, v70, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v71, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v85, v69, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v4, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v85, v74, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v85, v73, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s60
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_cndmask_b32 v78, v85, v78, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v79, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v85, v77, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v4, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v85, v82, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v85, v81, s[64:65]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+18], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+24], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+30], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+36], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+40], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+44], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+48], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+52], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+56], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+60], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+64], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+68], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+72], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+76], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+80], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+84], acc15          // copy acc to vreg[15]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 0, 2), (0, 0, 0, 3), (0, 0, 1, 0), (0, 0, 1, 1), (0, 0, 1, 2), (0, 0, 1, 3), (0, 0, 2, 0), (0, 0, 2, 1), (0, 0, 2, 2), (0, 0, 2, 3), (0, 0, 3, 0), (0, 0, 3, 1), (0, 0, 3, 2), (0, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+18], s[sgprAlpha], v[vgprValuC+18] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+18], v17, v[vgprValuC+18]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+18]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v18, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+18], v[vgprValuC+18] // check Nan
v_bfe_u32 v9, v[vgprValuC+18], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+18], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+18], v9, v11, s[60:61]
v_lshrrev_b32 v18, 16, v[vgprValuC+18]             // convert C to bf16
buffer_store_short v18, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+24], v23, v[vgprValuC+24]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+24]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v24, 16, v[vgprValuC+24]             // convert C to bf16
buffer_store_short v24, v19, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+30], v29, v[vgprValuC+30]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+30]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v30, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v30, 16, v[vgprValuC+30]             // convert C to bf16
buffer_store_short v30, v25, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+36], v35, v[vgprValuC+36]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+36]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v36, 16, v[vgprValuC+36]             // convert C to bf16
buffer_store_short v36, v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v17, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v37, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+44], v23, v[vgprValuC+44]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+44]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v44, 16, v[vgprValuC+44]             // convert C to bf16
buffer_store_short v44, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+48], v29, v[vgprValuC+48]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+48]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v48, 16, v[vgprValuC+48]             // convert C to bf16
buffer_store_short v48, v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+52], v35, v[vgprValuC+52]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+52]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v52, 16, v[vgprValuC+52]             // convert C to bf16
buffer_store_short v52, v49, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+56], v17, v[vgprValuC+56]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+56]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v56, 16, v[vgprValuC+56]             // convert C to bf16
buffer_store_short v56, v53, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v23, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v57, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+64], v29, v[vgprValuC+64]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+64]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v64, 16, v[vgprValuC+64]             // convert C to bf16
buffer_store_short v64, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+68], v35, v[vgprValuC+68]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+68]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v68, 16, v[vgprValuC+68]             // convert C to bf16
buffer_store_short v68, v65, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+72], v17, v[vgprValuC+72]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+72]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v72, 16, v[vgprValuC+72]             // convert C to bf16
buffer_store_short v72, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+76], v23, v[vgprValuC+76]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+76]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v76, 16, v[vgprValuC+76]             // convert C to bf16
buffer_store_short v76, v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v29, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+84], v35, v[vgprValuC+84]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+84]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v84, 16, v[vgprValuC+84]             // convert C to bf16
buffer_store_short v84, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (1,0,0,0:vw1); (1,0,0,1:vw1); (1,0,0,2:vw1); (1,0,0,3:vw1); (1,0,1,0:vw1); (1,0,1,1:vw1); (1,0,1,2:vw1); (1,0,1,3:vw1); (1,0,2,0:vw1); (1,0,2,1:vw1); (1,0,2,2:vw1); (1,0,2,3:vw1); (1,0,3,0:vw1); (1,0,3,1:vw1); (1,0,3,2:vw1); (1,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v85, BufferOOB
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v85, v14, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v16, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v17, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v85, v13, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v20, v4, s60
v_lshlrev_b32 v20, 0x2, v20                        // Bias address scaled by BPE
v_cndmask_b32 v20, v85, v20, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v22, v20 offset:0                      // load bias
v_lshlrev_b32 v21, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v23, v21, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v19, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v19, v85, v19, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v26, v4, s60
v_lshlrev_b32 v26, 0x2, v26                        // Bias address scaled by BPE
v_cndmask_b32 v26, v85, v26, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v28, v26 offset:0                      // load bias
v_lshlrev_b32 v27, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v29, v27, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v25, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v25, v85, v25, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s60
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v85, v32, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v34, v32 offset:0                      // load bias
v_lshlrev_b32 v33, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v35, v33, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v85, v31, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v38, v0, s60
v_lshlrev_b32 v38, 0x2, v38                        // Bias address scaled by BPE
v_cndmask_b32 v38, v85, v38, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v39, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v37, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v37, v85, v37, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v85, v42, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v85, v41, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v85, v46, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v85, v45, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v50, v4, s60
v_lshlrev_b32 v50, 0x2, v50                        // Bias address scaled by BPE
v_cndmask_b32 v50, v85, v50, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v51, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v49, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v49, v85, v49, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v85, v54, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v53, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v53, v85, v53, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v58, v4, s60
v_lshlrev_b32 v58, 0x2, v58                        // Bias address scaled by BPE
v_cndmask_b32 v58, v85, v58, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v59, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v57, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v57, v85, v57, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v4, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v85, v62, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v85, v61, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v66, v4, s60
v_lshlrev_b32 v66, 0x2, v66                        // Bias address scaled by BPE
v_cndmask_b32 v66, v85, v66, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v67, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v65, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v65, v85, v65, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v0, s60
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_cndmask_b32 v70, v85, v70, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v71, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v85, v69, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v4, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v85, v74, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v85, v73, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s60
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_cndmask_b32 v78, v85, v78, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v79, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v85, v77, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v4, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v85, v82, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v85, v81, s[64:65]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+18], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+24], acc20          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+30], acc24          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+36], acc28          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+40], acc17          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+44], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+48], acc25          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+52], acc29          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+56], acc18          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+60], acc22          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+64], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+68], acc30          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+72], acc19          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+76], acc23          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+80], acc27          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+84], acc31          // copy acc to vreg[31]

/* rC *= alpha batchElements=[(1, 0, 0, 0), (1, 0, 0, 1), (1, 0, 0, 2), (1, 0, 0, 3), (1, 0, 1, 0), (1, 0, 1, 1), (1, 0, 1, 2), (1, 0, 1, 3), (1, 0, 2, 0), (1, 0, 2, 1), (1, 0, 2, 2), (1, 0, 2, 3), (1, 0, 3, 0), (1, 0, 3, 1), (1, 0, 3, 2), (1, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+18], s[sgprAlpha], v[vgprValuC+18] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+18], v17, v[vgprValuC+18]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+18]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v18, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+18], v[vgprValuC+18] // check Nan
v_bfe_u32 v9, v[vgprValuC+18], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+18], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+18], v9, v11, s[60:61]
v_lshrrev_b32 v18, 16, v[vgprValuC+18]             // convert C to bf16
buffer_store_short v18, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+24], v23, v[vgprValuC+24]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+24]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v24, 16, v[vgprValuC+24]             // convert C to bf16
buffer_store_short v24, v19, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+30], v29, v[vgprValuC+30]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+30]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v30, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v30, 16, v[vgprValuC+30]             // convert C to bf16
buffer_store_short v30, v25, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+36], v35, v[vgprValuC+36]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+36]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v36, 16, v[vgprValuC+36]             // convert C to bf16
buffer_store_short v36, v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v17, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v37, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+44], v23, v[vgprValuC+44]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+44]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v44, 16, v[vgprValuC+44]             // convert C to bf16
buffer_store_short v44, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+48], v29, v[vgprValuC+48]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+48]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v48, 16, v[vgprValuC+48]             // convert C to bf16
buffer_store_short v48, v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+52], v35, v[vgprValuC+52]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+52]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v52, 16, v[vgprValuC+52]             // convert C to bf16
buffer_store_short v52, v49, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+56], v17, v[vgprValuC+56]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+56]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v56, 16, v[vgprValuC+56]             // convert C to bf16
buffer_store_short v56, v53, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v23, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v57, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+64], v29, v[vgprValuC+64]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+64]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v64, 16, v[vgprValuC+64]             // convert C to bf16
buffer_store_short v64, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+68], v35, v[vgprValuC+68]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+68]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v68, 16, v[vgprValuC+68]             // convert C to bf16
buffer_store_short v68, v65, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+72], v17, v[vgprValuC+72]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+72]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v72, 16, v[vgprValuC+72]             // convert C to bf16
buffer_store_short v72, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+76], v23, v[vgprValuC+76]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+76]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v76, 16, v[vgprValuC+76]             // convert C to bf16
buffer_store_short v76, v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v29, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+84], v35, v[vgprValuC+84]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+84]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v84, 16, v[vgprValuC+84]             // convert C to bf16
buffer_store_short v84, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #2 (d1,d0,vc1,vc0) = */
/*    (2,0,0,0:vw1); (2,0,0,1:vw1); (2,0,0,2:vw1); (2,0,0,3:vw1); (2,0,1,0:vw1); (2,0,1,1:vw1); (2,0,1,2:vw1); (2,0,1,3:vw1); (2,0,2,0:vw1); (2,0,2,1:vw1); (2,0,2,2:vw1); (2,0,2,3:vw1); (2,0,3,0:vw1); (2,0,3,1:vw1); (2,0,3,2:vw1); (2,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v85, BufferOOB
/* (d1,vc1,d0,vc0)=(2,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v85, v14, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v16, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v17, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v85, v13, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v20, v4, s60
v_lshlrev_b32 v20, 0x2, v20                        // Bias address scaled by BPE
v_cndmask_b32 v20, v85, v20, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v22, v20 offset:0                      // load bias
v_lshlrev_b32 v21, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v23, v21, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v19, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v19, v85, v19, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v26, v4, s60
v_lshlrev_b32 v26, 0x2, v26                        // Bias address scaled by BPE
v_cndmask_b32 v26, v85, v26, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v28, v26 offset:0                      // load bias
v_lshlrev_b32 v27, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v29, v27, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v25, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v25, v85, v25, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s60
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v85, v32, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v34, v32 offset:0                      // load bias
v_lshlrev_b32 v33, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v35, v33, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v85, v31, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v38, v0, s60
v_lshlrev_b32 v38, 0x2, v38                        // Bias address scaled by BPE
v_cndmask_b32 v38, v85, v38, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v39, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v37, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v37, v85, v37, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v85, v42, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v85, v41, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v85, v46, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v85, v45, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v50, v4, s60
v_lshlrev_b32 v50, 0x2, v50                        // Bias address scaled by BPE
v_cndmask_b32 v50, v85, v50, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v51, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v49, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v49, v85, v49, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v85, v54, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v53, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v53, v85, v53, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v58, v4, s60
v_lshlrev_b32 v58, 0x2, v58                        // Bias address scaled by BPE
v_cndmask_b32 v58, v85, v58, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v59, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v57, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v57, v85, v57, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v4, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v85, v62, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v85, v61, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v66, v4, s60
v_lshlrev_b32 v66, 0x2, v66                        // Bias address scaled by BPE
v_cndmask_b32 v66, v85, v66, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v67, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v65, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v65, v85, v65, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v0, s60
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_cndmask_b32 v70, v85, v70, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v71, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v85, v69, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v4, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v85, v74, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v85, v73, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s60
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_cndmask_b32 v78, v85, v78, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v79, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v85, v77, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v4, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v85, v82, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v85, v81, s[64:65]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+18], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+24], acc36          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+30], acc40          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+36], acc44          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+40], acc33          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+44], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+48], acc41          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+52], acc45          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+56], acc34          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+60], acc38          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+64], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+68], acc46          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+72], acc35          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+76], acc39          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+80], acc43          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+84], acc47          // copy acc to vreg[47]

/* rC *= alpha batchElements=[(2, 0, 0, 0), (2, 0, 0, 1), (2, 0, 0, 2), (2, 0, 0, 3), (2, 0, 1, 0), (2, 0, 1, 1), (2, 0, 1, 2), (2, 0, 1, 3), (2, 0, 2, 0), (2, 0, 2, 1), (2, 0, 2, 2), (2, 0, 2, 3), (2, 0, 3, 0), (2, 0, 3, 1), (2, 0, 3, 2), (2, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+18], s[sgprAlpha], v[vgprValuC+18] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+18], v17, v[vgprValuC+18]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+18]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v18, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+18], v[vgprValuC+18] // check Nan
v_bfe_u32 v9, v[vgprValuC+18], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+18], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+18], v9, v11, s[60:61]
v_lshrrev_b32 v18, 16, v[vgprValuC+18]             // convert C to bf16
buffer_store_short v18, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+24], v23, v[vgprValuC+24]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+24]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v24, 16, v[vgprValuC+24]             // convert C to bf16
buffer_store_short v24, v19, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+30], v29, v[vgprValuC+30]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+30]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v30, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v30, 16, v[vgprValuC+30]             // convert C to bf16
buffer_store_short v30, v25, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+36], v35, v[vgprValuC+36]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+36]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v36, 16, v[vgprValuC+36]             // convert C to bf16
buffer_store_short v36, v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v17, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v37, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+44], v23, v[vgprValuC+44]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+44]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v44, 16, v[vgprValuC+44]             // convert C to bf16
buffer_store_short v44, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+48], v29, v[vgprValuC+48]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+48]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v48, 16, v[vgprValuC+48]             // convert C to bf16
buffer_store_short v48, v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+52], v35, v[vgprValuC+52]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+52]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v52, 16, v[vgprValuC+52]             // convert C to bf16
buffer_store_short v52, v49, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+56], v17, v[vgprValuC+56]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+56]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v56, 16, v[vgprValuC+56]             // convert C to bf16
buffer_store_short v56, v53, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v23, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v57, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+64], v29, v[vgprValuC+64]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+64]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v64, 16, v[vgprValuC+64]             // convert C to bf16
buffer_store_short v64, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+68], v35, v[vgprValuC+68]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+68]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v68, 16, v[vgprValuC+68]             // convert C to bf16
buffer_store_short v68, v65, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+72], v17, v[vgprValuC+72]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+72]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v72, 16, v[vgprValuC+72]             // convert C to bf16
buffer_store_short v72, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+76], v23, v[vgprValuC+76]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+76]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v76, 16, v[vgprValuC+76]             // convert C to bf16
buffer_store_short v76, v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v29, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+84], v35, v[vgprValuC+84]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+84]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v84, 16, v[vgprValuC+84]             // convert C to bf16
buffer_store_short v84, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #3 (d1,d0,vc1,vc0) = */
/*    (3,0,0,0:vw1); (3,0,0,1:vw1); (3,0,0,2:vw1); (3,0,0,3:vw1); (3,0,1,0:vw1); (3,0,1,1:vw1); (3,0,1,2:vw1); (3,0,1,3:vw1); (3,0,2,0:vw1); (3,0,2,1:vw1); (3,0,2,2:vw1); (3,0,2,3:vw1); (3,0,3,0:vw1); (3,0,3,1:vw1); (3,0,3,2:vw1); (3,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v85, BufferOOB
/* (d1,vc1,d0,vc0)=(3,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v85, v14, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v16, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v17, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v85, v13, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v20, v4, s60
v_lshlrev_b32 v20, 0x2, v20                        // Bias address scaled by BPE
v_cndmask_b32 v20, v85, v20, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v22, v20 offset:0                      // load bias
v_lshlrev_b32 v21, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v23, v21, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v19, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v19, v85, v19, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v26, v4, s60
v_lshlrev_b32 v26, 0x2, v26                        // Bias address scaled by BPE
v_cndmask_b32 v26, v85, v26, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v28, v26 offset:0                      // load bias
v_lshlrev_b32 v27, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v29, v27, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v25, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v25, v85, v25, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s60
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v85, v32, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v34, v32 offset:0                      // load bias
v_lshlrev_b32 v33, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v35, v33, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v85, v31, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v38, v0, s60
v_lshlrev_b32 v38, 0x2, v38                        // Bias address scaled by BPE
v_cndmask_b32 v38, v85, v38, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v39, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v37, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v37, v85, v37, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v85, v42, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v85, v41, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v85, v46, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v85, v45, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v50, v4, s60
v_lshlrev_b32 v50, 0x2, v50                        // Bias address scaled by BPE
v_cndmask_b32 v50, v85, v50, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v51, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v49, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v49, v85, v49, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v85, v54, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v53, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v53, v85, v53, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v58, v4, s60
v_lshlrev_b32 v58, 0x2, v58                        // Bias address scaled by BPE
v_cndmask_b32 v58, v85, v58, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v59, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v57, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v57, v85, v57, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v4, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v85, v62, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v85, v61, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v66, v4, s60
v_lshlrev_b32 v66, 0x2, v66                        // Bias address scaled by BPE
v_cndmask_b32 v66, v85, v66, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v67, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v65, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v65, v85, v65, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v0, s60
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_cndmask_b32 v70, v85, v70, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v71, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v85, v69, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v4, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v85, v74, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v85, v73, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s60
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_cndmask_b32 v78, v85, v78, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v79, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v85, v77, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v4, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v85, v82, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v85, v81, s[64:65]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+18], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+24], acc52          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+30], acc56          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+36], acc60          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+40], acc49          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+44], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+48], acc57          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+52], acc61          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+56], acc50          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+60], acc54          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+64], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+68], acc62          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+72], acc51          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+76], acc55          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+80], acc59          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+84], acc63          // copy acc to vreg[63]

/* rC *= alpha batchElements=[(3, 0, 0, 0), (3, 0, 0, 1), (3, 0, 0, 2), (3, 0, 0, 3), (3, 0, 1, 0), (3, 0, 1, 1), (3, 0, 1, 2), (3, 0, 1, 3), (3, 0, 2, 0), (3, 0, 2, 1), (3, 0, 2, 2), (3, 0, 2, 3), (3, 0, 3, 0), (3, 0, 3, 1), (3, 0, 3, 2), (3, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+18], s[sgprAlpha], v[vgprValuC+18] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+18], v17, v[vgprValuC+18]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+18]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v18, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+18], v[vgprValuC+18] // check Nan
v_bfe_u32 v9, v[vgprValuC+18], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+18], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+18], v9, v11, s[60:61]
v_lshrrev_b32 v18, 16, v[vgprValuC+18]             // convert C to bf16
buffer_store_short v18, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+24], v23, v[vgprValuC+24]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+24]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v24, 16, v[vgprValuC+24]             // convert C to bf16
buffer_store_short v24, v19, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+30], v29, v[vgprValuC+30]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+30]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v30, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v30, 16, v[vgprValuC+30]             // convert C to bf16
buffer_store_short v30, v25, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+36], v35, v[vgprValuC+36]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+36]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v36, 16, v[vgprValuC+36]             // convert C to bf16
buffer_store_short v36, v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v17, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v37, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+44], v23, v[vgprValuC+44]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+44]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v44, 16, v[vgprValuC+44]             // convert C to bf16
buffer_store_short v44, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+48], v29, v[vgprValuC+48]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+48]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v48, 16, v[vgprValuC+48]             // convert C to bf16
buffer_store_short v48, v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+52], v35, v[vgprValuC+52]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+52]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v52, 16, v[vgprValuC+52]             // convert C to bf16
buffer_store_short v52, v49, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+56], v17, v[vgprValuC+56]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+56]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v56, 16, v[vgprValuC+56]             // convert C to bf16
buffer_store_short v56, v53, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v23, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v57, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+64], v29, v[vgprValuC+64]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+64]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v64, 16, v[vgprValuC+64]             // convert C to bf16
buffer_store_short v64, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+68], v35, v[vgprValuC+68]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+68]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v68, 16, v[vgprValuC+68]             // convert C to bf16
buffer_store_short v68, v65, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+72], v17, v[vgprValuC+72]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+72]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v72, 16, v[vgprValuC+72]             // convert C to bf16
buffer_store_short v72, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+76], v23, v[vgprValuC+76]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+76]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v76, 16, v[vgprValuC+76]             // convert C to bf16
buffer_store_short v76, v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v29, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+84], v35, v[vgprValuC+84]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+84]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v84, 16, v[vgprValuC+84]             // convert C to bf16
buffer_store_short v84, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #4 (d1,d0,vc1,vc0) = */
/*    (4,0,0,0:vw1); (4,0,0,1:vw1); (4,0,0,2:vw1); (4,0,0,3:vw1); (4,0,1,0:vw1); (4,0,1,1:vw1); (4,0,1,2:vw1); (4,0,1,3:vw1); (4,0,2,0:vw1); (4,0,2,1:vw1); (4,0,2,2:vw1); (4,0,2,3:vw1); (4,0,3,0:vw1); (4,0,3,1:vw1); (4,0,3,2:vw1); (4,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v85, BufferOOB
/* (d1,vc1,d0,vc0)=(4,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v85, v14, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v16, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v17, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v85, v13, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v20, v4, s60
v_lshlrev_b32 v20, 0x2, v20                        // Bias address scaled by BPE
v_cndmask_b32 v20, v85, v20, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v22, v20 offset:0                      // load bias
v_lshlrev_b32 v21, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v23, v21, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v19, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v19, v85, v19, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v26, v4, s60
v_lshlrev_b32 v26, 0x2, v26                        // Bias address scaled by BPE
v_cndmask_b32 v26, v85, v26, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v28, v26 offset:0                      // load bias
v_lshlrev_b32 v27, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v29, v27, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v25, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v25, v85, v25, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s60
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v85, v32, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v34, v32 offset:0                      // load bias
v_lshlrev_b32 v33, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v35, v33, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v85, v31, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v38, v0, s60
v_lshlrev_b32 v38, 0x2, v38                        // Bias address scaled by BPE
v_cndmask_b32 v38, v85, v38, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v39, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v37, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v37, v85, v37, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v85, v42, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v85, v41, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v85, v46, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v85, v45, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v50, v4, s60
v_lshlrev_b32 v50, 0x2, v50                        // Bias address scaled by BPE
v_cndmask_b32 v50, v85, v50, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v51, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v49, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v49, v85, v49, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v85, v54, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v53, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v53, v85, v53, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v58, v4, s60
v_lshlrev_b32 v58, 0x2, v58                        // Bias address scaled by BPE
v_cndmask_b32 v58, v85, v58, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v59, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v57, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v57, v85, v57, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v4, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v85, v62, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v85, v61, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v66, v4, s60
v_lshlrev_b32 v66, 0x2, v66                        // Bias address scaled by BPE
v_cndmask_b32 v66, v85, v66, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v67, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v65, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v65, v85, v65, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v0, s60
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_cndmask_b32 v70, v85, v70, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v71, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v85, v69, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v4, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v85, v74, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v85, v73, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s60
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_cndmask_b32 v78, v85, v78, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v79, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v85, v77, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v4, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v85, v82, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v85, v81, s[64:65]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+18], acc64          // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+24], acc68          // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+30], acc72          // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+36], acc76          // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+40], acc65          // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+44], acc69          // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+48], acc73          // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+52], acc77          // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+56], acc66          // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+60], acc70          // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+64], acc74          // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+68], acc78          // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+72], acc67          // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+76], acc71          // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+80], acc75          // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+84], acc79          // copy acc to vreg[79]

/* rC *= alpha batchElements=[(4, 0, 0, 0), (4, 0, 0, 1), (4, 0, 0, 2), (4, 0, 0, 3), (4, 0, 1, 0), (4, 0, 1, 1), (4, 0, 1, 2), (4, 0, 1, 3), (4, 0, 2, 0), (4, 0, 2, 1), (4, 0, 2, 2), (4, 0, 2, 3), (4, 0, 3, 0), (4, 0, 3, 1), (4, 0, 3, 2), (4, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+18], s[sgprAlpha], v[vgprValuC+18] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+18], v17, v[vgprValuC+18]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+18]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v18, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+18], v[vgprValuC+18] // check Nan
v_bfe_u32 v9, v[vgprValuC+18], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+18], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+18], v9, v11, s[60:61]
v_lshrrev_b32 v18, 16, v[vgprValuC+18]             // convert C to bf16
buffer_store_short v18, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+24], v23, v[vgprValuC+24]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+24]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v24, 16, v[vgprValuC+24]             // convert C to bf16
buffer_store_short v24, v19, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+30], v29, v[vgprValuC+30]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+30]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v30, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v30, 16, v[vgprValuC+30]             // convert C to bf16
buffer_store_short v30, v25, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+36], v35, v[vgprValuC+36]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+36]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v36, 16, v[vgprValuC+36]             // convert C to bf16
buffer_store_short v36, v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v17, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v37, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+44], v23, v[vgprValuC+44]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+44]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v44, 16, v[vgprValuC+44]             // convert C to bf16
buffer_store_short v44, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+48], v29, v[vgprValuC+48]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+48]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v48, 16, v[vgprValuC+48]             // convert C to bf16
buffer_store_short v48, v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+52], v35, v[vgprValuC+52]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+52]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v52, 16, v[vgprValuC+52]             // convert C to bf16
buffer_store_short v52, v49, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+56], v17, v[vgprValuC+56]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+56]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v56, 16, v[vgprValuC+56]             // convert C to bf16
buffer_store_short v56, v53, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v23, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v57, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+64], v29, v[vgprValuC+64]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+64]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v64, 16, v[vgprValuC+64]             // convert C to bf16
buffer_store_short v64, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+68], v35, v[vgprValuC+68]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+68]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v68, 16, v[vgprValuC+68]             // convert C to bf16
buffer_store_short v68, v65, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+72], v17, v[vgprValuC+72]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+72]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v72, 16, v[vgprValuC+72]             // convert C to bf16
buffer_store_short v72, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+76], v23, v[vgprValuC+76]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+76]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v76, 16, v[vgprValuC+76]             // convert C to bf16
buffer_store_short v76, v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v29, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+84], v35, v[vgprValuC+84]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+84]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v84, 16, v[vgprValuC+84]             // convert C to bf16
buffer_store_short v84, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #5 (d1,d0,vc1,vc0) = */
/*    (5,0,0,0:vw1); (5,0,0,1:vw1); (5,0,0,2:vw1); (5,0,0,3:vw1); (5,0,1,0:vw1); (5,0,1,1:vw1); (5,0,1,2:vw1); (5,0,1,3:vw1); (5,0,2,0:vw1); (5,0,2,1:vw1); (5,0,2,2:vw1); (5,0,2,3:vw1); (5,0,3,0:vw1); (5,0,3,1:vw1); (5,0,3,2:vw1); (5,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v85, BufferOOB
/* (d1,vc1,d0,vc0)=(5,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v85, v14, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v16, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v17, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v85, v13, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v20, v4, s60
v_lshlrev_b32 v20, 0x2, v20                        // Bias address scaled by BPE
v_cndmask_b32 v20, v85, v20, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v22, v20 offset:0                      // load bias
v_lshlrev_b32 v21, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v23, v21, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v19, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v19, v85, v19, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v26, v4, s60
v_lshlrev_b32 v26, 0x2, v26                        // Bias address scaled by BPE
v_cndmask_b32 v26, v85, v26, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v28, v26 offset:0                      // load bias
v_lshlrev_b32 v27, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v29, v27, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v25, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v25, v85, v25, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s60
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v85, v32, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v34, v32 offset:0                      // load bias
v_lshlrev_b32 v33, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v35, v33, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v85, v31, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v38, v0, s60
v_lshlrev_b32 v38, 0x2, v38                        // Bias address scaled by BPE
v_cndmask_b32 v38, v85, v38, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v39, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v37, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v37, v85, v37, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v85, v42, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v85, v41, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v85, v46, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v85, v45, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v50, v4, s60
v_lshlrev_b32 v50, 0x2, v50                        // Bias address scaled by BPE
v_cndmask_b32 v50, v85, v50, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v51, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v49, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v49, v85, v49, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v85, v54, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v53, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v53, v85, v53, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v58, v4, s60
v_lshlrev_b32 v58, 0x2, v58                        // Bias address scaled by BPE
v_cndmask_b32 v58, v85, v58, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v59, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v57, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v57, v85, v57, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v4, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v85, v62, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v85, v61, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v66, v4, s60
v_lshlrev_b32 v66, 0x2, v66                        // Bias address scaled by BPE
v_cndmask_b32 v66, v85, v66, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v67, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v65, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v65, v85, v65, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v0, s60
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_cndmask_b32 v70, v85, v70, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v71, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v85, v69, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v4, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v85, v74, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v85, v73, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s60
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_cndmask_b32 v78, v85, v78, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v79, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v85, v77, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v4, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v85, v82, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v85, v81, s[64:65]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+18], acc80          // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+24], acc84          // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+30], acc88          // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+36], acc92          // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+40], acc81          // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+44], acc85          // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+48], acc89          // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+52], acc93          // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+56], acc82          // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+60], acc86          // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+64], acc90          // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+68], acc94          // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+72], acc83          // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+76], acc87          // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+80], acc91          // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+84], acc95          // copy acc to vreg[95]

/* rC *= alpha batchElements=[(5, 0, 0, 0), (5, 0, 0, 1), (5, 0, 0, 2), (5, 0, 0, 3), (5, 0, 1, 0), (5, 0, 1, 1), (5, 0, 1, 2), (5, 0, 1, 3), (5, 0, 2, 0), (5, 0, 2, 1), (5, 0, 2, 2), (5, 0, 2, 3), (5, 0, 3, 0), (5, 0, 3, 1), (5, 0, 3, 2), (5, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+18], s[sgprAlpha], v[vgprValuC+18] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+18], v17, v[vgprValuC+18]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+18]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v18, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+18], v[vgprValuC+18] // check Nan
v_bfe_u32 v9, v[vgprValuC+18], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+18], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+18], v9, v11, s[60:61]
v_lshrrev_b32 v18, 16, v[vgprValuC+18]             // convert C to bf16
buffer_store_short v18, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+24], v23, v[vgprValuC+24]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+24]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v24, 16, v[vgprValuC+24]             // convert C to bf16
buffer_store_short v24, v19, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+30], v29, v[vgprValuC+30]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+30]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v30, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v30, 16, v[vgprValuC+30]             // convert C to bf16
buffer_store_short v30, v25, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+36], v35, v[vgprValuC+36]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+36]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v36, 16, v[vgprValuC+36]             // convert C to bf16
buffer_store_short v36, v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v17, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v37, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+44], v23, v[vgprValuC+44]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+44]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v44, 16, v[vgprValuC+44]             // convert C to bf16
buffer_store_short v44, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+48], v29, v[vgprValuC+48]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+48]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v48, 16, v[vgprValuC+48]             // convert C to bf16
buffer_store_short v48, v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+52], v35, v[vgprValuC+52]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+52]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v52, 16, v[vgprValuC+52]             // convert C to bf16
buffer_store_short v52, v49, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+56], v17, v[vgprValuC+56]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+56]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v56, 16, v[vgprValuC+56]             // convert C to bf16
buffer_store_short v56, v53, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v23, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v57, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+64], v29, v[vgprValuC+64]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+64]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v64, 16, v[vgprValuC+64]             // convert C to bf16
buffer_store_short v64, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+68], v35, v[vgprValuC+68]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+68]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v68, 16, v[vgprValuC+68]             // convert C to bf16
buffer_store_short v68, v65, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+72], v17, v[vgprValuC+72]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+72]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v72, 16, v[vgprValuC+72]             // convert C to bf16
buffer_store_short v72, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+76], v23, v[vgprValuC+76]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+76]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v76, 16, v[vgprValuC+76]             // convert C to bf16
buffer_store_short v76, v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v29, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+84], v35, v[vgprValuC+84]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+84]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v84, 16, v[vgprValuC+84]             // convert C to bf16
buffer_store_short v84, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #6 (d1,d0,vc1,vc0) = */
/*    (6,0,0,0:vw1); (6,0,0,1:vw1); (6,0,0,2:vw1); (6,0,0,3:vw1); (6,0,1,0:vw1); (6,0,1,1:vw1); (6,0,1,2:vw1); (6,0,1,3:vw1); (6,0,2,0:vw1); (6,0,2,1:vw1); (6,0,2,2:vw1); (6,0,2,3:vw1); (6,0,3,0:vw1); (6,0,3,1:vw1); (6,0,3,2:vw1); (6,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v85, BufferOOB
/* (d1,vc1,d0,vc0)=(6,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v85, v14, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v16, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v17, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v85, v13, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v20, v4, s60
v_lshlrev_b32 v20, 0x2, v20                        // Bias address scaled by BPE
v_cndmask_b32 v20, v85, v20, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v22, v20 offset:0                      // load bias
v_lshlrev_b32 v21, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v23, v21, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v19, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v19, v85, v19, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v26, v4, s60
v_lshlrev_b32 v26, 0x2, v26                        // Bias address scaled by BPE
v_cndmask_b32 v26, v85, v26, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v28, v26 offset:0                      // load bias
v_lshlrev_b32 v27, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v29, v27, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v25, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v25, v85, v25, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s60
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v85, v32, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v34, v32 offset:0                      // load bias
v_lshlrev_b32 v33, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v35, v33, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v85, v31, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v38, v0, s60
v_lshlrev_b32 v38, 0x2, v38                        // Bias address scaled by BPE
v_cndmask_b32 v38, v85, v38, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v39, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v37, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v37, v85, v37, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v85, v42, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v85, v41, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v85, v46, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v85, v45, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v50, v4, s60
v_lshlrev_b32 v50, 0x2, v50                        // Bias address scaled by BPE
v_cndmask_b32 v50, v85, v50, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v51, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v49, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v49, v85, v49, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v85, v54, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v53, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v53, v85, v53, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v58, v4, s60
v_lshlrev_b32 v58, 0x2, v58                        // Bias address scaled by BPE
v_cndmask_b32 v58, v85, v58, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v59, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v57, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v57, v85, v57, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v4, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v85, v62, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v85, v61, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v66, v4, s60
v_lshlrev_b32 v66, 0x2, v66                        // Bias address scaled by BPE
v_cndmask_b32 v66, v85, v66, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v67, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v65, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v65, v85, v65, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v0, s60
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_cndmask_b32 v70, v85, v70, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v71, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v85, v69, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v4, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v85, v74, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v85, v73, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s60
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_cndmask_b32 v78, v85, v78, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v79, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v85, v77, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v4, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v85, v82, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v85, v81, s[64:65]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+18], acc96          // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+24], acc100         // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+30], acc104         // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+36], acc108         // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+40], acc97          // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+44], acc101         // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+48], acc105         // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+52], acc109         // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+56], acc98          // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+60], acc102         // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+64], acc106         // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+68], acc110         // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+72], acc99          // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+76], acc103         // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+80], acc107         // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+84], acc111         // copy acc to vreg[111]

/* rC *= alpha batchElements=[(6, 0, 0, 0), (6, 0, 0, 1), (6, 0, 0, 2), (6, 0, 0, 3), (6, 0, 1, 0), (6, 0, 1, 1), (6, 0, 1, 2), (6, 0, 1, 3), (6, 0, 2, 0), (6, 0, 2, 1), (6, 0, 2, 2), (6, 0, 2, 3), (6, 0, 3, 0), (6, 0, 3, 1), (6, 0, 3, 2), (6, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+18], s[sgprAlpha], v[vgprValuC+18] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+18], v17, v[vgprValuC+18]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+18]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v18, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+18], v[vgprValuC+18] // check Nan
v_bfe_u32 v9, v[vgprValuC+18], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+18], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+18], v9, v11, s[60:61]
v_lshrrev_b32 v18, 16, v[vgprValuC+18]             // convert C to bf16
buffer_store_short v18, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+24], v23, v[vgprValuC+24]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+24]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v24, 16, v[vgprValuC+24]             // convert C to bf16
buffer_store_short v24, v19, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+30], v29, v[vgprValuC+30]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+30]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v30, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v30, 16, v[vgprValuC+30]             // convert C to bf16
buffer_store_short v30, v25, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+36], v35, v[vgprValuC+36]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+36]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v36, 16, v[vgprValuC+36]             // convert C to bf16
buffer_store_short v36, v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v17, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v37, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+44], v23, v[vgprValuC+44]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+44]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v44, 16, v[vgprValuC+44]             // convert C to bf16
buffer_store_short v44, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+48], v29, v[vgprValuC+48]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+48]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v48, 16, v[vgprValuC+48]             // convert C to bf16
buffer_store_short v48, v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+52], v35, v[vgprValuC+52]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+52]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v52, 16, v[vgprValuC+52]             // convert C to bf16
buffer_store_short v52, v49, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+56], v17, v[vgprValuC+56]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+56]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v56, 16, v[vgprValuC+56]             // convert C to bf16
buffer_store_short v56, v53, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v23, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v57, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+64], v29, v[vgprValuC+64]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+64]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v64, 16, v[vgprValuC+64]             // convert C to bf16
buffer_store_short v64, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+68], v35, v[vgprValuC+68]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+68]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v68, 16, v[vgprValuC+68]             // convert C to bf16
buffer_store_short v68, v65, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+72], v17, v[vgprValuC+72]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+72]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v72, 16, v[vgprValuC+72]             // convert C to bf16
buffer_store_short v72, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+76], v23, v[vgprValuC+76]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+76]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v76, 16, v[vgprValuC+76]             // convert C to bf16
buffer_store_short v76, v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v29, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+84], v35, v[vgprValuC+84]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+84]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v84, 16, v[vgprValuC+84]             // convert C to bf16
buffer_store_short v84, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #7 (d1,d0,vc1,vc0) = */
/*    (7,0,0,0:vw1); (7,0,0,1:vw1); (7,0,0,2:vw1); (7,0,0,3:vw1); (7,0,1,0:vw1); (7,0,1,1:vw1); (7,0,1,2:vw1); (7,0,1,3:vw1); (7,0,2,0:vw1); (7,0,2,1:vw1); (7,0,2,2:vw1); (7,0,2,3:vw1); (7,0,3,0:vw1); (7,0,3,1:vw1); (7,0,3,2:vw1); (7,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v85, BufferOOB
/* (d1,vc1,d0,vc0)=(7,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v85, v14, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v16, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v17, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v85, v13, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v20, v4, s60
v_lshlrev_b32 v20, 0x2, v20                        // Bias address scaled by BPE
v_cndmask_b32 v20, v85, v20, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v22, v20 offset:0                      // load bias
v_lshlrev_b32 v21, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v23, v21, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v19, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v19, v85, v19, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v26, v4, s60
v_lshlrev_b32 v26, 0x2, v26                        // Bias address scaled by BPE
v_cndmask_b32 v26, v85, v26, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v28, v26 offset:0                      // load bias
v_lshlrev_b32 v27, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v29, v27, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v25, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v25, v85, v25, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s60
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v85, v32, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v34, v32 offset:0                      // load bias
v_lshlrev_b32 v33, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v35, v33, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v85, v31, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v38, v0, s60
v_lshlrev_b32 v38, 0x2, v38                        // Bias address scaled by BPE
v_cndmask_b32 v38, v85, v38, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v39, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v37, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v37, v85, v37, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v85, v42, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v85, v41, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v85, v46, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v85, v45, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v50, v4, s60
v_lshlrev_b32 v50, 0x2, v50                        // Bias address scaled by BPE
v_cndmask_b32 v50, v85, v50, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v51, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v49, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v49, v85, v49, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v85, v54, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v53, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v53, v85, v53, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v58, v4, s60
v_lshlrev_b32 v58, 0x2, v58                        // Bias address scaled by BPE
v_cndmask_b32 v58, v85, v58, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v59, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v57, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v57, v85, v57, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v4, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v85, v62, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v85, v61, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v66, v4, s60
v_lshlrev_b32 v66, 0x2, v66                        // Bias address scaled by BPE
v_cndmask_b32 v66, v85, v66, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v67, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v65, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v65, v85, v65, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v0, s60
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_cndmask_b32 v70, v85, v70, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v71, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v85, v69, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v4, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v85, v74, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v85, v73, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s60
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_cndmask_b32 v78, v85, v78, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v79, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v85, v77, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v4, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v85, v82, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v85, v81, s[64:65]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+18], acc112         // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+24], acc116         // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+30], acc120         // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+36], acc124         // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+40], acc113         // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+44], acc117         // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+48], acc121         // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+52], acc125         // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+56], acc114         // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+60], acc118         // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+64], acc122         // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+68], acc126         // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+72], acc115         // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+76], acc119         // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+80], acc123         // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+84], acc127         // copy acc to vreg[127]

/* rC *= alpha batchElements=[(7, 0, 0, 0), (7, 0, 0, 1), (7, 0, 0, 2), (7, 0, 0, 3), (7, 0, 1, 0), (7, 0, 1, 1), (7, 0, 1, 2), (7, 0, 1, 3), (7, 0, 2, 0), (7, 0, 2, 1), (7, 0, 2, 2), (7, 0, 2, 3), (7, 0, 3, 0), (7, 0, 3, 1), (7, 0, 3, 2), (7, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+18], s[sgprAlpha], v[vgprValuC+18] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+18], v17, v[vgprValuC+18]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+18]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v18, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+18], v[vgprValuC+18] // check Nan
v_bfe_u32 v9, v[vgprValuC+18], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+18], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+18], v9, v11, s[60:61]
v_lshrrev_b32 v18, 16, v[vgprValuC+18]             // convert C to bf16
buffer_store_short v18, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+24], v23, v[vgprValuC+24]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+24]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v24, 16, v[vgprValuC+24]             // convert C to bf16
buffer_store_short v24, v19, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+30], v29, v[vgprValuC+30]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+30]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v30, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v30, 16, v[vgprValuC+30]             // convert C to bf16
buffer_store_short v30, v25, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+36], v35, v[vgprValuC+36]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+36]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v36, 16, v[vgprValuC+36]             // convert C to bf16
buffer_store_short v36, v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v17, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v37, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+44], v23, v[vgprValuC+44]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+44]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v44, 16, v[vgprValuC+44]             // convert C to bf16
buffer_store_short v44, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+48], v29, v[vgprValuC+48]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+48]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v48, 16, v[vgprValuC+48]             // convert C to bf16
buffer_store_short v48, v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+52], v35, v[vgprValuC+52]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+52]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v52, 16, v[vgprValuC+52]             // convert C to bf16
buffer_store_short v52, v49, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+56], v17, v[vgprValuC+56]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+56]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v56, 16, v[vgprValuC+56]             // convert C to bf16
buffer_store_short v56, v53, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v23, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v57, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+64], v29, v[vgprValuC+64]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+64]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v64, 16, v[vgprValuC+64]             // convert C to bf16
buffer_store_short v64, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+68], v35, v[vgprValuC+68]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+68]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v68, 16, v[vgprValuC+68]             // convert C to bf16
buffer_store_short v68, v65, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+72], v17, v[vgprValuC+72]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+72]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v72, 16, v[vgprValuC+72]             // convert C to bf16
buffer_store_short v72, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+76], v23, v[vgprValuC+76]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+76]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v76, 16, v[vgprValuC+76]             // convert C to bf16
buffer_store_short v76, v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v29, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+84], v35, v[vgprValuC+84]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+84]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v84, 16, v[vgprValuC+84]             // convert C to bf16
buffer_store_short v84, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #8 (d1,d0,vc1,vc0) = */
/*    (8,0,0,0:vw1); (8,0,0,1:vw1); (8,0,0,2:vw1); (8,0,0,3:vw1); (8,0,1,0:vw1); (8,0,1,1:vw1); (8,0,1,2:vw1); (8,0,1,3:vw1); (8,0,2,0:vw1); (8,0,2,1:vw1); (8,0,2,2:vw1); (8,0,2,3:vw1); (8,0,3,0:vw1); (8,0,3,1:vw1); (8,0,3,2:vw1); (8,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v85, BufferOOB
/* (d1,vc1,d0,vc0)=(8,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v85, v14, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v16, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v17, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v85, v13, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v20, v4, s60
v_lshlrev_b32 v20, 0x2, v20                        // Bias address scaled by BPE
v_cndmask_b32 v20, v85, v20, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v22, v20 offset:0                      // load bias
v_lshlrev_b32 v21, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v23, v21, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v19, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v19, v85, v19, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v26, v4, s60
v_lshlrev_b32 v26, 0x2, v26                        // Bias address scaled by BPE
v_cndmask_b32 v26, v85, v26, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v28, v26 offset:0                      // load bias
v_lshlrev_b32 v27, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v29, v27, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v25, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v25, v85, v25, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s60
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v85, v32, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v34, v32 offset:0                      // load bias
v_lshlrev_b32 v33, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v35, v33, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v85, v31, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v38, v0, s60
v_lshlrev_b32 v38, 0x2, v38                        // Bias address scaled by BPE
v_cndmask_b32 v38, v85, v38, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v39, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v37, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v37, v85, v37, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v85, v42, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v85, v41, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v85, v46, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v85, v45, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v50, v4, s60
v_lshlrev_b32 v50, 0x2, v50                        // Bias address scaled by BPE
v_cndmask_b32 v50, v85, v50, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v51, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v49, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v49, v85, v49, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v85, v54, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v53, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v53, v85, v53, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v58, v4, s60
v_lshlrev_b32 v58, 0x2, v58                        // Bias address scaled by BPE
v_cndmask_b32 v58, v85, v58, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v59, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v57, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v57, v85, v57, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v4, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v85, v62, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v85, v61, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v66, v4, s60
v_lshlrev_b32 v66, 0x2, v66                        // Bias address scaled by BPE
v_cndmask_b32 v66, v85, v66, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v67, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v65, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v65, v85, v65, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v0, s60
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_cndmask_b32 v70, v85, v70, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v71, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v85, v69, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v4, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v85, v74, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v85, v73, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s60
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_cndmask_b32 v78, v85, v78, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v79, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v85, v77, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v4, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v85, v82, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v85, v81, s[64:65]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+18], acc128         // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+24], acc132         // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+30], acc136         // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+36], acc140         // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+40], acc129         // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+44], acc133         // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+48], acc137         // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+52], acc141         // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+56], acc130         // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+60], acc134         // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+64], acc138         // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+68], acc142         // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+72], acc131         // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+76], acc135         // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+80], acc139         // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+84], acc143         // copy acc to vreg[143]

/* rC *= alpha batchElements=[(8, 0, 0, 0), (8, 0, 0, 1), (8, 0, 0, 2), (8, 0, 0, 3), (8, 0, 1, 0), (8, 0, 1, 1), (8, 0, 1, 2), (8, 0, 1, 3), (8, 0, 2, 0), (8, 0, 2, 1), (8, 0, 2, 2), (8, 0, 2, 3), (8, 0, 3, 0), (8, 0, 3, 1), (8, 0, 3, 2), (8, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+18], s[sgprAlpha], v[vgprValuC+18] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+18], v17, v[vgprValuC+18]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+18]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v18, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+18], v[vgprValuC+18] // check Nan
v_bfe_u32 v9, v[vgprValuC+18], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+18], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+18], v9, v11, s[60:61]
v_lshrrev_b32 v18, 16, v[vgprValuC+18]             // convert C to bf16
buffer_store_short v18, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+24], v23, v[vgprValuC+24]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+24]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v24, 16, v[vgprValuC+24]             // convert C to bf16
buffer_store_short v24, v19, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+30], v29, v[vgprValuC+30]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+30]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v30, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v30, 16, v[vgprValuC+30]             // convert C to bf16
buffer_store_short v30, v25, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+36], v35, v[vgprValuC+36]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+36]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v36, 16, v[vgprValuC+36]             // convert C to bf16
buffer_store_short v36, v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v17, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v37, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+44], v23, v[vgprValuC+44]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+44]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v44, 16, v[vgprValuC+44]             // convert C to bf16
buffer_store_short v44, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+48], v29, v[vgprValuC+48]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+48]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v48, 16, v[vgprValuC+48]             // convert C to bf16
buffer_store_short v48, v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+52], v35, v[vgprValuC+52]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+52]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v52, 16, v[vgprValuC+52]             // convert C to bf16
buffer_store_short v52, v49, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+56], v17, v[vgprValuC+56]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+56]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v56, 16, v[vgprValuC+56]             // convert C to bf16
buffer_store_short v56, v53, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v23, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v57, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+64], v29, v[vgprValuC+64]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+64]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v64, 16, v[vgprValuC+64]             // convert C to bf16
buffer_store_short v64, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+68], v35, v[vgprValuC+68]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+68]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v68, 16, v[vgprValuC+68]             // convert C to bf16
buffer_store_short v68, v65, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+72], v17, v[vgprValuC+72]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+72]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v72, 16, v[vgprValuC+72]             // convert C to bf16
buffer_store_short v72, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+76], v23, v[vgprValuC+76]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+76]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v76, 16, v[vgprValuC+76]             // convert C to bf16
buffer_store_short v76, v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v29, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+84], v35, v[vgprValuC+84]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+84]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v84, 16, v[vgprValuC+84]             // convert C to bf16
buffer_store_short v84, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #9 (d1,d0,vc1,vc0) = */
/*    (9,0,0,0:vw1); (9,0,0,1:vw1); (9,0,0,2:vw1); (9,0,0,3:vw1); (9,0,1,0:vw1); (9,0,1,1:vw1); (9,0,1,2:vw1); (9,0,1,3:vw1); (9,0,2,0:vw1); (9,0,2,1:vw1); (9,0,2,2:vw1); (9,0,2,3:vw1); (9,0,3,0:vw1); (9,0,3,1:vw1); (9,0,3,2:vw1); (9,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v85, BufferOOB
/* (d1,vc1,d0,vc0)=(9,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v85, v14, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v16, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v17, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v85, v13, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v20, v4, s60
v_lshlrev_b32 v20, 0x2, v20                        // Bias address scaled by BPE
v_cndmask_b32 v20, v85, v20, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v22, v20 offset:0                      // load bias
v_lshlrev_b32 v21, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v23, v21, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v19, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v19, v85, v19, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v26, v4, s60
v_lshlrev_b32 v26, 0x2, v26                        // Bias address scaled by BPE
v_cndmask_b32 v26, v85, v26, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v28, v26 offset:0                      // load bias
v_lshlrev_b32 v27, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v29, v27, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v25, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v25, v85, v25, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s60
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v85, v32, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v34, v32 offset:0                      // load bias
v_lshlrev_b32 v33, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v35, v33, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v85, v31, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v38, v0, s60
v_lshlrev_b32 v38, 0x2, v38                        // Bias address scaled by BPE
v_cndmask_b32 v38, v85, v38, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v39, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v37, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v37, v85, v37, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v85, v42, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v85, v41, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v85, v46, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v85, v45, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v50, v4, s60
v_lshlrev_b32 v50, 0x2, v50                        // Bias address scaled by BPE
v_cndmask_b32 v50, v85, v50, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v51, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v49, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v49, v85, v49, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v85, v54, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v53, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v53, v85, v53, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v58, v4, s60
v_lshlrev_b32 v58, 0x2, v58                        // Bias address scaled by BPE
v_cndmask_b32 v58, v85, v58, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v59, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v57, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v57, v85, v57, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v4, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v85, v62, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v85, v61, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v66, v4, s60
v_lshlrev_b32 v66, 0x2, v66                        // Bias address scaled by BPE
v_cndmask_b32 v66, v85, v66, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v67, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v65, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v65, v85, v65, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v0, s60
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_cndmask_b32 v70, v85, v70, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v71, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v85, v69, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v4, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v85, v74, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v85, v73, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s60
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_cndmask_b32 v78, v85, v78, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v79, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v85, v77, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v4, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v85, v82, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v85, v81, s[64:65]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+18], acc144         // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+24], acc148         // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+30], acc152         // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+36], acc156         // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+40], acc145         // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+44], acc149         // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+48], acc153         // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+52], acc157         // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+56], acc146         // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+60], acc150         // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+64], acc154         // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+68], acc158         // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+72], acc147         // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+76], acc151         // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+80], acc155         // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+84], acc159         // copy acc to vreg[159]

/* rC *= alpha batchElements=[(9, 0, 0, 0), (9, 0, 0, 1), (9, 0, 0, 2), (9, 0, 0, 3), (9, 0, 1, 0), (9, 0, 1, 1), (9, 0, 1, 2), (9, 0, 1, 3), (9, 0, 2, 0), (9, 0, 2, 1), (9, 0, 2, 2), (9, 0, 2, 3), (9, 0, 3, 0), (9, 0, 3, 1), (9, 0, 3, 2), (9, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+18], s[sgprAlpha], v[vgprValuC+18] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+18], v17, v[vgprValuC+18]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+18]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v18, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+18], v[vgprValuC+18] // check Nan
v_bfe_u32 v9, v[vgprValuC+18], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+18], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+18], v9, v11, s[60:61]
v_lshrrev_b32 v18, 16, v[vgprValuC+18]             // convert C to bf16
buffer_store_short v18, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+24], v23, v[vgprValuC+24]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+24]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v24, 16, v[vgprValuC+24]             // convert C to bf16
buffer_store_short v24, v19, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+30], v29, v[vgprValuC+30]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+30]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v30, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v30, 16, v[vgprValuC+30]             // convert C to bf16
buffer_store_short v30, v25, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+36], v35, v[vgprValuC+36]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+36]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v36, 16, v[vgprValuC+36]             // convert C to bf16
buffer_store_short v36, v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v17, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v37, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+44], v23, v[vgprValuC+44]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+44]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v44, 16, v[vgprValuC+44]             // convert C to bf16
buffer_store_short v44, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+48], v29, v[vgprValuC+48]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+48]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v48, 16, v[vgprValuC+48]             // convert C to bf16
buffer_store_short v48, v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+52], v35, v[vgprValuC+52]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+52]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v52, 16, v[vgprValuC+52]             // convert C to bf16
buffer_store_short v52, v49, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+56], v17, v[vgprValuC+56]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+56]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v56, 16, v[vgprValuC+56]             // convert C to bf16
buffer_store_short v56, v53, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v23, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v57, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+64], v29, v[vgprValuC+64]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+64]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v64, 16, v[vgprValuC+64]             // convert C to bf16
buffer_store_short v64, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+68], v35, v[vgprValuC+68]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+68]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v68, 16, v[vgprValuC+68]             // convert C to bf16
buffer_store_short v68, v65, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+72], v17, v[vgprValuC+72]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+72]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v72, 16, v[vgprValuC+72]             // convert C to bf16
buffer_store_short v72, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+76], v23, v[vgprValuC+76]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+76]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v76, 16, v[vgprValuC+76]             // convert C to bf16
buffer_store_short v76, v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v29, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+84], v35, v[vgprValuC+84]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+84]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v84, 16, v[vgprValuC+84]             // convert C to bf16
buffer_store_short v84, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #10 (d1,d0,vc1,vc0) = */
/*    (10,0,0,0:vw1); (10,0,0,1:vw1); (10,0,0,2:vw1); (10,0,0,3:vw1); (10,0,1,0:vw1); (10,0,1,1:vw1); (10,0,1,2:vw1); (10,0,1,3:vw1); (10,0,2,0:vw1); (10,0,2,1:vw1); (10,0,2,2:vw1); (10,0,2,3:vw1); (10,0,3,0:vw1); (10,0,3,1:vw1); (10,0,3,2:vw1); (10,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v85, BufferOOB
/* (d1,vc1,d0,vc0)=(10,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v85, v14, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v16, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v17, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v85, v13, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v20, v4, s60
v_lshlrev_b32 v20, 0x2, v20                        // Bias address scaled by BPE
v_cndmask_b32 v20, v85, v20, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v22, v20 offset:0                      // load bias
v_lshlrev_b32 v21, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v23, v21, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v19, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v19, v85, v19, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v26, v4, s60
v_lshlrev_b32 v26, 0x2, v26                        // Bias address scaled by BPE
v_cndmask_b32 v26, v85, v26, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v28, v26 offset:0                      // load bias
v_lshlrev_b32 v27, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v29, v27, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v25, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v25, v85, v25, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s60
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v85, v32, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v34, v32 offset:0                      // load bias
v_lshlrev_b32 v33, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v35, v33, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v85, v31, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v38, v0, s60
v_lshlrev_b32 v38, 0x2, v38                        // Bias address scaled by BPE
v_cndmask_b32 v38, v85, v38, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v39, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v37, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v37, v85, v37, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v85, v42, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v85, v41, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v85, v46, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v85, v45, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v50, v4, s60
v_lshlrev_b32 v50, 0x2, v50                        // Bias address scaled by BPE
v_cndmask_b32 v50, v85, v50, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v51, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v49, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v49, v85, v49, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v85, v54, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v53, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v53, v85, v53, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v58, v4, s60
v_lshlrev_b32 v58, 0x2, v58                        // Bias address scaled by BPE
v_cndmask_b32 v58, v85, v58, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v59, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v57, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v57, v85, v57, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v4, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v85, v62, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v85, v61, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v66, v4, s60
v_lshlrev_b32 v66, 0x2, v66                        // Bias address scaled by BPE
v_cndmask_b32 v66, v85, v66, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v67, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v65, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v65, v85, v65, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v0, s60
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_cndmask_b32 v70, v85, v70, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v71, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v85, v69, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v4, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v85, v74, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v85, v73, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s60
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_cndmask_b32 v78, v85, v78, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v79, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v85, v77, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v4, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v85, v82, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v85, v81, s[64:65]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+18], acc160         // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+24], acc164         // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+30], acc168         // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+36], acc172         // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+40], acc161         // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+44], acc165         // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+48], acc169         // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+52], acc173         // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+56], acc162         // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+60], acc166         // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+64], acc170         // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+68], acc174         // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+72], acc163         // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+76], acc167         // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+80], acc171         // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+84], acc175         // copy acc to vreg[175]

/* rC *= alpha batchElements=[(10, 0, 0, 0), (10, 0, 0, 1), (10, 0, 0, 2), (10, 0, 0, 3), (10, 0, 1, 0), (10, 0, 1, 1), (10, 0, 1, 2), (10, 0, 1, 3), (10, 0, 2, 0), (10, 0, 2, 1), (10, 0, 2, 2), (10, 0, 2, 3), (10, 0, 3, 0), (10, 0, 3, 1), (10, 0, 3, 2), (10, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+18], s[sgprAlpha], v[vgprValuC+18] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+18], v17, v[vgprValuC+18]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+18]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v18, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+18], v[vgprValuC+18] // check Nan
v_bfe_u32 v9, v[vgprValuC+18], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+18], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+18], v9, v11, s[60:61]
v_lshrrev_b32 v18, 16, v[vgprValuC+18]             // convert C to bf16
buffer_store_short v18, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+24], v23, v[vgprValuC+24]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+24]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v24, 16, v[vgprValuC+24]             // convert C to bf16
buffer_store_short v24, v19, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+30], v29, v[vgprValuC+30]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+30]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v30, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v30, 16, v[vgprValuC+30]             // convert C to bf16
buffer_store_short v30, v25, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+36], v35, v[vgprValuC+36]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+36]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v36, 16, v[vgprValuC+36]             // convert C to bf16
buffer_store_short v36, v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v17, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v37, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+44], v23, v[vgprValuC+44]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+44]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v44, 16, v[vgprValuC+44]             // convert C to bf16
buffer_store_short v44, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+48], v29, v[vgprValuC+48]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+48]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v48, 16, v[vgprValuC+48]             // convert C to bf16
buffer_store_short v48, v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+52], v35, v[vgprValuC+52]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+52]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v52, 16, v[vgprValuC+52]             // convert C to bf16
buffer_store_short v52, v49, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+56], v17, v[vgprValuC+56]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+56]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v56, 16, v[vgprValuC+56]             // convert C to bf16
buffer_store_short v56, v53, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v23, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v57, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+64], v29, v[vgprValuC+64]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+64]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v64, 16, v[vgprValuC+64]             // convert C to bf16
buffer_store_short v64, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+68], v35, v[vgprValuC+68]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+68]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v68, 16, v[vgprValuC+68]             // convert C to bf16
buffer_store_short v68, v65, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+72], v17, v[vgprValuC+72]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+72]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v72, 16, v[vgprValuC+72]             // convert C to bf16
buffer_store_short v72, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+76], v23, v[vgprValuC+76]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+76]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v76, 16, v[vgprValuC+76]             // convert C to bf16
buffer_store_short v76, v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v29, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+84], v35, v[vgprValuC+84]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+84]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v84, 16, v[vgprValuC+84]             // convert C to bf16
buffer_store_short v84, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #11 (d1,d0,vc1,vc0) = */
/*    (11,0,0,0:vw1); (11,0,0,1:vw1); (11,0,0,2:vw1); (11,0,0,3:vw1); (11,0,1,0:vw1); (11,0,1,1:vw1); (11,0,1,2:vw1); (11,0,1,3:vw1); (11,0,2,0:vw1); (11,0,2,1:vw1); (11,0,2,2:vw1); (11,0,2,3:vw1); (11,0,3,0:vw1); (11,0,3,1:vw1); (11,0,3,2:vw1); (11,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v85, BufferOOB
/* (d1,vc1,d0,vc0)=(11,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v85, v14, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v16, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v17, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v85, v13, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v20, v4, s60
v_lshlrev_b32 v20, 0x2, v20                        // Bias address scaled by BPE
v_cndmask_b32 v20, v85, v20, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v22, v20 offset:0                      // load bias
v_lshlrev_b32 v21, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v23, v21, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v19, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v19, v85, v19, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v26, v4, s60
v_lshlrev_b32 v26, 0x2, v26                        // Bias address scaled by BPE
v_cndmask_b32 v26, v85, v26, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v28, v26 offset:0                      // load bias
v_lshlrev_b32 v27, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v29, v27, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v25, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v25, v85, v25, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s60
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v85, v32, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v34, v32 offset:0                      // load bias
v_lshlrev_b32 v33, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v35, v33, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v85, v31, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v38, v0, s60
v_lshlrev_b32 v38, 0x2, v38                        // Bias address scaled by BPE
v_cndmask_b32 v38, v85, v38, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v39, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v37, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v37, v85, v37, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v85, v42, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v85, v41, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v85, v46, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v85, v45, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v50, v4, s60
v_lshlrev_b32 v50, 0x2, v50                        // Bias address scaled by BPE
v_cndmask_b32 v50, v85, v50, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v51, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v49, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v49, v85, v49, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v85, v54, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v53, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v53, v85, v53, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v58, v4, s60
v_lshlrev_b32 v58, 0x2, v58                        // Bias address scaled by BPE
v_cndmask_b32 v58, v85, v58, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v59, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v57, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v57, v85, v57, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v4, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v85, v62, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v85, v61, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v66, v4, s60
v_lshlrev_b32 v66, 0x2, v66                        // Bias address scaled by BPE
v_cndmask_b32 v66, v85, v66, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v67, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v65, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v65, v85, v65, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v0, s60
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_cndmask_b32 v70, v85, v70, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v71, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v85, v69, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v4, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v85, v74, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v85, v73, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s60
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_cndmask_b32 v78, v85, v78, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v79, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v85, v77, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v4, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v85, v82, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v85, v81, s[64:65]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+18], acc176         // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+24], acc180         // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+30], acc184         // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+36], acc188         // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+40], acc177         // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+44], acc181         // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+48], acc185         // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+52], acc189         // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+56], acc178         // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+60], acc182         // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+64], acc186         // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+68], acc190         // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+72], acc179         // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+76], acc183         // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+80], acc187         // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+84], acc191         // copy acc to vreg[191]

/* rC *= alpha batchElements=[(11, 0, 0, 0), (11, 0, 0, 1), (11, 0, 0, 2), (11, 0, 0, 3), (11, 0, 1, 0), (11, 0, 1, 1), (11, 0, 1, 2), (11, 0, 1, 3), (11, 0, 2, 0), (11, 0, 2, 1), (11, 0, 2, 2), (11, 0, 2, 3), (11, 0, 3, 0), (11, 0, 3, 1), (11, 0, 3, 2), (11, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+18], s[sgprAlpha], v[vgprValuC+18] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+18], v17, v[vgprValuC+18]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+18]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v18, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+18], v[vgprValuC+18] // check Nan
v_bfe_u32 v9, v[vgprValuC+18], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+18], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+18], v9, v11, s[60:61]
v_lshrrev_b32 v18, 16, v[vgprValuC+18]             // convert C to bf16
buffer_store_short v18, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+24], v23, v[vgprValuC+24]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+24]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v24, 16, v[vgprValuC+24]             // convert C to bf16
buffer_store_short v24, v19, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+30], v29, v[vgprValuC+30]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+30]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v30, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v30, 16, v[vgprValuC+30]             // convert C to bf16
buffer_store_short v30, v25, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+36], v35, v[vgprValuC+36]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+36]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v36, 16, v[vgprValuC+36]             // convert C to bf16
buffer_store_short v36, v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v17, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v37, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+44], v23, v[vgprValuC+44]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+44]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v44, 16, v[vgprValuC+44]             // convert C to bf16
buffer_store_short v44, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+48], v29, v[vgprValuC+48]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+48]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v48, 16, v[vgprValuC+48]             // convert C to bf16
buffer_store_short v48, v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+52], v35, v[vgprValuC+52]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+52]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v52, 16, v[vgprValuC+52]             // convert C to bf16
buffer_store_short v52, v49, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+56], v17, v[vgprValuC+56]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+56]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v56, 16, v[vgprValuC+56]             // convert C to bf16
buffer_store_short v56, v53, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v23, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v57, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+64], v29, v[vgprValuC+64]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+64]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v64, 16, v[vgprValuC+64]             // convert C to bf16
buffer_store_short v64, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+68], v35, v[vgprValuC+68]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+68]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v68, 16, v[vgprValuC+68]             // convert C to bf16
buffer_store_short v68, v65, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+72], v17, v[vgprValuC+72]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+72]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v72, 16, v[vgprValuC+72]             // convert C to bf16
buffer_store_short v72, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+76], v23, v[vgprValuC+76]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+76]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v76, 16, v[vgprValuC+76]             // convert C to bf16
buffer_store_short v76, v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v29, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+84], v35, v[vgprValuC+84]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+84]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v84, 16, v[vgprValuC+84]             // convert C to bf16
buffer_store_short v84, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #12 (d1,d0,vc1,vc0) = */
/*    (12,0,0,0:vw1); (12,0,0,1:vw1); (12,0,0,2:vw1); (12,0,0,3:vw1); (12,0,1,0:vw1); (12,0,1,1:vw1); (12,0,1,2:vw1); (12,0,1,3:vw1); (12,0,2,0:vw1); (12,0,2,1:vw1); (12,0,2,2:vw1); (12,0,2,3:vw1); (12,0,3,0:vw1); (12,0,3,1:vw1); (12,0,3,2:vw1); (12,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v85, BufferOOB
/* (d1,vc1,d0,vc0)=(12,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v85, v14, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v16, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v17, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v85, v13, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v20, v4, s60
v_lshlrev_b32 v20, 0x2, v20                        // Bias address scaled by BPE
v_cndmask_b32 v20, v85, v20, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v22, v20 offset:0                      // load bias
v_lshlrev_b32 v21, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v23, v21, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v19, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v19, v85, v19, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v26, v4, s60
v_lshlrev_b32 v26, 0x2, v26                        // Bias address scaled by BPE
v_cndmask_b32 v26, v85, v26, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v28, v26 offset:0                      // load bias
v_lshlrev_b32 v27, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v29, v27, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v25, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v25, v85, v25, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s60
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v85, v32, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v34, v32 offset:0                      // load bias
v_lshlrev_b32 v33, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v35, v33, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v85, v31, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v38, v0, s60
v_lshlrev_b32 v38, 0x2, v38                        // Bias address scaled by BPE
v_cndmask_b32 v38, v85, v38, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v39, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v37, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v37, v85, v37, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v85, v42, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v85, v41, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v85, v46, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v85, v45, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v50, v4, s60
v_lshlrev_b32 v50, 0x2, v50                        // Bias address scaled by BPE
v_cndmask_b32 v50, v85, v50, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v51, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v49, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v49, v85, v49, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v85, v54, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v53, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v53, v85, v53, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v58, v4, s60
v_lshlrev_b32 v58, 0x2, v58                        // Bias address scaled by BPE
v_cndmask_b32 v58, v85, v58, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v59, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v57, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v57, v85, v57, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v4, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v85, v62, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v85, v61, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v66, v4, s60
v_lshlrev_b32 v66, 0x2, v66                        // Bias address scaled by BPE
v_cndmask_b32 v66, v85, v66, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v67, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v65, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v65, v85, v65, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v0, s60
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_cndmask_b32 v70, v85, v70, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v71, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v85, v69, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v4, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v85, v74, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v85, v73, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s60
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_cndmask_b32 v78, v85, v78, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v79, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v85, v77, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v4, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v85, v82, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v85, v81, s[64:65]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+18], acc192         // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+24], acc196         // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+30], acc200         // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+36], acc204         // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+40], acc193         // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+44], acc197         // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+48], acc201         // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+52], acc205         // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+56], acc194         // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+60], acc198         // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+64], acc202         // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+68], acc206         // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+72], acc195         // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+76], acc199         // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+80], acc203         // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+84], acc207         // copy acc to vreg[207]

/* rC *= alpha batchElements=[(12, 0, 0, 0), (12, 0, 0, 1), (12, 0, 0, 2), (12, 0, 0, 3), (12, 0, 1, 0), (12, 0, 1, 1), (12, 0, 1, 2), (12, 0, 1, 3), (12, 0, 2, 0), (12, 0, 2, 1), (12, 0, 2, 2), (12, 0, 2, 3), (12, 0, 3, 0), (12, 0, 3, 1), (12, 0, 3, 2), (12, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+18], s[sgprAlpha], v[vgprValuC+18] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+18], v17, v[vgprValuC+18]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+18]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v18, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+18], v[vgprValuC+18] // check Nan
v_bfe_u32 v9, v[vgprValuC+18], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+18], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+18], v9, v11, s[60:61]
v_lshrrev_b32 v18, 16, v[vgprValuC+18]             // convert C to bf16
buffer_store_short v18, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+24], v23, v[vgprValuC+24]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+24]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v24, 16, v[vgprValuC+24]             // convert C to bf16
buffer_store_short v24, v19, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+30], v29, v[vgprValuC+30]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+30]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v30, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v30, 16, v[vgprValuC+30]             // convert C to bf16
buffer_store_short v30, v25, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+36], v35, v[vgprValuC+36]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+36]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v36, 16, v[vgprValuC+36]             // convert C to bf16
buffer_store_short v36, v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v17, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v37, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+44], v23, v[vgprValuC+44]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+44]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v44, 16, v[vgprValuC+44]             // convert C to bf16
buffer_store_short v44, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+48], v29, v[vgprValuC+48]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+48]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v48, 16, v[vgprValuC+48]             // convert C to bf16
buffer_store_short v48, v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+52], v35, v[vgprValuC+52]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+52]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v52, 16, v[vgprValuC+52]             // convert C to bf16
buffer_store_short v52, v49, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+56], v17, v[vgprValuC+56]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+56]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v56, 16, v[vgprValuC+56]             // convert C to bf16
buffer_store_short v56, v53, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v23, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v57, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+64], v29, v[vgprValuC+64]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+64]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v64, 16, v[vgprValuC+64]             // convert C to bf16
buffer_store_short v64, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+68], v35, v[vgprValuC+68]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+68]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v68, 16, v[vgprValuC+68]             // convert C to bf16
buffer_store_short v68, v65, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+72], v17, v[vgprValuC+72]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+72]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v72, 16, v[vgprValuC+72]             // convert C to bf16
buffer_store_short v72, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+76], v23, v[vgprValuC+76]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+76]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v76, 16, v[vgprValuC+76]             // convert C to bf16
buffer_store_short v76, v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v29, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+84], v35, v[vgprValuC+84]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+84]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v84, 16, v[vgprValuC+84]             // convert C to bf16
buffer_store_short v84, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Edge Batch #13 (d1,d0,vc1,vc0) = */
/*    (13,0,0,0:vw1); (13,0,0,1:vw1); (13,0,0,2:vw1); (13,0,0,3:vw1); (13,0,1,0:vw1); (13,0,1,1:vw1); (13,0,1,2:vw1); (13,0,1,3:vw1); (13,0,2,0:vw1); (13,0,2,1:vw1); (13,0,2,2:vw1); (13,0,2,3:vw1); (13,0,3,0:vw1); (13,0,3,1:vw1); (13,0,3,2:vw1); (13,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v85, BufferOOB
/* (d1,vc1,d0,vc0)=(13,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v85, v14, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v16, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v17, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v85, v13, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v20, v4, s60
v_lshlrev_b32 v20, 0x2, v20                        // Bias address scaled by BPE
v_cndmask_b32 v20, v85, v20, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v22, v20 offset:0                      // load bias
v_lshlrev_b32 v21, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v23, v21, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v19, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v19, v85, v19, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v26, v4, s60
v_lshlrev_b32 v26, 0x2, v26                        // Bias address scaled by BPE
v_cndmask_b32 v26, v85, v26, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v28, v26 offset:0                      // load bias
v_lshlrev_b32 v27, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v29, v27, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v25, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v25, v85, v25, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v32, v4, s60
v_lshlrev_b32 v32, 0x2, v32                        // Bias address scaled by BPE
v_cndmask_b32 v32, v85, v32, s[64:65]              // LDBias clip if OOB. offset
ds_read_b32 v34, v32 offset:0                      // load bias
v_lshlrev_b32 v33, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v35, v33, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v31, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v31, v85, v31, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v38, v0, s60
v_lshlrev_b32 v38, 0x2, v38                        // Bias address scaled by BPE
v_cndmask_b32 v38, v85, v38, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v39, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v37, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v37, v85, v37, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v4, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v85, v42, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v85, v41, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v46, v4, s60
v_lshlrev_b32 v46, 0x2, v46                        // Bias address scaled by BPE
v_cndmask_b32 v46, v85, v46, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v47, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v45, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v45, v85, v45, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v50, v4, s60
v_lshlrev_b32 v50, 0x2, v50                        // Bias address scaled by BPE
v_cndmask_b32 v50, v85, v50, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v51, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v49, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v49, v85, v49, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v85, v54, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v53, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v53, v85, v53, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v58, v4, s60
v_lshlrev_b32 v58, 0x2, v58                        // Bias address scaled by BPE
v_cndmask_b32 v58, v85, v58, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v59, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v57, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v57, v85, v57, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v4, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v85, v62, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v85, v61, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v66, v4, s60
v_lshlrev_b32 v66, 0x2, v66                        // Bias address scaled by BPE
v_cndmask_b32 v66, v85, v66, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v67, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v65, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v65, v85, v65, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v70, v0, s60
v_lshlrev_b32 v70, 0x2, v70                        // Bias address scaled by BPE
v_cndmask_b32 v70, v85, v70, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v71, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v85, v69, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v74, v4, s60
v_lshlrev_b32 v74, 0x2, v74                        // Bias address scaled by BPE
v_cndmask_b32 v74, v85, v74, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v75, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v73, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v73, v85, v73, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v78, v4, s60
v_lshlrev_b32 v78, 0x2, v78                        // Bias address scaled by BPE
v_cndmask_b32 v78, v85, v78, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v79, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v77, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v77, v85, v77, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v4, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v85, v82, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v85, v81, s[64:65]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+18], acc208         // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+24], acc212         // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+30], acc216         // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+36], acc220         // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+40], acc209         // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+44], acc213         // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+48], acc217         // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+52], acc221         // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+56], acc210         // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+60], acc214         // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+64], acc218         // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+68], acc222         // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+72], acc211         // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+76], acc215         // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+80], acc219         // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+84], acc223         // copy acc to vreg[223]

/* rC *= alpha batchElements=[(13, 0, 0, 0), (13, 0, 0, 1), (13, 0, 0, 2), (13, 0, 0, 3), (13, 0, 1, 0), (13, 0, 1, 1), (13, 0, 1, 2), (13, 0, 1, 3), (13, 0, 2, 0), (13, 0, 2, 1), (13, 0, 2, 2), (13, 0, 2, 3), (13, 0, 3, 0), (13, 0, 3, 1), (13, 0, 3, 2), (13, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+18], s[sgprAlpha], v[vgprValuC+18] // *= alpha
v_mul_f32 v[vgprValuC+24], s[sgprAlpha], v[vgprValuC+24] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+68], s[sgprAlpha], v[vgprValuC+68] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
s_waitcnt 0                                        // wait for ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+18], v17, v[vgprValuC+18]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+18]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v18, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+18], v[vgprValuC+18] // check Nan
v_bfe_u32 v9, v[vgprValuC+18], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+18], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+18], v9, v11, s[60:61]
v_lshrrev_b32 v18, 16, v[vgprValuC+18]             // convert C to bf16
buffer_store_short v18, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+24], v23, v[vgprValuC+24]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+24]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v24, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+24], v[vgprValuC+24] // check Nan
v_bfe_u32 v9, v[vgprValuC+24], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+24], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+24], v9, v11, s[60:61]
v_lshrrev_b32 v24, 16, v[vgprValuC+24]             // convert C to bf16
buffer_store_short v24, v19, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+30], v29, v[vgprValuC+30]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+30]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v30, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v30, 16, v[vgprValuC+30]             // convert C to bf16
buffer_store_short v30, v25, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+36], v35, v[vgprValuC+36]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+36]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v36, 16, v[vgprValuC+36]             // convert C to bf16
buffer_store_short v36, v31, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v17, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v37, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+44], v23, v[vgprValuC+44]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+44]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v44, 16, v[vgprValuC+44]             // convert C to bf16
buffer_store_short v44, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+48], v29, v[vgprValuC+48]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+48]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v48, 16, v[vgprValuC+48]             // convert C to bf16
buffer_store_short v48, v45, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+52], v35, v[vgprValuC+52]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+52]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v52, 16, v[vgprValuC+52]             // convert C to bf16
buffer_store_short v52, v49, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+56], v17, v[vgprValuC+56]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+56]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v56, 16, v[vgprValuC+56]             // convert C to bf16
buffer_store_short v56, v53, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v23, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v57, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+64], v29, v[vgprValuC+64]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+64]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v64, 16, v[vgprValuC+64]             // convert C to bf16
buffer_store_short v64, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+68], v35, v[vgprValuC+68]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+68]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v68, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+68], v[vgprValuC+68] // check Nan
v_bfe_u32 v9, v[vgprValuC+68], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+68], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+68], v9, v11, s[60:61]
v_lshrrev_b32 v68, 16, v[vgprValuC+68]             // convert C to bf16
buffer_store_short v68, v65, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v17, 1.0, v17, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+72], v17, v[vgprValuC+72]    // *= scaleAlphaVecVMul
v_add_f32 v4, v16, v[vgprValuC+72]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v72, 16, v[vgprValuC+72]             // convert C to bf16
buffer_store_short v72, v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v23, 1.0, v23, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+76], v23, v[vgprValuC+76]    // *= scaleAlphaVecVMul
v_add_f32 v4, v22, v[vgprValuC+76]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v76, 16, v[vgprValuC+76]             // convert C to bf16
buffer_store_short v76, v73, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v29, 1.0, v29, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v29, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_add_f32 v4, v28, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v77, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v35, 1.0, v35, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+84], v35, v[vgprValuC+84]    // *= scaleAlphaVecVMul
v_add_f32 v4, v34, v[vgprValuC+84]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v84, 16, v[vgprValuC+84]             // convert C to bf16
buffer_store_short v84, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_2                            // jump to end
label_GW_Beta_2:
s_and_b32 s60, 255, s[sgprSizeI]                   // s60 = s[sgprSizeI] % 256
s_add_u32 s61, -0x1, s[sgprNumWorkGroups0]
s_cmp_ge_u32 s[sgprWorkGroup0], s61                // wg0 >= nwg0-1 ?
s_cselect_b32 s60, s60, 0                          // set rMT0
s_cmpk_gt_u32 s60, 0x0                             // rMT0 > 0
s_cbranch_scc1 label_GW_B1_E1_M                    // jump if edges required
s_mov_b32 s63, 0x0                                 // STATIC_DIV: divisior=224
s_mul_i32 s62, 0x249, s[sgprSizeJ]                 // tmp1 = dividend * magic hi
s_lshl_b64 s[62:63], s[62:63], 0x10                // left shift 16 bits
s_mul_i32 s61, s[sgprSizeJ], 0x2493                // tmp0 = dividend * magic lo
s_add_u32 s62, s61, s62                            // add lo
s_addc_u32 s63, s63, 0x0                           // add hi
s_lshr_b64 s[62:63], s[62:63], 0x21                // tmp1 = (dividend * magic) << shift
s_mov_b32 s61, s62                                 // quotient
s_mul_i32 s62, s61, 0xe0                           // quotient*divisor
s_sub_u32 s60, s[sgprSizeJ], s62                   // rReg = dividend - quotient*divisor
s_add_u32 s61, -0x1, s[sgprNumWorkGroups1]
s_cmp_ge_u32 s[sgprWorkGroup1], s61                // wg1 >= nwg1-1
s_cselect_b32 s60, s60, 0                          // set rMT1
s_cmpk_gt_u32 s60, 0x0                             // rMT1 > 0
s_cbranch_scc1 label_GW_B1_E1_N                    // jump if edges required
label_GW_B1_E0:
s_cmpk_eq_u32 s[sgprActivationType], 1             // activationType == 1
s_cbranch_scc1 label_To_Activation_Abs_VW4_1_beta_1_edge_0 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 2             // activationType == 2
s_cbranch_scc1 label_To_Activation_Clippedrelu_VW4_1_beta_1_edge_0 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 3             // activationType == 3
s_cbranch_scc1 label_To_Activation_Gelu_VW4_1_beta_1_edge_0 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 4             // activationType == 4
s_cbranch_scc1 label_To_Activation_Leakyrelu_VW4_1_beta_1_edge_0 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 5             // activationType == 5
s_cbranch_scc1 label_To_Activation_Relu_VW4_1_beta_1_edge_0 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 6             // activationType == 6
s_cbranch_scc1 label_To_Activation_Sigmoid_VW4_1_beta_1_edge_0 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 7             // activationType == 7
s_cbranch_scc1 label_To_Activation_Tanh_VW4_1_beta_1_edge_0 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 9             // activationType == 9
s_cbranch_scc1 label_To_Activation_Geluscaling_VW4_1_beta_1_edge_0 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 10            // activationType == 10
s_cbranch_scc1 label_To_Activation_Silu_VW4_1_beta_1_edge_0 // Branch if true
label_To_Activation_None_VW4_1_beta_1_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_None_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_3
label_To_Activation_Abs_VW4_1_beta_1_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Abs_VW4, 0x4        // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_3
label_To_Activation_Clippedrelu_VW4_1_beta_1_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Clippedrelu_VW4, 0x4 // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_3
label_To_Activation_Gelu_VW4_1_beta_1_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Gelu_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_3
label_To_Activation_Leakyrelu_VW4_1_beta_1_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Leakyrelu_VW4, 0x4  // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_3
label_To_Activation_Relu_VW4_1_beta_1_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Relu_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_3
label_To_Activation_Sigmoid_VW4_1_beta_1_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Sigmoid_VW4, 0x4    // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_3
label_To_Activation_Tanh_VW4_1_beta_1_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Tanh_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_3
label_To_Activation_Geluscaling_VW4_1_beta_1_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Geluscaling_VW4, 0x4 // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_3
label_To_Activation_Silu_VW4_1_beta_1_edge_0:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Silu_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_3
label_ActivationSetPCAddrEnd_3:

/* edge=0, allocate 2 sgpr. perBatchTmpS=2 perBatchMaskS=0 perElementMaskS=0 elementsPerBatch=16 */
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 biasDim=0 */

/******************************************/
/* Global Write Beta Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4); (1,0,0,0:vw4); (1,0,1,0:vw4); (1,0,2,0:vw4); (1,0,3,0:vw4); (2,0,0,0:vw4); (2,0,1,0:vw4); (2,0,2,0:vw4); (2,0,3,0:vw4); (3,0,0,0:vw4); (3,0,1,0:vw4); (3,0,2,0:vw4); (3,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_add_lshl_u32 v14, v2, v0, 0x1                    // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
buffer_load_dwordx2 v[18:19], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s60
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[20:23], v15 offset:0                // load bias
v_lshlrev_b32 v16, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[24:27], v16, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[32:33], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[34:35], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[44:45], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
s_mul_i32 s60, s[sgprStrideC1J], 26                // scale StrideC *= numRows(13) * bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[46:47], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[56:57], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[58:59], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[68:69], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(2,0,0,0) */
s_mul_i32 s60, s[sgprStrideC1J], 26                // scale StrideC *= numRows(13) * bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[70:71], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(2,1,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[80:81], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(2,2,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[82:83], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(2,3,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[92:93], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(3,0,0,0) */
s_mul_i32 s60, s[sgprStrideC1J], 26                // scale StrideC *= numRows(13) * bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[94:95], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(3,1,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[104:105], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(3,2,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[106:107], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(3,3,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[116:117], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_add_lshl_u32 v13, v3, v0, 0x1                    // optSingleColVgpr scaleToBpe: sharedAddrVgpr <- cinRowPtr + coord0, scaled by BPE. BSHERE:coord0=0, coord0Vgpr=0
v_accvgpr_read_b32 v[vgprValuC+28], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+29], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+30], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+31], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+36], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+37], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+38], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+39], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+40], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+41], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+42], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+43], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+48], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+49], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+50], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+51], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+52], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+53], acc20          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+54], acc24          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+55], acc28          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+60], acc17          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+61], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+62], acc25          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+63], acc29          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+64], acc18          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+65], acc22          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+66], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+67], acc30          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+72], acc19          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+73], acc23          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+74], acc27          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+75], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+76], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+77], acc36          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+78], acc40          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+79], acc44          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+84], acc33          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+85], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+86], acc41          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+87], acc45          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+88], acc34          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+89], acc38          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+90], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+91], acc46          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+96], acc35          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+97], acc39          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+98], acc43          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+99], acc47          // copy acc to vreg[47]
v_accvgpr_read_b32 v[vgprValuC+100], acc48         // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+101], acc52         // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+102], acc56         // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+103], acc60         // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+108], acc49         // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+109], acc53         // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+110], acc57         // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+111], acc61         // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+112], acc50         // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+113], acc54         // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+114], acc58         // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+115], acc62         // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+120], acc51         // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+121], acc55         // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+122], acc59         // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+123], acc63         // copy acc to vreg[63]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0), (1, 0, 0, 0), (1, 0, 1, 0), (1, 0, 2, 0), (1, 0, 3, 0), (2, 0, 0, 0), (2, 0, 1, 0), (2, 0, 2, 0), (2, 0, 3, 0), (3, 0, 0, 0), (3, 0, 1, 0), (3, 0, 2, 0), (3, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+97], s[sgprAlpha], v[vgprValuC+97] // *= alpha
v_mul_f32 v[vgprValuC+98], s[sgprAlpha], v[vgprValuC+98] // *= alpha
v_mul_f32 v[vgprValuC+99], s[sgprAlpha], v[vgprValuC+99] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+112], s[sgprAlpha], v[vgprValuC+112] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+114], s[sgprAlpha], v[vgprValuC+114] // *= alpha
v_mul_f32 v[vgprValuC+115], s[sgprAlpha], v[vgprValuC+115] // *= alpha
v_mul_f32 v[vgprValuC+120], s[sgprAlpha], v[vgprValuC+120] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+122], s[sgprAlpha], v[vgprValuC+122] // *= alpha
v_mul_f32 v[vgprValuC+123], s[sgprAlpha], v[vgprValuC+123] // *= alpha

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16

s_waitcnt lgkmcnt(0), vmcnt(15)                    // vmcnt(15) = 17 - 1 (beta) - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v18                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+28], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v18, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+29], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v19                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+30], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v19, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+31], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(14) = 17 - 2 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v32                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+36], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v32, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+37], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v33                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+38], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v33, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+39], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(13) = 17 - 3 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[24:25], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[26:27], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v34                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v34, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+41], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v35                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+42], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v35, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+43], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_mov_b32 v41, v5
v_mov_b32 v42, v6
v_mov_b32 v43, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+40], 16, v[vgprValuC+40] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+41], v[vgprValuC+41] // check Nan
v_bfe_u32 v9, v[vgprValuC+41], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+41], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+41], v9, v11, s[60:61]
v_and_or_b32 v40, v[vgprValuC+41], v10, v[vgprValuC+40] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+42], v[vgprValuC+42] // check Nan
v_bfe_u32 v9, v[vgprValuC+42], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+42], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+42], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+42], 16, v[vgprValuC+42] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+43], v[vgprValuC+43] // check Nan
v_bfe_u32 v9, v[vgprValuC+43], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+43], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+43], v9, v11, s[60:61]
v_and_or_b32 v41, v[vgprValuC+43], v10, v[vgprValuC+42] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(12) = 17 - 4 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+48], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v44, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+49], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v45                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v45, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+51], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_mov_b32 v49, v5
v_mov_b32 v50, v6
v_mov_b32 v51, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+48], 16, v[vgprValuC+48] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+49], v[vgprValuC+49] // check Nan
v_bfe_u32 v9, v[vgprValuC+49], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+49], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+49], v9, v11, s[60:61]
v_and_or_b32 v48, v[vgprValuC+49], v10, v[vgprValuC+48] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+50], 16, v[vgprValuC+50] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+51], v[vgprValuC+51] // check Nan
v_bfe_u32 v9, v[vgprValuC+51], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+51], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+51], v9, v11, s[60:61]
v_and_or_b32 v49, v[vgprValuC+51], v10, v[vgprValuC+50] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(11) = 17 - 5 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[24:25], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[26:27], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v46                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+52], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v46, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+53], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v47                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+54], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v47, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_mov_b32 v53, v5
v_mov_b32 v54, v6
v_mov_b32 v55, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+52], 16, v[vgprValuC+52] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+53], v[vgprValuC+53] // check Nan
v_bfe_u32 v9, v[vgprValuC+53], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+53], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+53], v9, v11, s[60:61]
v_and_or_b32 v52, v[vgprValuC+53], v10, v[vgprValuC+52] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+54], v[vgprValuC+54] // check Nan
v_bfe_u32 v9, v[vgprValuC+54], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+54], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+54], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+54], 16, v[vgprValuC+54] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_and_or_b32 v53, v[vgprValuC+55], v10, v[vgprValuC+54] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[52:53], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(10) = 17 - 6 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[24:25], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[26:27], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v56                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v56, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+61], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v57                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+62], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v57, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+63], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_mov_b32 v61, v5
v_mov_b32 v62, v6
v_mov_b32 v63, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+60], 16, v[vgprValuC+60] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+61], v[vgprValuC+61] // check Nan
v_bfe_u32 v9, v[vgprValuC+61], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+61], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+61], v9, v11, s[60:61]
v_and_or_b32 v60, v[vgprValuC+61], v10, v[vgprValuC+60] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+62], v[vgprValuC+62] // check Nan
v_bfe_u32 v9, v[vgprValuC+62], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+62], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+62], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+62], 16, v[vgprValuC+62] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+63], v[vgprValuC+63] // check Nan
v_bfe_u32 v9, v[vgprValuC+63], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+63], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+63], v9, v11, s[60:61]
v_and_or_b32 v61, v[vgprValuC+63], v10, v[vgprValuC+62] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(9) = 17 - 7 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v58                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+64], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v58, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+66], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v59, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+67], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_mov_b32 v65, v5
v_mov_b32 v66, v6
v_mov_b32 v67, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+64], 16, v[vgprValuC+64] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_and_or_b32 v64, v[vgprValuC+65], v10, v[vgprValuC+64] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+66], v[vgprValuC+66] // check Nan
v_bfe_u32 v9, v[vgprValuC+66], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+66], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+66], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+66], 16, v[vgprValuC+66] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+67], v[vgprValuC+67] // check Nan
v_bfe_u32 v9, v[vgprValuC+67], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+67], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+67], v9, v11, s[60:61]
v_and_or_b32 v65, v[vgprValuC+67], v10, v[vgprValuC+66] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[64:65], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(8) = 17 - 8 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v68                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+72], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v68, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+73], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+74], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v69, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+74:vgprValuC+74+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_mov_b32 v73, v5
v_mov_b32 v74, v6
v_mov_b32 v75, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+72], 16, v[vgprValuC+72] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+73], v[vgprValuC+73] // check Nan
v_bfe_u32 v9, v[vgprValuC+73], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+73], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+73], v9, v11, s[60:61]
v_and_or_b32 v72, v[vgprValuC+73], v10, v[vgprValuC+72] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+74], v[vgprValuC+74] // check Nan
v_bfe_u32 v9, v[vgprValuC+74], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+74], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+74], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+74], 16, v[vgprValuC+74] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_and_or_b32 v73, v[vgprValuC+75], v10, v[vgprValuC+74] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[72:73], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(7) = 17 - 9 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[24:25], v[vgprValuC+76:vgprValuC+76+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[26:27], v[vgprValuC+78:vgprValuC+78+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v70                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+76], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v70, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+77], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v71                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+78], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v71, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+79], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+78:vgprValuC+78+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_mov_b32 v77, v5
v_mov_b32 v78, v6
v_mov_b32 v79, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+76], 16, v[vgprValuC+76] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+77], v[vgprValuC+77] // check Nan
v_bfe_u32 v9, v[vgprValuC+77], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+77], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+77], v9, v11, s[60:61]
v_and_or_b32 v76, v[vgprValuC+77], v10, v[vgprValuC+76] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+78], v[vgprValuC+78] // check Nan
v_bfe_u32 v9, v[vgprValuC+78], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+78], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+78], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+78], 16, v[vgprValuC+78] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+79], v[vgprValuC+79] // check Nan
v_bfe_u32 v9, v[vgprValuC+79], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+79], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+79], v9, v11, s[60:61]
v_and_or_b32 v77, v[vgprValuC+79], v10, v[vgprValuC+78] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(6) = 17 - 10 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[24:25], v[vgprValuC+84:vgprValuC+84+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[26:27], v[vgprValuC+86:vgprValuC+86+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v80                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+84], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v80, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v81                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+86], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v81, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+87], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+86:vgprValuC+86+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_mov_b32 v85, v5
v_mov_b32 v86, v6
v_mov_b32 v87, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+84], 16, v[vgprValuC+84] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_and_or_b32 v84, v[vgprValuC+85], v10, v[vgprValuC+84] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+86], v[vgprValuC+86] // check Nan
v_bfe_u32 v9, v[vgprValuC+86], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+86], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+86], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+86], 16, v[vgprValuC+86] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+87], v[vgprValuC+87] // check Nan
v_bfe_u32 v9, v[vgprValuC+87], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+87], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+87], v9, v11, s[60:61]
v_and_or_b32 v85, v[vgprValuC+87], v10, v[vgprValuC+86] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[84:85], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(5) = 17 - 11 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[24:25], v[vgprValuC+88:vgprValuC+88+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[26:27], v[vgprValuC+90:vgprValuC+90+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v82                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+88], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v82, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+89], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v83                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v83, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+91], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+90:vgprValuC+90+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v88, v4
v_mov_b32 v89, v5
v_mov_b32 v90, v6
v_mov_b32 v91, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+88], v[vgprValuC+88] // check Nan
v_bfe_u32 v9, v[vgprValuC+88], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+88], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+88], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+88], 16, v[vgprValuC+88] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+89], v[vgprValuC+89] // check Nan
v_bfe_u32 v9, v[vgprValuC+89], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+89], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+89], v9, v11, s[60:61]
v_and_or_b32 v88, v[vgprValuC+89], v10, v[vgprValuC+88] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+90], 16, v[vgprValuC+90] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+91], v[vgprValuC+91] // check Nan
v_bfe_u32 v9, v[vgprValuC+91], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+91], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+91], v9, v11, s[60:61]
v_and_or_b32 v89, v[vgprValuC+91], v10, v[vgprValuC+90] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[88:89], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(4) = 17 - 12 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+96:vgprValuC+96+1], v[24:25], v[vgprValuC+96:vgprValuC+96+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+98:vgprValuC+98+1], v[26:27], v[vgprValuC+98:vgprValuC+98+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v92                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+96], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v92, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+97], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v93                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+98], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v93, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+99], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+96:vgprValuC+96+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+98:vgprValuC+98+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v96, v4
v_mov_b32 v97, v5
v_mov_b32 v98, v6
v_mov_b32 v99, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+96], v[vgprValuC+96] // check Nan
v_bfe_u32 v9, v[vgprValuC+96], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+96], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+96], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+96], 16, v[vgprValuC+96] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+97], v[vgprValuC+97] // check Nan
v_bfe_u32 v9, v[vgprValuC+97], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+97], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+97], v9, v11, s[60:61]
v_and_or_b32 v96, v[vgprValuC+97], v10, v[vgprValuC+96] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+98], v[vgprValuC+98] // check Nan
v_bfe_u32 v9, v[vgprValuC+98], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+98], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+98], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+98], 16, v[vgprValuC+98] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+99], v[vgprValuC+99] // check Nan
v_bfe_u32 v9, v[vgprValuC+99], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+99], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+99], v9, v11, s[60:61]
v_and_or_b32 v97, v[vgprValuC+99], v10, v[vgprValuC+98] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[96:97], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(3) = 17 - 13 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[24:25], v[vgprValuC+100:vgprValuC+100+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[26:27], v[vgprValuC+102:vgprValuC+102+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v94, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+101], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v95                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+102], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v95, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+103], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+102:vgprValuC+102+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_mov_b32 v101, v5
v_mov_b32 v102, v6
v_mov_b32 v103, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+100], 16, v[vgprValuC+100] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+101], v[vgprValuC+101] // check Nan
v_bfe_u32 v9, v[vgprValuC+101], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+101], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+101], v9, v11, s[60:61]
v_and_or_b32 v100, v[vgprValuC+101], v10, v[vgprValuC+100] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+102], v[vgprValuC+102] // check Nan
v_bfe_u32 v9, v[vgprValuC+102], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+102], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+102], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+102], 16, v[vgprValuC+102] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+103], v[vgprValuC+103] // check Nan
v_bfe_u32 v9, v[vgprValuC+103], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+103], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+103], v9, v11, s[60:61]
v_and_or_b32 v101, v[vgprValuC+103], v10, v[vgprValuC+102] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[100:101], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(2) = 17 - 14 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[24:25], v[vgprValuC+108:vgprValuC+108+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[26:27], v[vgprValuC+110:vgprValuC+110+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v104                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+108], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v104, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+109], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v105                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+110], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v105, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+111], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+110:vgprValuC+110+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v108, v4
v_mov_b32 v109, v5
v_mov_b32 v110, v6
v_mov_b32 v111, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+108], v[vgprValuC+108] // check Nan
v_bfe_u32 v9, v[vgprValuC+108], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+108], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+108], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+108], 16, v[vgprValuC+108] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+109], v[vgprValuC+109] // check Nan
v_bfe_u32 v9, v[vgprValuC+109], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+109], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+109], v9, v11, s[60:61]
v_and_or_b32 v108, v[vgprValuC+109], v10, v[vgprValuC+108] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+110], v[vgprValuC+110] // check Nan
v_bfe_u32 v9, v[vgprValuC+110], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+110], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+110], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+110], 16, v[vgprValuC+110] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+111], v[vgprValuC+111] // check Nan
v_bfe_u32 v9, v[vgprValuC+111], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+111], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+111], v9, v11, s[60:61]
v_and_or_b32 v109, v[vgprValuC+111], v10, v[vgprValuC+110] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[108:109], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(1) = 17 - 15 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[24:25], v[vgprValuC+112:vgprValuC+112+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[26:27], v[vgprValuC+114:vgprValuC+114+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v106                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+112], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v106, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+113], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v107                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+114], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v107, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+115], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+114:vgprValuC+114+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v112, v4
v_mov_b32 v113, v5
v_mov_b32 v114, v6
v_mov_b32 v115, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+112], v[vgprValuC+112] // check Nan
v_bfe_u32 v9, v[vgprValuC+112], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+112], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+112], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+112], 16, v[vgprValuC+112] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+113], v[vgprValuC+113] // check Nan
v_bfe_u32 v9, v[vgprValuC+113], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+113], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+113], v9, v11, s[60:61]
v_and_or_b32 v112, v[vgprValuC+113], v10, v[vgprValuC+112] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+114], v[vgprValuC+114] // check Nan
v_bfe_u32 v9, v[vgprValuC+114], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+114], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+114], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+114], 16, v[vgprValuC+114] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+115], v[vgprValuC+115] // check Nan
v_bfe_u32 v9, v[vgprValuC+115], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+115], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+115], v9, v11, s[60:61]
v_and_or_b32 v113, v[vgprValuC+115], v10, v[vgprValuC+114] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[112:113], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(0) = 17 - 16 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+120:vgprValuC+120+1], v[24:25], v[vgprValuC+120:vgprValuC+120+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+122:vgprValuC+122+1], v[26:27], v[vgprValuC+122:vgprValuC+122+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v116                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+120], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v116, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+121], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v117                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+122], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v117, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+123], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+120:vgprValuC+120+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+122:vgprValuC+122+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v120, v4
v_mov_b32 v121, v5
v_mov_b32 v122, v6
v_mov_b32 v123, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+120], v[vgprValuC+120] // check Nan
v_bfe_u32 v9, v[vgprValuC+120], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+120], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+120], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+120], 16, v[vgprValuC+120] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+121], v[vgprValuC+121] // check Nan
v_bfe_u32 v9, v[vgprValuC+121], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+121], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+121], v9, v11, s[60:61]
v_and_or_b32 v120, v[vgprValuC+121], v10, v[vgprValuC+120] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+122], v[vgprValuC+122] // check Nan
v_bfe_u32 v9, v[vgprValuC+122], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+122], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+122], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+122], 16, v[vgprValuC+122] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+123], v[vgprValuC+123] // check Nan
v_bfe_u32 v9, v[vgprValuC+123], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+123], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+123], v9, v11, s[60:61]
v_and_or_b32 v121, v[vgprValuC+123], v10, v[vgprValuC+122] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[120:121], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 biasDim=0 */

/******************************************/
/* Global Write Beta Batch #1 (d1,d0,vc1,vc0) = */
/*    (4,0,0,0:vw4); (4,0,1,0:vw4); (4,0,2,0:vw4); (4,0,3,0:vw4); (5,0,0,0:vw4); (5,0,1,0:vw4); (5,0,2,0:vw4); (5,0,3,0:vw4); (6,0,0,0:vw4); (6,0,1,0:vw4); (6,0,2,0:vw4); (6,0,3,0:vw4); (7,0,0,0:vw4); (7,0,1,0:vw4); (7,0,2,0:vw4); (7,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(4,0,0,0) */
s_mul_i32 s60, s[sgprStrideC1J], 26                // scale StrideC *= numRows(13) * bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[18:19], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s60
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
ds_read_b128 v[20:23], v15 offset:0                // load bias
buffer_load_dwordx4 v[24:27], v16, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(4,1,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[32:33], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(4,2,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[34:35], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(4,3,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[44:45], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(5,0,0,0) */
s_mul_i32 s60, s[sgprStrideC1J], 26                // scale StrideC *= numRows(13) * bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[46:47], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(5,1,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[56:57], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(5,2,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[58:59], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(5,3,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[68:69], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(6,0,0,0) */
s_mul_i32 s60, s[sgprStrideC1J], 26                // scale StrideC *= numRows(13) * bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[70:71], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(6,1,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[80:81], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(6,2,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[82:83], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(6,3,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[92:93], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(7,0,0,0) */
s_mul_i32 s60, s[sgprStrideC1J], 26                // scale StrideC *= numRows(13) * bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[94:95], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(7,1,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[104:105], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(7,2,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[106:107], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(7,3,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[116:117], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_accvgpr_read_b32 v[vgprValuC+28], acc64          // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+29], acc68          // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+30], acc72          // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+31], acc76          // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+36], acc65          // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+37], acc69          // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+38], acc73          // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+39], acc77          // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+40], acc66          // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+41], acc70          // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+42], acc74          // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+43], acc78          // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+48], acc67          // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+49], acc71          // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+50], acc75          // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+51], acc79          // copy acc to vreg[79]
v_accvgpr_read_b32 v[vgprValuC+52], acc80          // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+53], acc84          // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+54], acc88          // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+55], acc92          // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+60], acc81          // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+61], acc85          // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+62], acc89          // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+63], acc93          // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+64], acc82          // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+65], acc86          // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+66], acc90          // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+67], acc94          // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+72], acc83          // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+73], acc87          // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+74], acc91          // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+75], acc95          // copy acc to vreg[95]
v_accvgpr_read_b32 v[vgprValuC+76], acc96          // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+77], acc100         // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+78], acc104         // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+79], acc108         // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+84], acc97          // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+85], acc101         // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+86], acc105         // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+87], acc109         // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+88], acc98          // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+89], acc102         // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+90], acc106         // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+91], acc110         // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+96], acc99          // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+97], acc103         // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+98], acc107         // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+99], acc111         // copy acc to vreg[111]
v_accvgpr_read_b32 v[vgprValuC+100], acc112        // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+101], acc116        // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+102], acc120        // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+103], acc124        // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+108], acc113        // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+109], acc117        // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+110], acc121        // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+111], acc125        // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+112], acc114        // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+113], acc118        // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+114], acc122        // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+115], acc126        // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+120], acc115        // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+121], acc119        // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+122], acc123        // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+123], acc127        // copy acc to vreg[127]

/* rC *= alpha batchElements=[(4, 0, 0, 0), (4, 0, 1, 0), (4, 0, 2, 0), (4, 0, 3, 0), (5, 0, 0, 0), (5, 0, 1, 0), (5, 0, 2, 0), (5, 0, 3, 0), (6, 0, 0, 0), (6, 0, 1, 0), (6, 0, 2, 0), (6, 0, 3, 0), (7, 0, 0, 0), (7, 0, 1, 0), (7, 0, 2, 0), (7, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+97], s[sgprAlpha], v[vgprValuC+97] // *= alpha
v_mul_f32 v[vgprValuC+98], s[sgprAlpha], v[vgprValuC+98] // *= alpha
v_mul_f32 v[vgprValuC+99], s[sgprAlpha], v[vgprValuC+99] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+112], s[sgprAlpha], v[vgprValuC+112] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+114], s[sgprAlpha], v[vgprValuC+114] // *= alpha
v_mul_f32 v[vgprValuC+115], s[sgprAlpha], v[vgprValuC+115] // *= alpha
v_mul_f32 v[vgprValuC+120], s[sgprAlpha], v[vgprValuC+120] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+122], s[sgprAlpha], v[vgprValuC+122] // *= alpha
v_mul_f32 v[vgprValuC+123], s[sgprAlpha], v[vgprValuC+123] // *= alpha

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16

s_waitcnt lgkmcnt(0), vmcnt(15)                    // vmcnt(15) = 17 - 1 (beta) - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v18                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+28], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v18, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+29], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v19                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+30], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v19, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+31], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(14) = 17 - 2 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v32                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+36], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v32, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+37], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v33                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+38], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v33, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+39], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(13) = 17 - 3 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[24:25], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[26:27], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v34                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v34, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+41], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v35                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+42], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v35, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+43], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_mov_b32 v41, v5
v_mov_b32 v42, v6
v_mov_b32 v43, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+40], 16, v[vgprValuC+40] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+41], v[vgprValuC+41] // check Nan
v_bfe_u32 v9, v[vgprValuC+41], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+41], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+41], v9, v11, s[60:61]
v_and_or_b32 v40, v[vgprValuC+41], v10, v[vgprValuC+40] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+42], v[vgprValuC+42] // check Nan
v_bfe_u32 v9, v[vgprValuC+42], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+42], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+42], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+42], 16, v[vgprValuC+42] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+43], v[vgprValuC+43] // check Nan
v_bfe_u32 v9, v[vgprValuC+43], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+43], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+43], v9, v11, s[60:61]
v_and_or_b32 v41, v[vgprValuC+43], v10, v[vgprValuC+42] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(12) = 17 - 4 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+48], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v44, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+49], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v45                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v45, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+51], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_mov_b32 v49, v5
v_mov_b32 v50, v6
v_mov_b32 v51, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+48], 16, v[vgprValuC+48] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+49], v[vgprValuC+49] // check Nan
v_bfe_u32 v9, v[vgprValuC+49], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+49], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+49], v9, v11, s[60:61]
v_and_or_b32 v48, v[vgprValuC+49], v10, v[vgprValuC+48] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+50], 16, v[vgprValuC+50] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+51], v[vgprValuC+51] // check Nan
v_bfe_u32 v9, v[vgprValuC+51], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+51], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+51], v9, v11, s[60:61]
v_and_or_b32 v49, v[vgprValuC+51], v10, v[vgprValuC+50] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(11) = 17 - 5 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[24:25], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[26:27], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v46                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+52], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v46, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+53], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v47                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+54], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v47, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_mov_b32 v53, v5
v_mov_b32 v54, v6
v_mov_b32 v55, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+52], 16, v[vgprValuC+52] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+53], v[vgprValuC+53] // check Nan
v_bfe_u32 v9, v[vgprValuC+53], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+53], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+53], v9, v11, s[60:61]
v_and_or_b32 v52, v[vgprValuC+53], v10, v[vgprValuC+52] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+54], v[vgprValuC+54] // check Nan
v_bfe_u32 v9, v[vgprValuC+54], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+54], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+54], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+54], 16, v[vgprValuC+54] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_and_or_b32 v53, v[vgprValuC+55], v10, v[vgprValuC+54] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[52:53], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(10) = 17 - 6 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[24:25], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[26:27], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v56                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v56, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+61], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v57                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+62], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v57, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+63], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_mov_b32 v61, v5
v_mov_b32 v62, v6
v_mov_b32 v63, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+60], 16, v[vgprValuC+60] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+61], v[vgprValuC+61] // check Nan
v_bfe_u32 v9, v[vgprValuC+61], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+61], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+61], v9, v11, s[60:61]
v_and_or_b32 v60, v[vgprValuC+61], v10, v[vgprValuC+60] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+62], v[vgprValuC+62] // check Nan
v_bfe_u32 v9, v[vgprValuC+62], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+62], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+62], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+62], 16, v[vgprValuC+62] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+63], v[vgprValuC+63] // check Nan
v_bfe_u32 v9, v[vgprValuC+63], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+63], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+63], v9, v11, s[60:61]
v_and_or_b32 v61, v[vgprValuC+63], v10, v[vgprValuC+62] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(9) = 17 - 7 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v58                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+64], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v58, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+66], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v59, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+67], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_mov_b32 v65, v5
v_mov_b32 v66, v6
v_mov_b32 v67, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+64], 16, v[vgprValuC+64] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_and_or_b32 v64, v[vgprValuC+65], v10, v[vgprValuC+64] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+66], v[vgprValuC+66] // check Nan
v_bfe_u32 v9, v[vgprValuC+66], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+66], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+66], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+66], 16, v[vgprValuC+66] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+67], v[vgprValuC+67] // check Nan
v_bfe_u32 v9, v[vgprValuC+67], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+67], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+67], v9, v11, s[60:61]
v_and_or_b32 v65, v[vgprValuC+67], v10, v[vgprValuC+66] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[64:65], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(8) = 17 - 8 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v68                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+72], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v68, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+73], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+74], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v69, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+74:vgprValuC+74+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_mov_b32 v73, v5
v_mov_b32 v74, v6
v_mov_b32 v75, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+72], 16, v[vgprValuC+72] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+73], v[vgprValuC+73] // check Nan
v_bfe_u32 v9, v[vgprValuC+73], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+73], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+73], v9, v11, s[60:61]
v_and_or_b32 v72, v[vgprValuC+73], v10, v[vgprValuC+72] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+74], v[vgprValuC+74] // check Nan
v_bfe_u32 v9, v[vgprValuC+74], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+74], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+74], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+74], 16, v[vgprValuC+74] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_and_or_b32 v73, v[vgprValuC+75], v10, v[vgprValuC+74] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[72:73], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(7) = 17 - 9 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[24:25], v[vgprValuC+76:vgprValuC+76+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[26:27], v[vgprValuC+78:vgprValuC+78+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v70                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+76], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v70, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+77], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v71                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+78], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v71, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+79], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+78:vgprValuC+78+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_mov_b32 v77, v5
v_mov_b32 v78, v6
v_mov_b32 v79, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+76], 16, v[vgprValuC+76] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+77], v[vgprValuC+77] // check Nan
v_bfe_u32 v9, v[vgprValuC+77], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+77], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+77], v9, v11, s[60:61]
v_and_or_b32 v76, v[vgprValuC+77], v10, v[vgprValuC+76] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+78], v[vgprValuC+78] // check Nan
v_bfe_u32 v9, v[vgprValuC+78], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+78], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+78], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+78], 16, v[vgprValuC+78] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+79], v[vgprValuC+79] // check Nan
v_bfe_u32 v9, v[vgprValuC+79], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+79], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+79], v9, v11, s[60:61]
v_and_or_b32 v77, v[vgprValuC+79], v10, v[vgprValuC+78] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(6) = 17 - 10 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[24:25], v[vgprValuC+84:vgprValuC+84+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[26:27], v[vgprValuC+86:vgprValuC+86+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v80                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+84], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v80, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v81                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+86], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v81, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+87], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+86:vgprValuC+86+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_mov_b32 v85, v5
v_mov_b32 v86, v6
v_mov_b32 v87, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+84], 16, v[vgprValuC+84] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_and_or_b32 v84, v[vgprValuC+85], v10, v[vgprValuC+84] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+86], v[vgprValuC+86] // check Nan
v_bfe_u32 v9, v[vgprValuC+86], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+86], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+86], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+86], 16, v[vgprValuC+86] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+87], v[vgprValuC+87] // check Nan
v_bfe_u32 v9, v[vgprValuC+87], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+87], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+87], v9, v11, s[60:61]
v_and_or_b32 v85, v[vgprValuC+87], v10, v[vgprValuC+86] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[84:85], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(5) = 17 - 11 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[24:25], v[vgprValuC+88:vgprValuC+88+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[26:27], v[vgprValuC+90:vgprValuC+90+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v82                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+88], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v82, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+89], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v83                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v83, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+91], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+90:vgprValuC+90+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v88, v4
v_mov_b32 v89, v5
v_mov_b32 v90, v6
v_mov_b32 v91, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+88], v[vgprValuC+88] // check Nan
v_bfe_u32 v9, v[vgprValuC+88], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+88], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+88], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+88], 16, v[vgprValuC+88] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+89], v[vgprValuC+89] // check Nan
v_bfe_u32 v9, v[vgprValuC+89], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+89], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+89], v9, v11, s[60:61]
v_and_or_b32 v88, v[vgprValuC+89], v10, v[vgprValuC+88] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+90], 16, v[vgprValuC+90] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+91], v[vgprValuC+91] // check Nan
v_bfe_u32 v9, v[vgprValuC+91], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+91], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+91], v9, v11, s[60:61]
v_and_or_b32 v89, v[vgprValuC+91], v10, v[vgprValuC+90] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[88:89], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(4) = 17 - 12 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+96:vgprValuC+96+1], v[24:25], v[vgprValuC+96:vgprValuC+96+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+98:vgprValuC+98+1], v[26:27], v[vgprValuC+98:vgprValuC+98+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v92                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+96], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v92, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+97], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v93                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+98], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v93, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+99], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+96:vgprValuC+96+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+98:vgprValuC+98+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v96, v4
v_mov_b32 v97, v5
v_mov_b32 v98, v6
v_mov_b32 v99, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+96], v[vgprValuC+96] // check Nan
v_bfe_u32 v9, v[vgprValuC+96], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+96], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+96], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+96], 16, v[vgprValuC+96] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+97], v[vgprValuC+97] // check Nan
v_bfe_u32 v9, v[vgprValuC+97], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+97], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+97], v9, v11, s[60:61]
v_and_or_b32 v96, v[vgprValuC+97], v10, v[vgprValuC+96] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+98], v[vgprValuC+98] // check Nan
v_bfe_u32 v9, v[vgprValuC+98], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+98], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+98], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+98], 16, v[vgprValuC+98] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+99], v[vgprValuC+99] // check Nan
v_bfe_u32 v9, v[vgprValuC+99], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+99], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+99], v9, v11, s[60:61]
v_and_or_b32 v97, v[vgprValuC+99], v10, v[vgprValuC+98] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[96:97], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(3) = 17 - 13 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[24:25], v[vgprValuC+100:vgprValuC+100+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[26:27], v[vgprValuC+102:vgprValuC+102+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v94, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+101], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v95                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+102], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v95, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+103], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+102:vgprValuC+102+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_mov_b32 v101, v5
v_mov_b32 v102, v6
v_mov_b32 v103, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+100], 16, v[vgprValuC+100] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+101], v[vgprValuC+101] // check Nan
v_bfe_u32 v9, v[vgprValuC+101], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+101], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+101], v9, v11, s[60:61]
v_and_or_b32 v100, v[vgprValuC+101], v10, v[vgprValuC+100] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+102], v[vgprValuC+102] // check Nan
v_bfe_u32 v9, v[vgprValuC+102], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+102], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+102], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+102], 16, v[vgprValuC+102] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+103], v[vgprValuC+103] // check Nan
v_bfe_u32 v9, v[vgprValuC+103], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+103], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+103], v9, v11, s[60:61]
v_and_or_b32 v101, v[vgprValuC+103], v10, v[vgprValuC+102] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[100:101], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(2) = 17 - 14 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[24:25], v[vgprValuC+108:vgprValuC+108+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[26:27], v[vgprValuC+110:vgprValuC+110+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v104                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+108], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v104, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+109], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v105                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+110], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v105, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+111], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+110:vgprValuC+110+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v108, v4
v_mov_b32 v109, v5
v_mov_b32 v110, v6
v_mov_b32 v111, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+108], v[vgprValuC+108] // check Nan
v_bfe_u32 v9, v[vgprValuC+108], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+108], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+108], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+108], 16, v[vgprValuC+108] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+109], v[vgprValuC+109] // check Nan
v_bfe_u32 v9, v[vgprValuC+109], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+109], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+109], v9, v11, s[60:61]
v_and_or_b32 v108, v[vgprValuC+109], v10, v[vgprValuC+108] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+110], v[vgprValuC+110] // check Nan
v_bfe_u32 v9, v[vgprValuC+110], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+110], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+110], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+110], 16, v[vgprValuC+110] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+111], v[vgprValuC+111] // check Nan
v_bfe_u32 v9, v[vgprValuC+111], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+111], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+111], v9, v11, s[60:61]
v_and_or_b32 v109, v[vgprValuC+111], v10, v[vgprValuC+110] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[108:109], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(1) = 17 - 15 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[24:25], v[vgprValuC+112:vgprValuC+112+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[26:27], v[vgprValuC+114:vgprValuC+114+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v106                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+112], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v106, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+113], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v107                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+114], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v107, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+115], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+114:vgprValuC+114+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v112, v4
v_mov_b32 v113, v5
v_mov_b32 v114, v6
v_mov_b32 v115, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+112], v[vgprValuC+112] // check Nan
v_bfe_u32 v9, v[vgprValuC+112], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+112], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+112], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+112], 16, v[vgprValuC+112] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+113], v[vgprValuC+113] // check Nan
v_bfe_u32 v9, v[vgprValuC+113], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+113], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+113], v9, v11, s[60:61]
v_and_or_b32 v112, v[vgprValuC+113], v10, v[vgprValuC+112] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+114], v[vgprValuC+114] // check Nan
v_bfe_u32 v9, v[vgprValuC+114], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+114], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+114], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+114], 16, v[vgprValuC+114] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+115], v[vgprValuC+115] // check Nan
v_bfe_u32 v9, v[vgprValuC+115], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+115], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+115], v9, v11, s[60:61]
v_and_or_b32 v113, v[vgprValuC+115], v10, v[vgprValuC+114] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[112:113], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(0) = 17 - 16 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+120:vgprValuC+120+1], v[24:25], v[vgprValuC+120:vgprValuC+120+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+122:vgprValuC+122+1], v[26:27], v[vgprValuC+122:vgprValuC+122+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v116                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+120], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v116, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+121], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v117                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+122], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v117, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+123], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+120:vgprValuC+120+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+122:vgprValuC+122+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v120, v4
v_mov_b32 v121, v5
v_mov_b32 v122, v6
v_mov_b32 v123, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+120], v[vgprValuC+120] // check Nan
v_bfe_u32 v9, v[vgprValuC+120], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+120], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+120], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+120], 16, v[vgprValuC+120] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+121], v[vgprValuC+121] // check Nan
v_bfe_u32 v9, v[vgprValuC+121], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+121], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+121], v9, v11, s[60:61]
v_and_or_b32 v120, v[vgprValuC+121], v10, v[vgprValuC+120] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+122], v[vgprValuC+122] // check Nan
v_bfe_u32 v9, v[vgprValuC+122], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+122], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+122], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+122], 16, v[vgprValuC+122] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+123], v[vgprValuC+123] // check Nan
v_bfe_u32 v9, v[vgprValuC+123], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+123], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+123], v9, v11, s[60:61]
v_and_or_b32 v121, v[vgprValuC+123], v10, v[vgprValuC+122] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[120:121], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 biasDim=0 */

/******************************************/
/* Global Write Beta Batch #2 (d1,d0,vc1,vc0) = */
/*    (8,0,0,0:vw4); (8,0,1,0:vw4); (8,0,2,0:vw4); (8,0,3,0:vw4); (9,0,0,0:vw4); (9,0,1,0:vw4); (9,0,2,0:vw4); (9,0,3,0:vw4); (10,0,0,0:vw4); (10,0,1,0:vw4); (10,0,2,0:vw4); (10,0,3,0:vw4); (11,0,0,0:vw4); (11,0,1,0:vw4); (11,0,2,0:vw4); (11,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(8,0,0,0) */
s_mul_i32 s60, s[sgprStrideC1J], 26                // scale StrideC *= numRows(13) * bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[18:19], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s60
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
ds_read_b128 v[20:23], v15 offset:0                // load bias
buffer_load_dwordx4 v[24:27], v16, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(8,1,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[32:33], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(8,2,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[34:35], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(8,3,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[44:45], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(9,0,0,0) */
s_mul_i32 s60, s[sgprStrideC1J], 26                // scale StrideC *= numRows(13) * bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[46:47], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(9,1,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[56:57], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(9,2,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[58:59], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(9,3,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[68:69], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(10,0,0,0) */
s_mul_i32 s60, s[sgprStrideC1J], 26                // scale StrideC *= numRows(13) * bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[70:71], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(10,1,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[80:81], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(10,2,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[82:83], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(10,3,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[92:93], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(11,0,0,0) */
s_mul_i32 s60, s[sgprStrideC1J], 26                // scale StrideC *= numRows(13) * bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[94:95], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(11,1,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[104:105], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(11,2,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[106:107], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(11,3,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[116:117], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_accvgpr_read_b32 v[vgprValuC+28], acc128         // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+29], acc132         // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+30], acc136         // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+31], acc140         // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+36], acc129         // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+37], acc133         // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+38], acc137         // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+39], acc141         // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+40], acc130         // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+41], acc134         // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+42], acc138         // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+43], acc142         // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+48], acc131         // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+49], acc135         // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+50], acc139         // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+51], acc143         // copy acc to vreg[143]
v_accvgpr_read_b32 v[vgprValuC+52], acc144         // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+53], acc148         // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+54], acc152         // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+55], acc156         // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+60], acc145         // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+61], acc149         // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+62], acc153         // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+63], acc157         // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+64], acc146         // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+65], acc150         // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+66], acc154         // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+67], acc158         // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+72], acc147         // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+73], acc151         // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+74], acc155         // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+75], acc159         // copy acc to vreg[159]
v_accvgpr_read_b32 v[vgprValuC+76], acc160         // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+77], acc164         // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+78], acc168         // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+79], acc172         // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+84], acc161         // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+85], acc165         // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+86], acc169         // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+87], acc173         // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+88], acc162         // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+89], acc166         // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+90], acc170         // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+91], acc174         // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+96], acc163         // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+97], acc167         // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+98], acc171         // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+99], acc175         // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+100], acc176        // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+101], acc180        // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+102], acc184        // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+103], acc188        // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+108], acc177        // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+109], acc181        // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+110], acc185        // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+111], acc189        // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+112], acc178        // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+113], acc182        // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+114], acc186        // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+115], acc190        // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+120], acc179        // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+121], acc183        // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+122], acc187        // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+123], acc191        // copy acc to vreg[191]

/* rC *= alpha batchElements=[(8, 0, 0, 0), (8, 0, 1, 0), (8, 0, 2, 0), (8, 0, 3, 0), (9, 0, 0, 0), (9, 0, 1, 0), (9, 0, 2, 0), (9, 0, 3, 0), (10, 0, 0, 0), (10, 0, 1, 0), (10, 0, 2, 0), (10, 0, 3, 0), (11, 0, 0, 0), (11, 0, 1, 0), (11, 0, 2, 0), (11, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+76], s[sgprAlpha], v[vgprValuC+76] // *= alpha
v_mul_f32 v[vgprValuC+77], s[sgprAlpha], v[vgprValuC+77] // *= alpha
v_mul_f32 v[vgprValuC+78], s[sgprAlpha], v[vgprValuC+78] // *= alpha
v_mul_f32 v[vgprValuC+79], s[sgprAlpha], v[vgprValuC+79] // *= alpha
v_mul_f32 v[vgprValuC+84], s[sgprAlpha], v[vgprValuC+84] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+86], s[sgprAlpha], v[vgprValuC+86] // *= alpha
v_mul_f32 v[vgprValuC+87], s[sgprAlpha], v[vgprValuC+87] // *= alpha
v_mul_f32 v[vgprValuC+88], s[sgprAlpha], v[vgprValuC+88] // *= alpha
v_mul_f32 v[vgprValuC+89], s[sgprAlpha], v[vgprValuC+89] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+91], s[sgprAlpha], v[vgprValuC+91] // *= alpha
v_mul_f32 v[vgprValuC+96], s[sgprAlpha], v[vgprValuC+96] // *= alpha
v_mul_f32 v[vgprValuC+97], s[sgprAlpha], v[vgprValuC+97] // *= alpha
v_mul_f32 v[vgprValuC+98], s[sgprAlpha], v[vgprValuC+98] // *= alpha
v_mul_f32 v[vgprValuC+99], s[sgprAlpha], v[vgprValuC+99] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+112], s[sgprAlpha], v[vgprValuC+112] // *= alpha
v_mul_f32 v[vgprValuC+113], s[sgprAlpha], v[vgprValuC+113] // *= alpha
v_mul_f32 v[vgprValuC+114], s[sgprAlpha], v[vgprValuC+114] // *= alpha
v_mul_f32 v[vgprValuC+115], s[sgprAlpha], v[vgprValuC+115] // *= alpha
v_mul_f32 v[vgprValuC+120], s[sgprAlpha], v[vgprValuC+120] // *= alpha
v_mul_f32 v[vgprValuC+121], s[sgprAlpha], v[vgprValuC+121] // *= alpha
v_mul_f32 v[vgprValuC+122], s[sgprAlpha], v[vgprValuC+122] // *= alpha
v_mul_f32 v[vgprValuC+123], s[sgprAlpha], v[vgprValuC+123] // *= alpha

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16

s_waitcnt lgkmcnt(0), vmcnt(15)                    // vmcnt(15) = 17 - 1 (beta) - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v18                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+28], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v18, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+29], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v19                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+30], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v19, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+31], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(14) = 17 - 2 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v32                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+36], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v32, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+37], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v33                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+38], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v33, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+39], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(13) = 17 - 3 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[24:25], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[26:27], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v34                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v34, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+41], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v35                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+42], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v35, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+43], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_mov_b32 v41, v5
v_mov_b32 v42, v6
v_mov_b32 v43, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+40], 16, v[vgprValuC+40] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+41], v[vgprValuC+41] // check Nan
v_bfe_u32 v9, v[vgprValuC+41], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+41], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+41], v9, v11, s[60:61]
v_and_or_b32 v40, v[vgprValuC+41], v10, v[vgprValuC+40] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+42], v[vgprValuC+42] // check Nan
v_bfe_u32 v9, v[vgprValuC+42], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+42], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+42], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+42], 16, v[vgprValuC+42] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+43], v[vgprValuC+43] // check Nan
v_bfe_u32 v9, v[vgprValuC+43], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+43], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+43], v9, v11, s[60:61]
v_and_or_b32 v41, v[vgprValuC+43], v10, v[vgprValuC+42] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(12) = 17 - 4 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+48], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v44, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+49], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v45                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v45, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+51], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_mov_b32 v49, v5
v_mov_b32 v50, v6
v_mov_b32 v51, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+48], 16, v[vgprValuC+48] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+49], v[vgprValuC+49] // check Nan
v_bfe_u32 v9, v[vgprValuC+49], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+49], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+49], v9, v11, s[60:61]
v_and_or_b32 v48, v[vgprValuC+49], v10, v[vgprValuC+48] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+50], 16, v[vgprValuC+50] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+51], v[vgprValuC+51] // check Nan
v_bfe_u32 v9, v[vgprValuC+51], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+51], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+51], v9, v11, s[60:61]
v_and_or_b32 v49, v[vgprValuC+51], v10, v[vgprValuC+50] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(11) = 17 - 5 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[24:25], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[26:27], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v46                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+52], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v46, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+53], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v47                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+54], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v47, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_mov_b32 v53, v5
v_mov_b32 v54, v6
v_mov_b32 v55, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+52], 16, v[vgprValuC+52] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+53], v[vgprValuC+53] // check Nan
v_bfe_u32 v9, v[vgprValuC+53], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+53], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+53], v9, v11, s[60:61]
v_and_or_b32 v52, v[vgprValuC+53], v10, v[vgprValuC+52] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+54], v[vgprValuC+54] // check Nan
v_bfe_u32 v9, v[vgprValuC+54], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+54], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+54], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+54], 16, v[vgprValuC+54] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_and_or_b32 v53, v[vgprValuC+55], v10, v[vgprValuC+54] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[52:53], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(10) = 17 - 6 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[24:25], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[26:27], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v56                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v56, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+61], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v57                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+62], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v57, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+63], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_mov_b32 v61, v5
v_mov_b32 v62, v6
v_mov_b32 v63, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+60], 16, v[vgprValuC+60] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+61], v[vgprValuC+61] // check Nan
v_bfe_u32 v9, v[vgprValuC+61], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+61], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+61], v9, v11, s[60:61]
v_and_or_b32 v60, v[vgprValuC+61], v10, v[vgprValuC+60] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+62], v[vgprValuC+62] // check Nan
v_bfe_u32 v9, v[vgprValuC+62], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+62], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+62], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+62], 16, v[vgprValuC+62] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+63], v[vgprValuC+63] // check Nan
v_bfe_u32 v9, v[vgprValuC+63], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+63], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+63], v9, v11, s[60:61]
v_and_or_b32 v61, v[vgprValuC+63], v10, v[vgprValuC+62] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(9) = 17 - 7 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v58                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+64], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v58, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+66], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v59, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+67], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_mov_b32 v65, v5
v_mov_b32 v66, v6
v_mov_b32 v67, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+64], 16, v[vgprValuC+64] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_and_or_b32 v64, v[vgprValuC+65], v10, v[vgprValuC+64] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+66], v[vgprValuC+66] // check Nan
v_bfe_u32 v9, v[vgprValuC+66], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+66], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+66], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+66], 16, v[vgprValuC+66] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+67], v[vgprValuC+67] // check Nan
v_bfe_u32 v9, v[vgprValuC+67], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+67], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+67], v9, v11, s[60:61]
v_and_or_b32 v65, v[vgprValuC+67], v10, v[vgprValuC+66] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[64:65], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(8) = 17 - 8 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v68                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+72], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v68, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+73], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+74], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v69, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+74:vgprValuC+74+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_mov_b32 v73, v5
v_mov_b32 v74, v6
v_mov_b32 v75, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+72], 16, v[vgprValuC+72] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+73], v[vgprValuC+73] // check Nan
v_bfe_u32 v9, v[vgprValuC+73], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+73], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+73], v9, v11, s[60:61]
v_and_or_b32 v72, v[vgprValuC+73], v10, v[vgprValuC+72] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+74], v[vgprValuC+74] // check Nan
v_bfe_u32 v9, v[vgprValuC+74], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+74], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+74], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+74], 16, v[vgprValuC+74] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_and_or_b32 v73, v[vgprValuC+75], v10, v[vgprValuC+74] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[72:73], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(7) = 17 - 9 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+76:vgprValuC+76+1], v[24:25], v[vgprValuC+76:vgprValuC+76+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+78:vgprValuC+78+1], v[26:27], v[vgprValuC+78:vgprValuC+78+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v70                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+76], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v70, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+77], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v71                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+78], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v71, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+79], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+76:vgprValuC+76+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+78:vgprValuC+78+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v76, v4
v_mov_b32 v77, v5
v_mov_b32 v78, v6
v_mov_b32 v79, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+76], v[vgprValuC+76] // check Nan
v_bfe_u32 v9, v[vgprValuC+76], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+76], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+76], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+76], 16, v[vgprValuC+76] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+77], v[vgprValuC+77] // check Nan
v_bfe_u32 v9, v[vgprValuC+77], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+77], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+77], v9, v11, s[60:61]
v_and_or_b32 v76, v[vgprValuC+77], v10, v[vgprValuC+76] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+78], v[vgprValuC+78] // check Nan
v_bfe_u32 v9, v[vgprValuC+78], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+78], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+78], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+78], 16, v[vgprValuC+78] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+79], v[vgprValuC+79] // check Nan
v_bfe_u32 v9, v[vgprValuC+79], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+79], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+79], v9, v11, s[60:61]
v_and_or_b32 v77, v[vgprValuC+79], v10, v[vgprValuC+78] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[76:77], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(6) = 17 - 10 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+84:vgprValuC+84+1], v[24:25], v[vgprValuC+84:vgprValuC+84+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+86:vgprValuC+86+1], v[26:27], v[vgprValuC+86:vgprValuC+86+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v80                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+84], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v80, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v81                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+86], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v81, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+87], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+84:vgprValuC+84+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+86:vgprValuC+86+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v84, v4
v_mov_b32 v85, v5
v_mov_b32 v86, v6
v_mov_b32 v87, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+84], v[vgprValuC+84] // check Nan
v_bfe_u32 v9, v[vgprValuC+84], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+84], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+84], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+84], 16, v[vgprValuC+84] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_and_or_b32 v84, v[vgprValuC+85], v10, v[vgprValuC+84] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+86], v[vgprValuC+86] // check Nan
v_bfe_u32 v9, v[vgprValuC+86], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+86], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+86], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+86], 16, v[vgprValuC+86] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+87], v[vgprValuC+87] // check Nan
v_bfe_u32 v9, v[vgprValuC+87], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+87], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+87], v9, v11, s[60:61]
v_and_or_b32 v85, v[vgprValuC+87], v10, v[vgprValuC+86] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[84:85], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(5) = 17 - 11 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+88:vgprValuC+88+1], v[24:25], v[vgprValuC+88:vgprValuC+88+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+90:vgprValuC+90+1], v[26:27], v[vgprValuC+90:vgprValuC+90+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v82                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+88], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v82, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+89], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v83                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v83, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+91], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+88:vgprValuC+88+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+90:vgprValuC+90+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v88, v4
v_mov_b32 v89, v5
v_mov_b32 v90, v6
v_mov_b32 v91, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+88], v[vgprValuC+88] // check Nan
v_bfe_u32 v9, v[vgprValuC+88], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+88], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+88], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+88], 16, v[vgprValuC+88] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+89], v[vgprValuC+89] // check Nan
v_bfe_u32 v9, v[vgprValuC+89], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+89], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+89], v9, v11, s[60:61]
v_and_or_b32 v88, v[vgprValuC+89], v10, v[vgprValuC+88] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+90], 16, v[vgprValuC+90] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+91], v[vgprValuC+91] // check Nan
v_bfe_u32 v9, v[vgprValuC+91], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+91], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+91], v9, v11, s[60:61]
v_and_or_b32 v89, v[vgprValuC+91], v10, v[vgprValuC+90] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[88:89], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(4) = 17 - 12 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+96:vgprValuC+96+1], v[24:25], v[vgprValuC+96:vgprValuC+96+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+98:vgprValuC+98+1], v[26:27], v[vgprValuC+98:vgprValuC+98+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v92                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+96], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v92, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+97], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v93                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+98], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v93, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+99], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+96:vgprValuC+96+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+98:vgprValuC+98+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v96, v4
v_mov_b32 v97, v5
v_mov_b32 v98, v6
v_mov_b32 v99, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+96], v[vgprValuC+96] // check Nan
v_bfe_u32 v9, v[vgprValuC+96], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+96], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+96], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+96], 16, v[vgprValuC+96] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+97], v[vgprValuC+97] // check Nan
v_bfe_u32 v9, v[vgprValuC+97], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+97], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+97], v9, v11, s[60:61]
v_and_or_b32 v96, v[vgprValuC+97], v10, v[vgprValuC+96] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+98], v[vgprValuC+98] // check Nan
v_bfe_u32 v9, v[vgprValuC+98], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+98], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+98], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+98], 16, v[vgprValuC+98] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+99], v[vgprValuC+99] // check Nan
v_bfe_u32 v9, v[vgprValuC+99], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+99], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+99], v9, v11, s[60:61]
v_and_or_b32 v97, v[vgprValuC+99], v10, v[vgprValuC+98] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[96:97], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(3) = 17 - 13 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[24:25], v[vgprValuC+100:vgprValuC+100+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[26:27], v[vgprValuC+102:vgprValuC+102+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v94, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+101], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v95                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+102], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v95, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+103], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+102:vgprValuC+102+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_mov_b32 v101, v5
v_mov_b32 v102, v6
v_mov_b32 v103, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+100], 16, v[vgprValuC+100] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+101], v[vgprValuC+101] // check Nan
v_bfe_u32 v9, v[vgprValuC+101], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+101], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+101], v9, v11, s[60:61]
v_and_or_b32 v100, v[vgprValuC+101], v10, v[vgprValuC+100] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+102], v[vgprValuC+102] // check Nan
v_bfe_u32 v9, v[vgprValuC+102], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+102], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+102], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+102], 16, v[vgprValuC+102] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+103], v[vgprValuC+103] // check Nan
v_bfe_u32 v9, v[vgprValuC+103], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+103], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+103], v9, v11, s[60:61]
v_and_or_b32 v101, v[vgprValuC+103], v10, v[vgprValuC+102] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[100:101], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(2) = 17 - 14 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[24:25], v[vgprValuC+108:vgprValuC+108+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[26:27], v[vgprValuC+110:vgprValuC+110+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v104                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+108], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v104, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+109], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v105                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+110], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v105, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+111], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+110:vgprValuC+110+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v108, v4
v_mov_b32 v109, v5
v_mov_b32 v110, v6
v_mov_b32 v111, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+108], v[vgprValuC+108] // check Nan
v_bfe_u32 v9, v[vgprValuC+108], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+108], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+108], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+108], 16, v[vgprValuC+108] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+109], v[vgprValuC+109] // check Nan
v_bfe_u32 v9, v[vgprValuC+109], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+109], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+109], v9, v11, s[60:61]
v_and_or_b32 v108, v[vgprValuC+109], v10, v[vgprValuC+108] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+110], v[vgprValuC+110] // check Nan
v_bfe_u32 v9, v[vgprValuC+110], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+110], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+110], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+110], 16, v[vgprValuC+110] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+111], v[vgprValuC+111] // check Nan
v_bfe_u32 v9, v[vgprValuC+111], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+111], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+111], v9, v11, s[60:61]
v_and_or_b32 v109, v[vgprValuC+111], v10, v[vgprValuC+110] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[108:109], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(1) = 17 - 15 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+112:vgprValuC+112+1], v[24:25], v[vgprValuC+112:vgprValuC+112+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+114:vgprValuC+114+1], v[26:27], v[vgprValuC+114:vgprValuC+114+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v106                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+112], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v106, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+113], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v107                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+114], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v107, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+115], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+112:vgprValuC+112+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+114:vgprValuC+114+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v112, v4
v_mov_b32 v113, v5
v_mov_b32 v114, v6
v_mov_b32 v115, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+112], v[vgprValuC+112] // check Nan
v_bfe_u32 v9, v[vgprValuC+112], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+112], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+112], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+112], 16, v[vgprValuC+112] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+113], v[vgprValuC+113] // check Nan
v_bfe_u32 v9, v[vgprValuC+113], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+113], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+113], v9, v11, s[60:61]
v_and_or_b32 v112, v[vgprValuC+113], v10, v[vgprValuC+112] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+114], v[vgprValuC+114] // check Nan
v_bfe_u32 v9, v[vgprValuC+114], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+114], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+114], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+114], 16, v[vgprValuC+114] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+115], v[vgprValuC+115] // check Nan
v_bfe_u32 v9, v[vgprValuC+115], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+115], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+115], v9, v11, s[60:61]
v_and_or_b32 v113, v[vgprValuC+115], v10, v[vgprValuC+114] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[112:113], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(15)                                // vmcnt(0) = 17 - 16 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+120:vgprValuC+120+1], v[24:25], v[vgprValuC+120:vgprValuC+120+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+122:vgprValuC+122+1], v[26:27], v[vgprValuC+122:vgprValuC+122+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v116                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+120], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v116, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+121], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v117                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+122], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v117, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+123], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+120:vgprValuC+120+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+122:vgprValuC+122+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v120, v4
v_mov_b32 v121, v5
v_mov_b32 v122, v6
v_mov_b32 v123, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+120], v[vgprValuC+120] // check Nan
v_bfe_u32 v9, v[vgprValuC+120], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+120], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+120], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+120], 16, v[vgprValuC+120] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+121], v[vgprValuC+121] // check Nan
v_bfe_u32 v9, v[vgprValuC+121], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+121], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+121], v9, v11, s[60:61]
v_and_or_b32 v120, v[vgprValuC+121], v10, v[vgprValuC+120] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+122], v[vgprValuC+122] // check Nan
v_bfe_u32 v9, v[vgprValuC+122], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+122], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+122], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+122], 16, v[vgprValuC+122] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+123], v[vgprValuC+123] // check Nan
v_bfe_u32 v9, v[vgprValuC+123], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+123], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+123], v9, v11, s[60:61]
v_and_or_b32 v121, v[vgprValuC+123], v10, v[vgprValuC+122] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[120:121], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=1 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Mask optSrdIncForRow=1 biasDim=0 */

/******************************************/
/* Global Write Beta Batch #3 (d1,d0,vc1,vc0) = */
/*    (12,0,0,0:vw4); (12,0,1,0:vw4); (12,0,2,0:vw4); (12,0,3,0:vw4); (13,0,0,0:vw4); (13,0,1,0:vw4); (13,0,2,0:vw4); (13,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
/* (d1,vc1,d0,vc0)=(12,0,0,0) */
s_mul_i32 s60, s[sgprStrideC1J], 26                // scale StrideC *= numRows(13) * bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[18:19], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v15, v0, s60
v_lshlrev_b32 v15, 0x2, v15                        // Bias address scaled by BPE
ds_read_b128 v[20:23], v15 offset:0                // load bias
buffer_load_dwordx4 v[24:27], v16, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
/* (d1,vc1,d0,vc0)=(12,1,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[32:33], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(12,2,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[34:35], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(12,3,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[44:45], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(13,0,0,0) */
s_mul_i32 s60, s[sgprStrideC1J], 26                // scale StrideC *= numRows(13) * bpe
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[46:47], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(13,1,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[56:57], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(13,2,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[58:59], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
/* (d1,vc1,d0,vc0)=(13,3,0,0) */
s_lshl_b32 s60, s[sgprStrideC1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdC+0], s[sgprSrdC+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdC+1], s[sgprSrdC+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_load_dwordx2 v[68:69], v14, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
v_accvgpr_read_b32 v[vgprValuC+28], acc192         // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+29], acc196         // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+30], acc200         // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+31], acc204         // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+36], acc193         // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+37], acc197         // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+38], acc201         // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+39], acc205         // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+40], acc194         // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+41], acc198         // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+42], acc202         // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+43], acc206         // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+48], acc195         // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+49], acc199         // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+50], acc203         // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+51], acc207         // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+52], acc208         // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+53], acc212         // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+54], acc216         // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+55], acc220         // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+60], acc209         // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+61], acc213         // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+62], acc217         // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+63], acc221         // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+64], acc210         // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+65], acc214         // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+66], acc218         // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+67], acc222         // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+72], acc211         // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+73], acc215         // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+74], acc219         // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+75], acc223         // copy acc to vreg[223]

/* rC *= alpha batchElements=[(12, 0, 0, 0), (12, 0, 1, 0), (12, 0, 2, 0), (12, 0, 3, 0), (13, 0, 0, 0), (13, 0, 1, 0), (13, 0, 2, 0), (13, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+41], s[sgprAlpha], v[vgprValuC+41] // *= alpha
v_mul_f32 v[vgprValuC+42], s[sgprAlpha], v[vgprValuC+42] // *= alpha
v_mul_f32 v[vgprValuC+43], s[sgprAlpha], v[vgprValuC+43] // *= alpha
v_mul_f32 v[vgprValuC+48], s[sgprAlpha], v[vgprValuC+48] // *= alpha
v_mul_f32 v[vgprValuC+49], s[sgprAlpha], v[vgprValuC+49] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+51], s[sgprAlpha], v[vgprValuC+51] // *= alpha
v_mul_f32 v[vgprValuC+52], s[sgprAlpha], v[vgprValuC+52] // *= alpha
v_mul_f32 v[vgprValuC+53], s[sgprAlpha], v[vgprValuC+53] // *= alpha
v_mul_f32 v[vgprValuC+54], s[sgprAlpha], v[vgprValuC+54] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+61], s[sgprAlpha], v[vgprValuC+61] // *= alpha
v_mul_f32 v[vgprValuC+62], s[sgprAlpha], v[vgprValuC+62] // *= alpha
v_mul_f32 v[vgprValuC+63], s[sgprAlpha], v[vgprValuC+63] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16

s_waitcnt lgkmcnt(0), vmcnt(7)                     // vmcnt(7) = 9 - 1 (beta) - 1 (scaleAlphaVec) lgkmcnt(0) = 1 - 1 (bias) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v18                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+28], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v18, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+29], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v19                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+30], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v19, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+31], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(7)                                 // vmcnt(6) = 9 - 2 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v32                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+36], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v32, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+37], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v33                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+38], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v33, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+39], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[36:37], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(7)                                 // vmcnt(5) = 9 - 3 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+40:vgprValuC+40+1], v[24:25], v[vgprValuC+40:vgprValuC+40+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+42:vgprValuC+42+1], v[26:27], v[vgprValuC+42:vgprValuC+42+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v34                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v34, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+41], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v35                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+42], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v35, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+43], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+40:vgprValuC+40+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+42:vgprValuC+42+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_mov_b32 v41, v5
v_mov_b32 v42, v6
v_mov_b32 v43, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+40], 16, v[vgprValuC+40] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+41], v[vgprValuC+41] // check Nan
v_bfe_u32 v9, v[vgprValuC+41], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+41], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+41], v9, v11, s[60:61]
v_and_or_b32 v40, v[vgprValuC+41], v10, v[vgprValuC+40] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+42], v[vgprValuC+42] // check Nan
v_bfe_u32 v9, v[vgprValuC+42], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+42], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+42], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+42], 16, v[vgprValuC+42] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+43], v[vgprValuC+43] // check Nan
v_bfe_u32 v9, v[vgprValuC+43], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+43], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+43], v9, v11, s[60:61]
v_and_or_b32 v41, v[vgprValuC+43], v10, v[vgprValuC+42] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[40:41], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(7)                                 // vmcnt(4) = 9 - 4 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+48:vgprValuC+48+1], v[24:25], v[vgprValuC+48:vgprValuC+48+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+50:vgprValuC+50+1], v[26:27], v[vgprValuC+50:vgprValuC+50+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+48], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v44, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+49], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v45                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v45, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+51], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+48:vgprValuC+48+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+50:vgprValuC+50+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v48, v4
v_mov_b32 v49, v5
v_mov_b32 v50, v6
v_mov_b32 v51, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+48], v[vgprValuC+48] // check Nan
v_bfe_u32 v9, v[vgprValuC+48], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+48], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+48], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+48], 16, v[vgprValuC+48] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+49], v[vgprValuC+49] // check Nan
v_bfe_u32 v9, v[vgprValuC+49], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+49], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+49], v9, v11, s[60:61]
v_and_or_b32 v48, v[vgprValuC+49], v10, v[vgprValuC+48] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+50], 16, v[vgprValuC+50] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+51], v[vgprValuC+51] // check Nan
v_bfe_u32 v9, v[vgprValuC+51], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+51], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+51], v9, v11, s[60:61]
v_and_or_b32 v49, v[vgprValuC+51], v10, v[vgprValuC+50] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[48:49], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(7)                                 // vmcnt(3) = 9 - 5 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+52:vgprValuC+52+1], v[24:25], v[vgprValuC+52:vgprValuC+52+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+54:vgprValuC+54+1], v[26:27], v[vgprValuC+54:vgprValuC+54+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v46                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+52], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v46, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+53], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v47                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+54], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v47, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+52:vgprValuC+52+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+54:vgprValuC+54+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v52, v4
v_mov_b32 v53, v5
v_mov_b32 v54, v6
v_mov_b32 v55, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+52], v[vgprValuC+52] // check Nan
v_bfe_u32 v9, v[vgprValuC+52], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+52], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+52], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+52], 16, v[vgprValuC+52] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+53], v[vgprValuC+53] // check Nan
v_bfe_u32 v9, v[vgprValuC+53], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+53], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+53], v9, v11, s[60:61]
v_and_or_b32 v52, v[vgprValuC+53], v10, v[vgprValuC+52] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+54], v[vgprValuC+54] // check Nan
v_bfe_u32 v9, v[vgprValuC+54], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+54], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+54], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+54], 16, v[vgprValuC+54] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_and_or_b32 v53, v[vgprValuC+55], v10, v[vgprValuC+54] // pack two bf16 to dword
s_mul_i32 s60, s[sgprStrideD1J], 26                // scale StrideD *= numRows(13) * bpe
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[52:53], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(7)                                 // vmcnt(2) = 9 - 6 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+60:vgprValuC+60+1], v[24:25], v[vgprValuC+60:vgprValuC+60+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+62:vgprValuC+62+1], v[26:27], v[vgprValuC+62:vgprValuC+62+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v56                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v56, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+61], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v57                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+62], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v57, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+63], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+60:vgprValuC+60+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+62:vgprValuC+62+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_mov_b32 v61, v5
v_mov_b32 v62, v6
v_mov_b32 v63, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+60], 16, v[vgprValuC+60] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+61], v[vgprValuC+61] // check Nan
v_bfe_u32 v9, v[vgprValuC+61], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+61], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+61], v9, v11, s[60:61]
v_and_or_b32 v60, v[vgprValuC+61], v10, v[vgprValuC+60] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+62], v[vgprValuC+62] // check Nan
v_bfe_u32 v9, v[vgprValuC+62], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+62], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+62], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+62], 16, v[vgprValuC+62] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+63], v[vgprValuC+63] // check Nan
v_bfe_u32 v9, v[vgprValuC+63], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+63], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+63], v9, v11, s[60:61]
v_and_or_b32 v61, v[vgprValuC+63], v10, v[vgprValuC+62] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[60:61], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(7)                                 // vmcnt(1) = 9 - 7 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v58                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+64], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v58, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+66], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v59, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+67], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_mov_b32 v65, v5
v_mov_b32 v66, v6
v_mov_b32 v67, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+64], 16, v[vgprValuC+64] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_and_or_b32 v64, v[vgprValuC+65], v10, v[vgprValuC+64] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+66], v[vgprValuC+66] // check Nan
v_bfe_u32 v9, v[vgprValuC+66], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+66], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+66], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+66], 16, v[vgprValuC+66] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+67], v[vgprValuC+67] // check Nan
v_bfe_u32 v9, v[vgprValuC+67], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+67], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+67], v9, v11, s[60:61]
v_and_or_b32 v65, v[vgprValuC+67], v10, v[vgprValuC+66] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[64:65], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D

s_waitcnt vmcnt(7)                                 // vmcnt(0) = 9 - 8 (beta) - 1 (scaleAlphaVec) (interleaved)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v68                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+72], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v68, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+73], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+74], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v69, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+74:vgprValuC+74+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_mov_b32 v73, v5
v_mov_b32 v74, v6
v_mov_b32 v75, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+72], 16, v[vgprValuC+72] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+73], v[vgprValuC+73] // check Nan
v_bfe_u32 v9, v[vgprValuC+73], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+73], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+73], v9, v11, s[60:61]
v_and_or_b32 v72, v[vgprValuC+73], v10, v[vgprValuC+72] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+74], v[vgprValuC+74] // check Nan
v_bfe_u32 v9, v[vgprValuC+74], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+74], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+74], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+74], 16, v[vgprValuC+74] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_and_or_b32 v73, v[vgprValuC+75], v10, v[vgprValuC+74] // pack two bf16 to dword
s_lshl_b32 s60, s[sgprStrideD1J], 1                // incToNextRow: Scale by BPE
s_add_u32 s[sgprSrdD+0], s[sgprSrdD+0], s60        // incToNextRow: gra SRD += inc(lower)
s_addc_u32 s[sgprSrdD+1], s[sgprSrdD+1], 0         // incToNextRow: gra SRD += inc(upper)
buffer_store_dwordx2 v[72:73], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_2                            // jump to end
label_GW_B1_E1_N:
s_cmpk_eq_u32 s[sgprActivationType], 1             // activationType == 1
s_cbranch_scc1 label_To_Activation_Abs_VW4_1_beta_1_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 2             // activationType == 2
s_cbranch_scc1 label_To_Activation_Clippedrelu_VW4_1_beta_1_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 3             // activationType == 3
s_cbranch_scc1 label_To_Activation_Gelu_VW4_1_beta_1_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 4             // activationType == 4
s_cbranch_scc1 label_To_Activation_Leakyrelu_VW4_1_beta_1_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 5             // activationType == 5
s_cbranch_scc1 label_To_Activation_Relu_VW4_1_beta_1_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 6             // activationType == 6
s_cbranch_scc1 label_To_Activation_Sigmoid_VW4_1_beta_1_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 7             // activationType == 7
s_cbranch_scc1 label_To_Activation_Tanh_VW4_1_beta_1_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 9             // activationType == 9
s_cbranch_scc1 label_To_Activation_Geluscaling_VW4_1_beta_1_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 10            // activationType == 10
s_cbranch_scc1 label_To_Activation_Silu_VW4_1_beta_1_edge_1 // Branch if true
label_To_Activation_None_VW4_1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_None_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_2
label_To_Activation_Abs_VW4_1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Abs_VW4, 0x4        // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_2
label_To_Activation_Clippedrelu_VW4_1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Clippedrelu_VW4, 0x4 // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_2
label_To_Activation_Gelu_VW4_1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Gelu_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_2
label_To_Activation_Leakyrelu_VW4_1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Leakyrelu_VW4, 0x4  // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_2
label_To_Activation_Relu_VW4_1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Relu_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_2
label_To_Activation_Sigmoid_VW4_1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Sigmoid_VW4, 0x4    // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_2
label_To_Activation_Tanh_VW4_1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Tanh_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_2
label_To_Activation_Geluscaling_VW4_1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Geluscaling_VW4, 0x4 // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_2
label_To_Activation_Silu_VW4_1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Silu_VW4, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_2
label_ActivationSetPCAddrEnd_2:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=12 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw4); (0,0,1,0:vw4); (0,0,2,0:vw4); (0,0,3,0:vw4); (1,0,0,0:vw4); (1,0,1,0:vw4); (1,0,2,0:vw4); (1,0,3,0:vw4); (2,0,0,0:vw4); (2,0,1,0:vw4); (2,0,2,0:vw4); (2,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v123, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v123, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[16:17], v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v123, v14, s[64:65]             // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b128 v[20:23], v14 offset:0                // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[24:27], v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v123, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v18, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v123, v18, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[34:35], v18, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v19, v0, s60
v_lshlrev_b32 v19, 0x2, v19                        // Bias address scaled by BPE
v_cndmask_b32 v19, v123, v19, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v32, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v18, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v123, v18, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v33, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v33, v123, v33, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[42:43], v33, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s60
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v123, v40, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v33, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v33, v123, v33, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v48, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v123, v48, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[52:53], v48, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v0, s60
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
v_cndmask_b32 v49, v123, v49, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v50, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v48, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v123, v48, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v123, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[60:61], v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v123, v54, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v123, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v62, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v123, v62, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[70:71], v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v0, s60
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
v_cndmask_b32 v63, v123, v63, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v62, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v123, v62, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v69, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v123, v69, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[78:79], v69, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v76, v0, s60
v_lshlrev_b32 v76, 0x2, v76                        // Bias address scaled by BPE
v_cndmask_b32 v76, v123, v76, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v77, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v123, v69, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v84, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v123, v84, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[88:89], v84, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v85, v0, s60
v_lshlrev_b32 v85, 0x2, v85                        // Bias address scaled by BPE
v_cndmask_b32 v85, v123, v85, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v86, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v84, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v123, v84, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v87, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v123, v87, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[96:97], v87, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v90, v0, s60
v_lshlrev_b32 v90, 0x2, v90                        // Bias address scaled by BPE
v_cndmask_b32 v90, v123, v90, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v91, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v87, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v123, v87, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v98, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v123, v98, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[106:107], v98, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v99, v0, s60
v_lshlrev_b32 v99, 0x2, v99                        // Bias address scaled by BPE
v_cndmask_b32 v99, v123, v99, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v104, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v98, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v123, v98, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v105, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v105, v123, v105, s[64:65]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[114:115], v105, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v112, v0, s60
v_lshlrev_b32 v112, 0x2, v112                      // Bias address scaled by BPE
v_cndmask_b32 v112, v123, v112, s[64:65]           // LDBias clip if OOB. offset
v_lshlrev_b32 v113, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v105, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v105, v123, v105, s[64:65]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v120, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v120, v123, v120, s[64:65]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[124:125], v120, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v121, v0, s60
v_lshlrev_b32 v121, 0x2, v121                      // Bias address scaled by BPE
v_cndmask_b32 v121, v123, v121, s[64:65]           // LDBias clip if OOB. offset
v_lshlrev_b32 v122, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v120, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v120, v123, v120, s[64:65]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+28], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+29], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+30], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+31], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+36], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+37], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+38], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+39], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+44], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+45], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+46], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+47], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+56], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+57], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+58], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+59], acc15          // copy acc to vreg[15]
v_accvgpr_read_b32 v[vgprValuC+64], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+65], acc20          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+66], acc24          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+67], acc28          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+72], acc17          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+73], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+74], acc25          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+75], acc29          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+80], acc18          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+81], acc22          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+82], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+83], acc30          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+92], acc19          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+93], acc23          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+94], acc27          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+95], acc31          // copy acc to vreg[31]
v_accvgpr_read_b32 v[vgprValuC+100], acc32         // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+101], acc36         // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+102], acc40         // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+103], acc44         // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+108], acc33         // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+109], acc37         // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+110], acc41         // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+111], acc45         // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+116], acc34         // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+117], acc38         // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+118], acc42         // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+119], acc46         // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+128], acc35         // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+129], acc39         // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+130], acc43         // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+131], acc47         // copy acc to vreg[47]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 1, 0), (0, 0, 2, 0), (0, 0, 3, 0), (1, 0, 0, 0), (1, 0, 1, 0), (1, 0, 2, 0), (1, 0, 3, 0), (2, 0, 0, 0), (2, 0, 1, 0), (2, 0, 2, 0), (2, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
v_mul_f32 v[vgprValuC+128], s[sgprAlpha], v[vgprValuC+128] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+130], s[sgprAlpha], v[vgprValuC+130] // *= alpha
v_mul_f32 v[vgprValuC+131], s[sgprAlpha], v[vgprValuC+131] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+28], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v16, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+29], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v17                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+30], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v17, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+31], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v34                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+36], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v34, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+37], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v35                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+38], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v35, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+39], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
buffer_store_dwordx2 v[36:37], v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[24:25], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[26:27], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v42                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+44], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v42, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v43                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+46], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v43, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+47], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_mov_b32 v45, v5
v_mov_b32 v46, v6
v_mov_b32 v47, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+44], 16, v[vgprValuC+44] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_and_or_b32 v44, v[vgprValuC+45], v10, v[vgprValuC+44] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+46], v[vgprValuC+46] // check Nan
v_bfe_u32 v9, v[vgprValuC+46], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+46], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+46], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+46], 16, v[vgprValuC+46] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+47], v[vgprValuC+47] // check Nan
v_bfe_u32 v9, v[vgprValuC+47], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+47], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+47], v9, v11, s[60:61]
v_and_or_b32 v45, v[vgprValuC+47], v10, v[vgprValuC+46] // pack two bf16 to dword
buffer_store_dwordx2 v[44:45], v33, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[24:25], v[vgprValuC+56:vgprValuC+56+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[26:27], v[vgprValuC+58:vgprValuC+58+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v52                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+56], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v52, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+57], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v53                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+58], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v53, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+59], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+58:vgprValuC+58+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_mov_b32 v57, v5
v_mov_b32 v58, v6
v_mov_b32 v59, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+56], 16, v[vgprValuC+56] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+57], v[vgprValuC+57] // check Nan
v_bfe_u32 v9, v[vgprValuC+57], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+57], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+57], v9, v11, s[60:61]
v_and_or_b32 v56, v[vgprValuC+57], v10, v[vgprValuC+56] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+58], v[vgprValuC+58] // check Nan
v_bfe_u32 v9, v[vgprValuC+58], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+58], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+58], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+58], 16, v[vgprValuC+58] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+59], v[vgprValuC+59] // check Nan
v_bfe_u32 v9, v[vgprValuC+59], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+59], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+59], v9, v11, s[60:61]
v_and_or_b32 v57, v[vgprValuC+59], v10, v[vgprValuC+58] // pack two bf16 to dword
buffer_store_dwordx2 v[56:57], v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v60                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+64], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v60, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v61                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+66], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v61, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+67], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_mov_b32 v65, v5
v_mov_b32 v66, v6
v_mov_b32 v67, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+64], 16, v[vgprValuC+64] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_and_or_b32 v64, v[vgprValuC+65], v10, v[vgprValuC+64] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+66], v[vgprValuC+66] // check Nan
v_bfe_u32 v9, v[vgprValuC+66], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+66], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+66], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+66], 16, v[vgprValuC+66] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+67], v[vgprValuC+67] // check Nan
v_bfe_u32 v9, v[vgprValuC+67], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+67], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+67], v9, v11, s[60:61]
v_and_or_b32 v65, v[vgprValuC+67], v10, v[vgprValuC+66] // pack two bf16 to dword
buffer_store_dwordx2 v[64:65], v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v70                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+72], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v70, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+73], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v71                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+74], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v71, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+74:vgprValuC+74+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_mov_b32 v73, v5
v_mov_b32 v74, v6
v_mov_b32 v75, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+72], 16, v[vgprValuC+72] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+73], v[vgprValuC+73] // check Nan
v_bfe_u32 v9, v[vgprValuC+73], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+73], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+73], v9, v11, s[60:61]
v_and_or_b32 v72, v[vgprValuC+73], v10, v[vgprValuC+72] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+74], v[vgprValuC+74] // check Nan
v_bfe_u32 v9, v[vgprValuC+74], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+74], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+74], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+74], 16, v[vgprValuC+74] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_and_or_b32 v73, v[vgprValuC+75], v10, v[vgprValuC+74] // pack two bf16 to dword
buffer_store_dwordx2 v[72:73], v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v78                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v78, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+81], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+82], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v79, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+83], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_mov_b32 v81, v5
v_mov_b32 v82, v6
v_mov_b32 v83, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+80], 16, v[vgprValuC+80] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+81], v[vgprValuC+81] // check Nan
v_bfe_u32 v9, v[vgprValuC+81], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+81], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+81], v9, v11, s[60:61]
v_and_or_b32 v80, v[vgprValuC+81], v10, v[vgprValuC+80] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+82], v[vgprValuC+82] // check Nan
v_bfe_u32 v9, v[vgprValuC+82], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+82], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+82], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+82], 16, v[vgprValuC+82] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+83], v[vgprValuC+83] // check Nan
v_bfe_u32 v9, v[vgprValuC+83], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+83], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+83], v9, v11, s[60:61]
v_and_or_b32 v81, v[vgprValuC+83], v10, v[vgprValuC+82] // pack two bf16 to dword
buffer_store_dwordx2 v[80:81], v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[24:25], v[vgprValuC+92:vgprValuC+92+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[26:27], v[vgprValuC+94:vgprValuC+94+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v88                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+92], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v88, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+93], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+94], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v89, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+94:vgprValuC+94+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v92, v4
v_mov_b32 v93, v5
v_mov_b32 v94, v6
v_mov_b32 v95, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+92], v[vgprValuC+92] // check Nan
v_bfe_u32 v9, v[vgprValuC+92], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+92], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+92], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+92], 16, v[vgprValuC+92] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+93], v[vgprValuC+93] // check Nan
v_bfe_u32 v9, v[vgprValuC+93], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+93], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+93], v9, v11, s[60:61]
v_and_or_b32 v92, v[vgprValuC+93], v10, v[vgprValuC+92] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+94], v[vgprValuC+94] // check Nan
v_bfe_u32 v9, v[vgprValuC+94], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+94], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+94], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+94], 16, v[vgprValuC+94] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_and_or_b32 v93, v[vgprValuC+95], v10, v[vgprValuC+94] // pack two bf16 to dword
buffer_store_dwordx2 v[92:93], v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[24:25], v[vgprValuC+100:vgprValuC+100+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[26:27], v[vgprValuC+102:vgprValuC+102+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v96                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v96, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+101], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v97                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+102], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v97, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+103], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+102:vgprValuC+102+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_mov_b32 v101, v5
v_mov_b32 v102, v6
v_mov_b32 v103, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+100], 16, v[vgprValuC+100] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+101], v[vgprValuC+101] // check Nan
v_bfe_u32 v9, v[vgprValuC+101], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+101], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+101], v9, v11, s[60:61]
v_and_or_b32 v100, v[vgprValuC+101], v10, v[vgprValuC+100] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+102], v[vgprValuC+102] // check Nan
v_bfe_u32 v9, v[vgprValuC+102], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+102], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+102], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+102], 16, v[vgprValuC+102] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+103], v[vgprValuC+103] // check Nan
v_bfe_u32 v9, v[vgprValuC+103], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+103], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+103], v9, v11, s[60:61]
v_and_or_b32 v101, v[vgprValuC+103], v10, v[vgprValuC+102] // pack two bf16 to dword
buffer_store_dwordx2 v[100:101], v87, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[24:25], v[vgprValuC+108:vgprValuC+108+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[26:27], v[vgprValuC+110:vgprValuC+110+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v106                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+108], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v106, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+109], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v107                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+110], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v107, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+111], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+110:vgprValuC+110+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v108, v4
v_mov_b32 v109, v5
v_mov_b32 v110, v6
v_mov_b32 v111, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+108], v[vgprValuC+108] // check Nan
v_bfe_u32 v9, v[vgprValuC+108], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+108], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+108], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+108], 16, v[vgprValuC+108] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+109], v[vgprValuC+109] // check Nan
v_bfe_u32 v9, v[vgprValuC+109], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+109], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+109], v9, v11, s[60:61]
v_and_or_b32 v108, v[vgprValuC+109], v10, v[vgprValuC+108] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+110], v[vgprValuC+110] // check Nan
v_bfe_u32 v9, v[vgprValuC+110], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+110], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+110], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+110], 16, v[vgprValuC+110] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+111], v[vgprValuC+111] // check Nan
v_bfe_u32 v9, v[vgprValuC+111], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+111], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+111], v9, v11, s[60:61]
v_and_or_b32 v109, v[vgprValuC+111], v10, v[vgprValuC+110] // pack two bf16 to dword
buffer_store_dwordx2 v[108:109], v98, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[24:25], v[vgprValuC+116:vgprValuC+116+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[26:27], v[vgprValuC+118:vgprValuC+118+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v114                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+116], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v114, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+117], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v115                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+118], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v115, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+119], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+118:vgprValuC+118+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v116, v4
v_mov_b32 v117, v5
v_mov_b32 v118, v6
v_mov_b32 v119, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+116], v[vgprValuC+116] // check Nan
v_bfe_u32 v9, v[vgprValuC+116], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+116], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+116], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+116], 16, v[vgprValuC+116] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+117], v[vgprValuC+117] // check Nan
v_bfe_u32 v9, v[vgprValuC+117], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+117], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+117], v9, v11, s[60:61]
v_and_or_b32 v116, v[vgprValuC+117], v10, v[vgprValuC+116] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+118], v[vgprValuC+118] // check Nan
v_bfe_u32 v9, v[vgprValuC+118], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+118], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+118], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+118], 16, v[vgprValuC+118] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+119], v[vgprValuC+119] // check Nan
v_bfe_u32 v9, v[vgprValuC+119], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+119], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+119], v9, v11, s[60:61]
v_and_or_b32 v117, v[vgprValuC+119], v10, v[vgprValuC+118] // pack two bf16 to dword
buffer_store_dwordx2 v[116:117], v105, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+128:vgprValuC+128+1], v[24:25], v[vgprValuC+128:vgprValuC+128+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+130:vgprValuC+130+1], v[26:27], v[vgprValuC+130:vgprValuC+130+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v124                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+128], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v124, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+129], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v125                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+130], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v125, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+131], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+128:vgprValuC+128+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+130:vgprValuC+130+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v128, v4
v_mov_b32 v129, v5
v_mov_b32 v130, v6
v_mov_b32 v131, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+128], v[vgprValuC+128] // check Nan
v_bfe_u32 v9, v[vgprValuC+128], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+128], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+128], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+128], 16, v[vgprValuC+128] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+129], v[vgprValuC+129] // check Nan
v_bfe_u32 v9, v[vgprValuC+129], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+129], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+129], v9, v11, s[60:61]
v_and_or_b32 v128, v[vgprValuC+129], v10, v[vgprValuC+128] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+130], v[vgprValuC+130] // check Nan
v_bfe_u32 v9, v[vgprValuC+130], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+130], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+130], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+130], 16, v[vgprValuC+130] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+131], v[vgprValuC+131] // check Nan
v_bfe_u32 v9, v[vgprValuC+131], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+131], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+131], v9, v11, s[60:61]
v_and_or_b32 v129, v[vgprValuC+131], v10, v[vgprValuC+130] // pack two bf16 to dword
buffer_store_dwordx2 v[128:129], v120, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (3,0,0,0:vw4); (3,0,1,0:vw4); (3,0,2,0:vw4); (3,0,3,0:vw4); (4,0,0,0:vw4); (4,0,1,0:vw4); (4,0,2,0:vw4); (4,0,3,0:vw4); (5,0,0,0:vw4); (5,0,1,0:vw4); (5,0,2,0:vw4); (5,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v123, BufferOOB
/* (d1,vc1,d0,vc0)=(3,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v123, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[16:17], v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v123, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b128 v[20:23], v14 offset:0                // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[24:27], v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v123, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v18, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v123, v18, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[34:35], v18, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v19, v0, s60
v_lshlrev_b32 v19, 0x2, v19                        // Bias address scaled by BPE
v_cndmask_b32 v19, v123, v19, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v32, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v18, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v123, v18, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v33, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v33, v123, v33, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[42:43], v33, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s60
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v123, v40, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v33, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v33, v123, v33, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v48, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v123, v48, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[52:53], v48, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v0, s60
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
v_cndmask_b32 v49, v123, v49, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v50, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v48, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v123, v48, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v123, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[60:61], v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v123, v54, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v123, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v62, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v123, v62, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[70:71], v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v0, s60
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
v_cndmask_b32 v63, v123, v63, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v62, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v123, v62, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v69, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v123, v69, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[78:79], v69, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v76, v0, s60
v_lshlrev_b32 v76, 0x2, v76                        // Bias address scaled by BPE
v_cndmask_b32 v76, v123, v76, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v77, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v123, v69, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v84, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v123, v84, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[88:89], v84, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v85, v0, s60
v_lshlrev_b32 v85, 0x2, v85                        // Bias address scaled by BPE
v_cndmask_b32 v85, v123, v85, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v86, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v84, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v123, v84, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v87, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v123, v87, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[96:97], v87, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v90, v0, s60
v_lshlrev_b32 v90, 0x2, v90                        // Bias address scaled by BPE
v_cndmask_b32 v90, v123, v90, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v91, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v87, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v123, v87, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v98, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v123, v98, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[106:107], v98, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v99, v0, s60
v_lshlrev_b32 v99, 0x2, v99                        // Bias address scaled by BPE
v_cndmask_b32 v99, v123, v99, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v104, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v98, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v123, v98, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v105, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v105, v123, v105, s[64:65]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[114:115], v105, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v112, v0, s60
v_lshlrev_b32 v112, 0x2, v112                      // Bias address scaled by BPE
v_cndmask_b32 v112, v123, v112, s[64:65]           // LDBias clip if OOB. offset
v_lshlrev_b32 v113, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v105, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v105, v123, v105, s[64:65]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v120, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v120, v123, v120, s[64:65]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[124:125], v120, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v121, v0, s60
v_lshlrev_b32 v121, 0x2, v121                      // Bias address scaled by BPE
v_cndmask_b32 v121, v123, v121, s[64:65]           // LDBias clip if OOB. offset
v_lshlrev_b32 v122, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v120, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v120, v123, v120, s[64:65]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+28], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+29], acc52          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+30], acc56          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+31], acc60          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+36], acc49          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+37], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+38], acc57          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+39], acc61          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+44], acc50          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+45], acc54          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+46], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+47], acc62          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+56], acc51          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+57], acc55          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+58], acc59          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+59], acc63          // copy acc to vreg[63]
v_accvgpr_read_b32 v[vgprValuC+64], acc64          // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+65], acc68          // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+66], acc72          // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+67], acc76          // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+72], acc65          // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+73], acc69          // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+74], acc73          // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+75], acc77          // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+80], acc66          // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+81], acc70          // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+82], acc74          // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+83], acc78          // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+92], acc67          // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+93], acc71          // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+94], acc75          // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+95], acc79          // copy acc to vreg[79]
v_accvgpr_read_b32 v[vgprValuC+100], acc80         // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+101], acc84         // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+102], acc88         // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+103], acc92         // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+108], acc81         // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+109], acc85         // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+110], acc89         // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+111], acc93         // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+116], acc82         // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+117], acc86         // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+118], acc90         // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+119], acc94         // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+128], acc83         // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+129], acc87         // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+130], acc91         // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+131], acc95         // copy acc to vreg[95]

/* rC *= alpha batchElements=[(3, 0, 0, 0), (3, 0, 1, 0), (3, 0, 2, 0), (3, 0, 3, 0), (4, 0, 0, 0), (4, 0, 1, 0), (4, 0, 2, 0), (4, 0, 3, 0), (5, 0, 0, 0), (5, 0, 1, 0), (5, 0, 2, 0), (5, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
v_mul_f32 v[vgprValuC+128], s[sgprAlpha], v[vgprValuC+128] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+130], s[sgprAlpha], v[vgprValuC+130] // *= alpha
v_mul_f32 v[vgprValuC+131], s[sgprAlpha], v[vgprValuC+131] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+28], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v16, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+29], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v17                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+30], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v17, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+31], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v34                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+36], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v34, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+37], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v35                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+38], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v35, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+39], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
buffer_store_dwordx2 v[36:37], v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[24:25], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[26:27], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v42                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+44], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v42, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v43                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+46], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v43, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+47], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_mov_b32 v45, v5
v_mov_b32 v46, v6
v_mov_b32 v47, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+44], 16, v[vgprValuC+44] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_and_or_b32 v44, v[vgprValuC+45], v10, v[vgprValuC+44] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+46], v[vgprValuC+46] // check Nan
v_bfe_u32 v9, v[vgprValuC+46], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+46], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+46], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+46], 16, v[vgprValuC+46] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+47], v[vgprValuC+47] // check Nan
v_bfe_u32 v9, v[vgprValuC+47], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+47], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+47], v9, v11, s[60:61]
v_and_or_b32 v45, v[vgprValuC+47], v10, v[vgprValuC+46] // pack two bf16 to dword
buffer_store_dwordx2 v[44:45], v33, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[24:25], v[vgprValuC+56:vgprValuC+56+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[26:27], v[vgprValuC+58:vgprValuC+58+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v52                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+56], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v52, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+57], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v53                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+58], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v53, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+59], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+58:vgprValuC+58+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_mov_b32 v57, v5
v_mov_b32 v58, v6
v_mov_b32 v59, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+56], 16, v[vgprValuC+56] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+57], v[vgprValuC+57] // check Nan
v_bfe_u32 v9, v[vgprValuC+57], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+57], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+57], v9, v11, s[60:61]
v_and_or_b32 v56, v[vgprValuC+57], v10, v[vgprValuC+56] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+58], v[vgprValuC+58] // check Nan
v_bfe_u32 v9, v[vgprValuC+58], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+58], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+58], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+58], 16, v[vgprValuC+58] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+59], v[vgprValuC+59] // check Nan
v_bfe_u32 v9, v[vgprValuC+59], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+59], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+59], v9, v11, s[60:61]
v_and_or_b32 v57, v[vgprValuC+59], v10, v[vgprValuC+58] // pack two bf16 to dword
buffer_store_dwordx2 v[56:57], v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v60                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+64], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v60, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v61                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+66], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v61, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+67], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_mov_b32 v65, v5
v_mov_b32 v66, v6
v_mov_b32 v67, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+64], 16, v[vgprValuC+64] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_and_or_b32 v64, v[vgprValuC+65], v10, v[vgprValuC+64] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+66], v[vgprValuC+66] // check Nan
v_bfe_u32 v9, v[vgprValuC+66], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+66], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+66], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+66], 16, v[vgprValuC+66] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+67], v[vgprValuC+67] // check Nan
v_bfe_u32 v9, v[vgprValuC+67], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+67], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+67], v9, v11, s[60:61]
v_and_or_b32 v65, v[vgprValuC+67], v10, v[vgprValuC+66] // pack two bf16 to dword
buffer_store_dwordx2 v[64:65], v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v70                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+72], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v70, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+73], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v71                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+74], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v71, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+74:vgprValuC+74+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_mov_b32 v73, v5
v_mov_b32 v74, v6
v_mov_b32 v75, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+72], 16, v[vgprValuC+72] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+73], v[vgprValuC+73] // check Nan
v_bfe_u32 v9, v[vgprValuC+73], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+73], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+73], v9, v11, s[60:61]
v_and_or_b32 v72, v[vgprValuC+73], v10, v[vgprValuC+72] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+74], v[vgprValuC+74] // check Nan
v_bfe_u32 v9, v[vgprValuC+74], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+74], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+74], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+74], 16, v[vgprValuC+74] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_and_or_b32 v73, v[vgprValuC+75], v10, v[vgprValuC+74] // pack two bf16 to dword
buffer_store_dwordx2 v[72:73], v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v78                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v78, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+81], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+82], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v79, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+83], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_mov_b32 v81, v5
v_mov_b32 v82, v6
v_mov_b32 v83, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+80], 16, v[vgprValuC+80] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+81], v[vgprValuC+81] // check Nan
v_bfe_u32 v9, v[vgprValuC+81], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+81], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+81], v9, v11, s[60:61]
v_and_or_b32 v80, v[vgprValuC+81], v10, v[vgprValuC+80] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+82], v[vgprValuC+82] // check Nan
v_bfe_u32 v9, v[vgprValuC+82], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+82], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+82], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+82], 16, v[vgprValuC+82] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+83], v[vgprValuC+83] // check Nan
v_bfe_u32 v9, v[vgprValuC+83], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+83], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+83], v9, v11, s[60:61]
v_and_or_b32 v81, v[vgprValuC+83], v10, v[vgprValuC+82] // pack two bf16 to dword
buffer_store_dwordx2 v[80:81], v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[24:25], v[vgprValuC+92:vgprValuC+92+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[26:27], v[vgprValuC+94:vgprValuC+94+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v88                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+92], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v88, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+93], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+94], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v89, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+94:vgprValuC+94+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v92, v4
v_mov_b32 v93, v5
v_mov_b32 v94, v6
v_mov_b32 v95, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+92], v[vgprValuC+92] // check Nan
v_bfe_u32 v9, v[vgprValuC+92], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+92], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+92], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+92], 16, v[vgprValuC+92] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+93], v[vgprValuC+93] // check Nan
v_bfe_u32 v9, v[vgprValuC+93], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+93], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+93], v9, v11, s[60:61]
v_and_or_b32 v92, v[vgprValuC+93], v10, v[vgprValuC+92] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+94], v[vgprValuC+94] // check Nan
v_bfe_u32 v9, v[vgprValuC+94], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+94], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+94], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+94], 16, v[vgprValuC+94] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_and_or_b32 v93, v[vgprValuC+95], v10, v[vgprValuC+94] // pack two bf16 to dword
buffer_store_dwordx2 v[92:93], v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[24:25], v[vgprValuC+100:vgprValuC+100+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[26:27], v[vgprValuC+102:vgprValuC+102+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v96                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v96, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+101], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v97                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+102], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v97, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+103], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+102:vgprValuC+102+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_mov_b32 v101, v5
v_mov_b32 v102, v6
v_mov_b32 v103, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+100], 16, v[vgprValuC+100] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+101], v[vgprValuC+101] // check Nan
v_bfe_u32 v9, v[vgprValuC+101], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+101], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+101], v9, v11, s[60:61]
v_and_or_b32 v100, v[vgprValuC+101], v10, v[vgprValuC+100] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+102], v[vgprValuC+102] // check Nan
v_bfe_u32 v9, v[vgprValuC+102], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+102], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+102], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+102], 16, v[vgprValuC+102] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+103], v[vgprValuC+103] // check Nan
v_bfe_u32 v9, v[vgprValuC+103], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+103], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+103], v9, v11, s[60:61]
v_and_or_b32 v101, v[vgprValuC+103], v10, v[vgprValuC+102] // pack two bf16 to dword
buffer_store_dwordx2 v[100:101], v87, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[24:25], v[vgprValuC+108:vgprValuC+108+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[26:27], v[vgprValuC+110:vgprValuC+110+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v106                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+108], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v106, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+109], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v107                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+110], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v107, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+111], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+110:vgprValuC+110+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v108, v4
v_mov_b32 v109, v5
v_mov_b32 v110, v6
v_mov_b32 v111, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+108], v[vgprValuC+108] // check Nan
v_bfe_u32 v9, v[vgprValuC+108], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+108], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+108], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+108], 16, v[vgprValuC+108] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+109], v[vgprValuC+109] // check Nan
v_bfe_u32 v9, v[vgprValuC+109], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+109], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+109], v9, v11, s[60:61]
v_and_or_b32 v108, v[vgprValuC+109], v10, v[vgprValuC+108] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+110], v[vgprValuC+110] // check Nan
v_bfe_u32 v9, v[vgprValuC+110], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+110], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+110], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+110], 16, v[vgprValuC+110] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+111], v[vgprValuC+111] // check Nan
v_bfe_u32 v9, v[vgprValuC+111], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+111], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+111], v9, v11, s[60:61]
v_and_or_b32 v109, v[vgprValuC+111], v10, v[vgprValuC+110] // pack two bf16 to dword
buffer_store_dwordx2 v[108:109], v98, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[24:25], v[vgprValuC+116:vgprValuC+116+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[26:27], v[vgprValuC+118:vgprValuC+118+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v114                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+116], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v114, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+117], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v115                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+118], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v115, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+119], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+118:vgprValuC+118+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v116, v4
v_mov_b32 v117, v5
v_mov_b32 v118, v6
v_mov_b32 v119, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+116], v[vgprValuC+116] // check Nan
v_bfe_u32 v9, v[vgprValuC+116], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+116], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+116], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+116], 16, v[vgprValuC+116] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+117], v[vgprValuC+117] // check Nan
v_bfe_u32 v9, v[vgprValuC+117], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+117], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+117], v9, v11, s[60:61]
v_and_or_b32 v116, v[vgprValuC+117], v10, v[vgprValuC+116] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+118], v[vgprValuC+118] // check Nan
v_bfe_u32 v9, v[vgprValuC+118], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+118], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+118], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+118], 16, v[vgprValuC+118] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+119], v[vgprValuC+119] // check Nan
v_bfe_u32 v9, v[vgprValuC+119], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+119], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+119], v9, v11, s[60:61]
v_and_or_b32 v117, v[vgprValuC+119], v10, v[vgprValuC+118] // pack two bf16 to dword
buffer_store_dwordx2 v[116:117], v105, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+128:vgprValuC+128+1], v[24:25], v[vgprValuC+128:vgprValuC+128+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+130:vgprValuC+130+1], v[26:27], v[vgprValuC+130:vgprValuC+130+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v124                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+128], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v124, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+129], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v125                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+130], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v125, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+131], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+128:vgprValuC+128+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+130:vgprValuC+130+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v128, v4
v_mov_b32 v129, v5
v_mov_b32 v130, v6
v_mov_b32 v131, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+128], v[vgprValuC+128] // check Nan
v_bfe_u32 v9, v[vgprValuC+128], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+128], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+128], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+128], 16, v[vgprValuC+128] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+129], v[vgprValuC+129] // check Nan
v_bfe_u32 v9, v[vgprValuC+129], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+129], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+129], v9, v11, s[60:61]
v_and_or_b32 v128, v[vgprValuC+129], v10, v[vgprValuC+128] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+130], v[vgprValuC+130] // check Nan
v_bfe_u32 v9, v[vgprValuC+130], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+130], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+130], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+130], 16, v[vgprValuC+130] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+131], v[vgprValuC+131] // check Nan
v_bfe_u32 v9, v[vgprValuC+131], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+131], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+131], v9, v11, s[60:61]
v_and_or_b32 v129, v[vgprValuC+131], v10, v[vgprValuC+130] // pack two bf16 to dword
buffer_store_dwordx2 v[128:129], v120, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #2 (d1,d0,vc1,vc0) = */
/*    (6,0,0,0:vw4); (6,0,1,0:vw4); (6,0,2,0:vw4); (6,0,3,0:vw4); (7,0,0,0:vw4); (7,0,1,0:vw4); (7,0,2,0:vw4); (7,0,3,0:vw4); (8,0,0,0:vw4); (8,0,1,0:vw4); (8,0,2,0:vw4); (8,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v123, BufferOOB
/* (d1,vc1,d0,vc0)=(6,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v123, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[16:17], v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v123, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b128 v[20:23], v14 offset:0                // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[24:27], v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v123, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v18, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v123, v18, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[34:35], v18, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v19, v0, s60
v_lshlrev_b32 v19, 0x2, v19                        // Bias address scaled by BPE
v_cndmask_b32 v19, v123, v19, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v32, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v18, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v123, v18, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v33, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v33, v123, v33, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[42:43], v33, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s60
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v123, v40, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v33, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v33, v123, v33, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v48, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v123, v48, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[52:53], v48, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v0, s60
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
v_cndmask_b32 v49, v123, v49, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v50, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v48, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v123, v48, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v123, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[60:61], v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v123, v54, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v123, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v62, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v123, v62, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[70:71], v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v0, s60
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
v_cndmask_b32 v63, v123, v63, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v62, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v123, v62, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v69, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v123, v69, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[78:79], v69, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v76, v0, s60
v_lshlrev_b32 v76, 0x2, v76                        // Bias address scaled by BPE
v_cndmask_b32 v76, v123, v76, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v77, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v123, v69, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v84, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v123, v84, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[88:89], v84, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v85, v0, s60
v_lshlrev_b32 v85, 0x2, v85                        // Bias address scaled by BPE
v_cndmask_b32 v85, v123, v85, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v86, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v84, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v123, v84, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v87, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v123, v87, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[96:97], v87, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v90, v0, s60
v_lshlrev_b32 v90, 0x2, v90                        // Bias address scaled by BPE
v_cndmask_b32 v90, v123, v90, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v91, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v87, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v123, v87, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v98, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v123, v98, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[106:107], v98, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v99, v0, s60
v_lshlrev_b32 v99, 0x2, v99                        // Bias address scaled by BPE
v_cndmask_b32 v99, v123, v99, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v104, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v98, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v123, v98, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v105, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v105, v123, v105, s[64:65]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[114:115], v105, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v112, v0, s60
v_lshlrev_b32 v112, 0x2, v112                      // Bias address scaled by BPE
v_cndmask_b32 v112, v123, v112, s[64:65]           // LDBias clip if OOB. offset
v_lshlrev_b32 v113, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v105, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v105, v123, v105, s[64:65]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v120, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v120, v123, v120, s[64:65]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[124:125], v120, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v121, v0, s60
v_lshlrev_b32 v121, 0x2, v121                      // Bias address scaled by BPE
v_cndmask_b32 v121, v123, v121, s[64:65]           // LDBias clip if OOB. offset
v_lshlrev_b32 v122, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v120, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v120, v123, v120, s[64:65]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+28], acc96          // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+29], acc100         // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+30], acc104         // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+31], acc108         // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+36], acc97          // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+37], acc101         // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+38], acc105         // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+39], acc109         // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+44], acc98          // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+45], acc102         // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+46], acc106         // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+47], acc110         // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+56], acc99          // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+57], acc103         // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+58], acc107         // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+59], acc111         // copy acc to vreg[111]
v_accvgpr_read_b32 v[vgprValuC+64], acc112         // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+65], acc116         // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+66], acc120         // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+67], acc124         // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+72], acc113         // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+73], acc117         // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+74], acc121         // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+75], acc125         // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+80], acc114         // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+81], acc118         // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+82], acc122         // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+83], acc126         // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+92], acc115         // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+93], acc119         // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+94], acc123         // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+95], acc127         // copy acc to vreg[127]
v_accvgpr_read_b32 v[vgprValuC+100], acc128        // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+101], acc132        // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+102], acc136        // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+103], acc140        // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+108], acc129        // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+109], acc133        // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+110], acc137        // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+111], acc141        // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+116], acc130        // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+117], acc134        // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+118], acc138        // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+119], acc142        // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+128], acc131        // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+129], acc135        // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+130], acc139        // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+131], acc143        // copy acc to vreg[143]

/* rC *= alpha batchElements=[(6, 0, 0, 0), (6, 0, 1, 0), (6, 0, 2, 0), (6, 0, 3, 0), (7, 0, 0, 0), (7, 0, 1, 0), (7, 0, 2, 0), (7, 0, 3, 0), (8, 0, 0, 0), (8, 0, 1, 0), (8, 0, 2, 0), (8, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
v_mul_f32 v[vgprValuC+128], s[sgprAlpha], v[vgprValuC+128] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+130], s[sgprAlpha], v[vgprValuC+130] // *= alpha
v_mul_f32 v[vgprValuC+131], s[sgprAlpha], v[vgprValuC+131] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+28], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v16, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+29], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v17                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+30], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v17, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+31], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v34                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+36], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v34, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+37], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v35                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+38], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v35, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+39], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
buffer_store_dwordx2 v[36:37], v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[24:25], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[26:27], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v42                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+44], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v42, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v43                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+46], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v43, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+47], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_mov_b32 v45, v5
v_mov_b32 v46, v6
v_mov_b32 v47, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+44], 16, v[vgprValuC+44] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_and_or_b32 v44, v[vgprValuC+45], v10, v[vgprValuC+44] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+46], v[vgprValuC+46] // check Nan
v_bfe_u32 v9, v[vgprValuC+46], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+46], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+46], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+46], 16, v[vgprValuC+46] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+47], v[vgprValuC+47] // check Nan
v_bfe_u32 v9, v[vgprValuC+47], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+47], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+47], v9, v11, s[60:61]
v_and_or_b32 v45, v[vgprValuC+47], v10, v[vgprValuC+46] // pack two bf16 to dword
buffer_store_dwordx2 v[44:45], v33, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[24:25], v[vgprValuC+56:vgprValuC+56+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[26:27], v[vgprValuC+58:vgprValuC+58+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v52                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+56], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v52, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+57], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v53                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+58], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v53, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+59], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+58:vgprValuC+58+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_mov_b32 v57, v5
v_mov_b32 v58, v6
v_mov_b32 v59, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+56], 16, v[vgprValuC+56] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+57], v[vgprValuC+57] // check Nan
v_bfe_u32 v9, v[vgprValuC+57], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+57], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+57], v9, v11, s[60:61]
v_and_or_b32 v56, v[vgprValuC+57], v10, v[vgprValuC+56] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+58], v[vgprValuC+58] // check Nan
v_bfe_u32 v9, v[vgprValuC+58], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+58], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+58], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+58], 16, v[vgprValuC+58] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+59], v[vgprValuC+59] // check Nan
v_bfe_u32 v9, v[vgprValuC+59], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+59], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+59], v9, v11, s[60:61]
v_and_or_b32 v57, v[vgprValuC+59], v10, v[vgprValuC+58] // pack two bf16 to dword
buffer_store_dwordx2 v[56:57], v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v60                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+64], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v60, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v61                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+66], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v61, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+67], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_mov_b32 v65, v5
v_mov_b32 v66, v6
v_mov_b32 v67, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+64], 16, v[vgprValuC+64] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_and_or_b32 v64, v[vgprValuC+65], v10, v[vgprValuC+64] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+66], v[vgprValuC+66] // check Nan
v_bfe_u32 v9, v[vgprValuC+66], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+66], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+66], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+66], 16, v[vgprValuC+66] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+67], v[vgprValuC+67] // check Nan
v_bfe_u32 v9, v[vgprValuC+67], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+67], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+67], v9, v11, s[60:61]
v_and_or_b32 v65, v[vgprValuC+67], v10, v[vgprValuC+66] // pack two bf16 to dword
buffer_store_dwordx2 v[64:65], v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v70                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+72], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v70, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+73], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v71                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+74], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v71, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+74:vgprValuC+74+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_mov_b32 v73, v5
v_mov_b32 v74, v6
v_mov_b32 v75, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+72], 16, v[vgprValuC+72] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+73], v[vgprValuC+73] // check Nan
v_bfe_u32 v9, v[vgprValuC+73], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+73], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+73], v9, v11, s[60:61]
v_and_or_b32 v72, v[vgprValuC+73], v10, v[vgprValuC+72] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+74], v[vgprValuC+74] // check Nan
v_bfe_u32 v9, v[vgprValuC+74], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+74], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+74], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+74], 16, v[vgprValuC+74] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_and_or_b32 v73, v[vgprValuC+75], v10, v[vgprValuC+74] // pack two bf16 to dword
buffer_store_dwordx2 v[72:73], v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v78                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v78, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+81], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+82], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v79, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+83], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_mov_b32 v81, v5
v_mov_b32 v82, v6
v_mov_b32 v83, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+80], 16, v[vgprValuC+80] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+81], v[vgprValuC+81] // check Nan
v_bfe_u32 v9, v[vgprValuC+81], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+81], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+81], v9, v11, s[60:61]
v_and_or_b32 v80, v[vgprValuC+81], v10, v[vgprValuC+80] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+82], v[vgprValuC+82] // check Nan
v_bfe_u32 v9, v[vgprValuC+82], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+82], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+82], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+82], 16, v[vgprValuC+82] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+83], v[vgprValuC+83] // check Nan
v_bfe_u32 v9, v[vgprValuC+83], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+83], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+83], v9, v11, s[60:61]
v_and_or_b32 v81, v[vgprValuC+83], v10, v[vgprValuC+82] // pack two bf16 to dword
buffer_store_dwordx2 v[80:81], v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[24:25], v[vgprValuC+92:vgprValuC+92+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[26:27], v[vgprValuC+94:vgprValuC+94+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v88                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+92], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v88, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+93], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+94], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v89, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+94:vgprValuC+94+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v92, v4
v_mov_b32 v93, v5
v_mov_b32 v94, v6
v_mov_b32 v95, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+92], v[vgprValuC+92] // check Nan
v_bfe_u32 v9, v[vgprValuC+92], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+92], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+92], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+92], 16, v[vgprValuC+92] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+93], v[vgprValuC+93] // check Nan
v_bfe_u32 v9, v[vgprValuC+93], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+93], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+93], v9, v11, s[60:61]
v_and_or_b32 v92, v[vgprValuC+93], v10, v[vgprValuC+92] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+94], v[vgprValuC+94] // check Nan
v_bfe_u32 v9, v[vgprValuC+94], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+94], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+94], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+94], 16, v[vgprValuC+94] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_and_or_b32 v93, v[vgprValuC+95], v10, v[vgprValuC+94] // pack two bf16 to dword
buffer_store_dwordx2 v[92:93], v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[24:25], v[vgprValuC+100:vgprValuC+100+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[26:27], v[vgprValuC+102:vgprValuC+102+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v96                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v96, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+101], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v97                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+102], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v97, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+103], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+102:vgprValuC+102+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_mov_b32 v101, v5
v_mov_b32 v102, v6
v_mov_b32 v103, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+100], 16, v[vgprValuC+100] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+101], v[vgprValuC+101] // check Nan
v_bfe_u32 v9, v[vgprValuC+101], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+101], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+101], v9, v11, s[60:61]
v_and_or_b32 v100, v[vgprValuC+101], v10, v[vgprValuC+100] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+102], v[vgprValuC+102] // check Nan
v_bfe_u32 v9, v[vgprValuC+102], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+102], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+102], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+102], 16, v[vgprValuC+102] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+103], v[vgprValuC+103] // check Nan
v_bfe_u32 v9, v[vgprValuC+103], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+103], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+103], v9, v11, s[60:61]
v_and_or_b32 v101, v[vgprValuC+103], v10, v[vgprValuC+102] // pack two bf16 to dword
buffer_store_dwordx2 v[100:101], v87, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[24:25], v[vgprValuC+108:vgprValuC+108+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[26:27], v[vgprValuC+110:vgprValuC+110+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v106                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+108], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v106, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+109], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v107                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+110], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v107, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+111], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+110:vgprValuC+110+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v108, v4
v_mov_b32 v109, v5
v_mov_b32 v110, v6
v_mov_b32 v111, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+108], v[vgprValuC+108] // check Nan
v_bfe_u32 v9, v[vgprValuC+108], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+108], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+108], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+108], 16, v[vgprValuC+108] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+109], v[vgprValuC+109] // check Nan
v_bfe_u32 v9, v[vgprValuC+109], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+109], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+109], v9, v11, s[60:61]
v_and_or_b32 v108, v[vgprValuC+109], v10, v[vgprValuC+108] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+110], v[vgprValuC+110] // check Nan
v_bfe_u32 v9, v[vgprValuC+110], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+110], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+110], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+110], 16, v[vgprValuC+110] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+111], v[vgprValuC+111] // check Nan
v_bfe_u32 v9, v[vgprValuC+111], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+111], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+111], v9, v11, s[60:61]
v_and_or_b32 v109, v[vgprValuC+111], v10, v[vgprValuC+110] // pack two bf16 to dword
buffer_store_dwordx2 v[108:109], v98, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[24:25], v[vgprValuC+116:vgprValuC+116+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[26:27], v[vgprValuC+118:vgprValuC+118+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v114                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+116], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v114, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+117], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v115                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+118], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v115, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+119], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+118:vgprValuC+118+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v116, v4
v_mov_b32 v117, v5
v_mov_b32 v118, v6
v_mov_b32 v119, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+116], v[vgprValuC+116] // check Nan
v_bfe_u32 v9, v[vgprValuC+116], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+116], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+116], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+116], 16, v[vgprValuC+116] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+117], v[vgprValuC+117] // check Nan
v_bfe_u32 v9, v[vgprValuC+117], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+117], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+117], v9, v11, s[60:61]
v_and_or_b32 v116, v[vgprValuC+117], v10, v[vgprValuC+116] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+118], v[vgprValuC+118] // check Nan
v_bfe_u32 v9, v[vgprValuC+118], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+118], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+118], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+118], 16, v[vgprValuC+118] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+119], v[vgprValuC+119] // check Nan
v_bfe_u32 v9, v[vgprValuC+119], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+119], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+119], v9, v11, s[60:61]
v_and_or_b32 v117, v[vgprValuC+119], v10, v[vgprValuC+118] // pack two bf16 to dword
buffer_store_dwordx2 v[116:117], v105, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+128:vgprValuC+128+1], v[24:25], v[vgprValuC+128:vgprValuC+128+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+130:vgprValuC+130+1], v[26:27], v[vgprValuC+130:vgprValuC+130+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v124                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+128], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v124, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+129], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v125                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+130], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v125, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+131], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+128:vgprValuC+128+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+130:vgprValuC+130+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v128, v4
v_mov_b32 v129, v5
v_mov_b32 v130, v6
v_mov_b32 v131, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+128], v[vgprValuC+128] // check Nan
v_bfe_u32 v9, v[vgprValuC+128], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+128], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+128], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+128], 16, v[vgprValuC+128] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+129], v[vgprValuC+129] // check Nan
v_bfe_u32 v9, v[vgprValuC+129], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+129], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+129], v9, v11, s[60:61]
v_and_or_b32 v128, v[vgprValuC+129], v10, v[vgprValuC+128] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+130], v[vgprValuC+130] // check Nan
v_bfe_u32 v9, v[vgprValuC+130], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+130], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+130], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+130], 16, v[vgprValuC+130] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+131], v[vgprValuC+131] // check Nan
v_bfe_u32 v9, v[vgprValuC+131], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+131], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+131], v9, v11, s[60:61]
v_and_or_b32 v129, v[vgprValuC+131], v10, v[vgprValuC+130] // pack two bf16 to dword
buffer_store_dwordx2 v[128:129], v120, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #3 (d1,d0,vc1,vc0) = */
/*    (9,0,0,0:vw4); (9,0,1,0:vw4); (9,0,2,0:vw4); (9,0,3,0:vw4); (10,0,0,0:vw4); (10,0,1,0:vw4); (10,0,2,0:vw4); (10,0,3,0:vw4); (11,0,0,0:vw4); (11,0,1,0:vw4); (11,0,2,0:vw4); (11,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v123, BufferOOB
/* (d1,vc1,d0,vc0)=(9,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v123, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[16:17], v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v123, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b128 v[20:23], v14 offset:0                // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[24:27], v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v123, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v18, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v123, v18, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[34:35], v18, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v19, v0, s60
v_lshlrev_b32 v19, 0x2, v19                        // Bias address scaled by BPE
v_cndmask_b32 v19, v123, v19, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v32, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v18, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v123, v18, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v33, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v33, v123, v33, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[42:43], v33, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s60
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v123, v40, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v33, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v33, v123, v33, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v48, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v123, v48, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[52:53], v48, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v0, s60
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
v_cndmask_b32 v49, v123, v49, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v50, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v48, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v123, v48, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v123, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[60:61], v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v123, v54, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v123, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v62, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v123, v62, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[70:71], v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v0, s60
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
v_cndmask_b32 v63, v123, v63, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v62, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v123, v62, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v69, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v123, v69, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[78:79], v69, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v76, v0, s60
v_lshlrev_b32 v76, 0x2, v76                        // Bias address scaled by BPE
v_cndmask_b32 v76, v123, v76, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v77, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v123, v69, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v84, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v123, v84, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[88:89], v84, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v85, v0, s60
v_lshlrev_b32 v85, 0x2, v85                        // Bias address scaled by BPE
v_cndmask_b32 v85, v123, v85, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v86, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v84, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v123, v84, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v87, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v123, v87, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[96:97], v87, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v90, v0, s60
v_lshlrev_b32 v90, 0x2, v90                        // Bias address scaled by BPE
v_cndmask_b32 v90, v123, v90, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v91, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v87, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v87, v123, v87, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v98, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v123, v98, s[64:65]             // LDC clip if OOB. offset
buffer_load_dwordx2 v[106:107], v98, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v99, v0, s60
v_lshlrev_b32 v99, 0x2, v99                        // Bias address scaled by BPE
v_cndmask_b32 v99, v123, v99, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v104, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v98, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v98, v123, v98, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v105, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v105, v123, v105, s[64:65]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[114:115], v105, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v112, v0, s60
v_lshlrev_b32 v112, 0x2, v112                      // Bias address scaled by BPE
v_cndmask_b32 v112, v123, v112, s[64:65]           // LDBias clip if OOB. offset
v_lshlrev_b32 v113, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v105, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v105, v123, v105, s[64:65]           // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v120, v2, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v120, v123, v120, s[64:65]           // LDC clip if OOB. offset
buffer_load_dwordx2 v[124:125], v120, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v121, v0, s60
v_lshlrev_b32 v121, 0x2, v121                      // Bias address scaled by BPE
v_cndmask_b32 v121, v123, v121, s[64:65]           // LDBias clip if OOB. offset
v_lshlrev_b32 v122, 0x2, v0                        // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v120, v3, v0, 0x1                   // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v120, v123, v120, s[64:65]           // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+28], acc144         // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+29], acc148         // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+30], acc152         // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+31], acc156         // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+36], acc145         // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+37], acc149         // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+38], acc153         // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+39], acc157         // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+44], acc146         // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+45], acc150         // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+46], acc154         // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+47], acc158         // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+56], acc147         // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+57], acc151         // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+58], acc155         // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+59], acc159         // copy acc to vreg[159]
v_accvgpr_read_b32 v[vgprValuC+64], acc160         // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+65], acc164         // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+66], acc168         // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+67], acc172         // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+72], acc161         // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+73], acc165         // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+74], acc169         // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+75], acc173         // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+80], acc162         // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+81], acc166         // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+82], acc170         // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+83], acc174         // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+92], acc163         // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+93], acc167         // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+94], acc171         // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+95], acc175         // copy acc to vreg[175]
v_accvgpr_read_b32 v[vgprValuC+100], acc176        // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+101], acc180        // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+102], acc184        // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+103], acc188        // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+108], acc177        // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+109], acc181        // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+110], acc185        // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+111], acc189        // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+116], acc178        // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+117], acc182        // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+118], acc186        // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+119], acc190        // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+128], acc179        // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+129], acc183        // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+130], acc187        // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+131], acc191        // copy acc to vreg[191]

/* rC *= alpha batchElements=[(9, 0, 0, 0), (9, 0, 1, 0), (9, 0, 2, 0), (9, 0, 3, 0), (10, 0, 0, 0), (10, 0, 1, 0), (10, 0, 2, 0), (10, 0, 3, 0), (11, 0, 0, 0), (11, 0, 1, 0), (11, 0, 2, 0), (11, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
v_mul_f32 v[vgprValuC+101], s[sgprAlpha], v[vgprValuC+101] // *= alpha
v_mul_f32 v[vgprValuC+102], s[sgprAlpha], v[vgprValuC+102] // *= alpha
v_mul_f32 v[vgprValuC+103], s[sgprAlpha], v[vgprValuC+103] // *= alpha
v_mul_f32 v[vgprValuC+108], s[sgprAlpha], v[vgprValuC+108] // *= alpha
v_mul_f32 v[vgprValuC+109], s[sgprAlpha], v[vgprValuC+109] // *= alpha
v_mul_f32 v[vgprValuC+110], s[sgprAlpha], v[vgprValuC+110] // *= alpha
v_mul_f32 v[vgprValuC+111], s[sgprAlpha], v[vgprValuC+111] // *= alpha
v_mul_f32 v[vgprValuC+116], s[sgprAlpha], v[vgprValuC+116] // *= alpha
v_mul_f32 v[vgprValuC+117], s[sgprAlpha], v[vgprValuC+117] // *= alpha
v_mul_f32 v[vgprValuC+118], s[sgprAlpha], v[vgprValuC+118] // *= alpha
v_mul_f32 v[vgprValuC+119], s[sgprAlpha], v[vgprValuC+119] // *= alpha
v_mul_f32 v[vgprValuC+128], s[sgprAlpha], v[vgprValuC+128] // *= alpha
v_mul_f32 v[vgprValuC+129], s[sgprAlpha], v[vgprValuC+129] // *= alpha
v_mul_f32 v[vgprValuC+130], s[sgprAlpha], v[vgprValuC+130] // *= alpha
v_mul_f32 v[vgprValuC+131], s[sgprAlpha], v[vgprValuC+131] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+28], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v16, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+29], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v17                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+30], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v17, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+31], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v34                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+36], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v34, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+37], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v35                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+38], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v35, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+39], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
buffer_store_dwordx2 v[36:37], v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[24:25], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[26:27], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v42                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+44], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v42, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v43                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+46], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v43, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+47], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_mov_b32 v45, v5
v_mov_b32 v46, v6
v_mov_b32 v47, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+44], 16, v[vgprValuC+44] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_and_or_b32 v44, v[vgprValuC+45], v10, v[vgprValuC+44] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+46], v[vgprValuC+46] // check Nan
v_bfe_u32 v9, v[vgprValuC+46], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+46], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+46], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+46], 16, v[vgprValuC+46] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+47], v[vgprValuC+47] // check Nan
v_bfe_u32 v9, v[vgprValuC+47], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+47], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+47], v9, v11, s[60:61]
v_and_or_b32 v45, v[vgprValuC+47], v10, v[vgprValuC+46] // pack two bf16 to dword
buffer_store_dwordx2 v[44:45], v33, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[24:25], v[vgprValuC+56:vgprValuC+56+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[26:27], v[vgprValuC+58:vgprValuC+58+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v52                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+56], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v52, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+57], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v53                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+58], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v53, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+59], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+58:vgprValuC+58+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_mov_b32 v57, v5
v_mov_b32 v58, v6
v_mov_b32 v59, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+56], 16, v[vgprValuC+56] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+57], v[vgprValuC+57] // check Nan
v_bfe_u32 v9, v[vgprValuC+57], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+57], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+57], v9, v11, s[60:61]
v_and_or_b32 v56, v[vgprValuC+57], v10, v[vgprValuC+56] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+58], v[vgprValuC+58] // check Nan
v_bfe_u32 v9, v[vgprValuC+58], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+58], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+58], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+58], 16, v[vgprValuC+58] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+59], v[vgprValuC+59] // check Nan
v_bfe_u32 v9, v[vgprValuC+59], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+59], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+59], v9, v11, s[60:61]
v_and_or_b32 v57, v[vgprValuC+59], v10, v[vgprValuC+58] // pack two bf16 to dword
buffer_store_dwordx2 v[56:57], v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v60                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+64], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v60, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v61                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+66], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v61, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+67], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_mov_b32 v65, v5
v_mov_b32 v66, v6
v_mov_b32 v67, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+64], 16, v[vgprValuC+64] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_and_or_b32 v64, v[vgprValuC+65], v10, v[vgprValuC+64] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+66], v[vgprValuC+66] // check Nan
v_bfe_u32 v9, v[vgprValuC+66], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+66], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+66], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+66], 16, v[vgprValuC+66] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+67], v[vgprValuC+67] // check Nan
v_bfe_u32 v9, v[vgprValuC+67], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+67], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+67], v9, v11, s[60:61]
v_and_or_b32 v65, v[vgprValuC+67], v10, v[vgprValuC+66] // pack two bf16 to dword
buffer_store_dwordx2 v[64:65], v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v70                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+72], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v70, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+73], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v71                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+74], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v71, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+74:vgprValuC+74+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_mov_b32 v73, v5
v_mov_b32 v74, v6
v_mov_b32 v75, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+72], 16, v[vgprValuC+72] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+73], v[vgprValuC+73] // check Nan
v_bfe_u32 v9, v[vgprValuC+73], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+73], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+73], v9, v11, s[60:61]
v_and_or_b32 v72, v[vgprValuC+73], v10, v[vgprValuC+72] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+74], v[vgprValuC+74] // check Nan
v_bfe_u32 v9, v[vgprValuC+74], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+74], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+74], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+74], 16, v[vgprValuC+74] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_and_or_b32 v73, v[vgprValuC+75], v10, v[vgprValuC+74] // pack two bf16 to dword
buffer_store_dwordx2 v[72:73], v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v78                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v78, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+81], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+82], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v79, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+83], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_mov_b32 v81, v5
v_mov_b32 v82, v6
v_mov_b32 v83, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+80], 16, v[vgprValuC+80] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+81], v[vgprValuC+81] // check Nan
v_bfe_u32 v9, v[vgprValuC+81], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+81], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+81], v9, v11, s[60:61]
v_and_or_b32 v80, v[vgprValuC+81], v10, v[vgprValuC+80] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+82], v[vgprValuC+82] // check Nan
v_bfe_u32 v9, v[vgprValuC+82], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+82], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+82], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+82], 16, v[vgprValuC+82] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+83], v[vgprValuC+83] // check Nan
v_bfe_u32 v9, v[vgprValuC+83], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+83], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+83], v9, v11, s[60:61]
v_and_or_b32 v81, v[vgprValuC+83], v10, v[vgprValuC+82] // pack two bf16 to dword
buffer_store_dwordx2 v[80:81], v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[24:25], v[vgprValuC+92:vgprValuC+92+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[26:27], v[vgprValuC+94:vgprValuC+94+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v88                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+92], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v88, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+93], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+94], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v89, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+94:vgprValuC+94+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v92, v4
v_mov_b32 v93, v5
v_mov_b32 v94, v6
v_mov_b32 v95, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+92], v[vgprValuC+92] // check Nan
v_bfe_u32 v9, v[vgprValuC+92], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+92], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+92], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+92], 16, v[vgprValuC+92] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+93], v[vgprValuC+93] // check Nan
v_bfe_u32 v9, v[vgprValuC+93], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+93], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+93], v9, v11, s[60:61]
v_and_or_b32 v92, v[vgprValuC+93], v10, v[vgprValuC+92] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+94], v[vgprValuC+94] // check Nan
v_bfe_u32 v9, v[vgprValuC+94], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+94], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+94], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+94], 16, v[vgprValuC+94] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_and_or_b32 v93, v[vgprValuC+95], v10, v[vgprValuC+94] // pack two bf16 to dword
buffer_store_dwordx2 v[92:93], v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+100:vgprValuC+100+1], v[24:25], v[vgprValuC+100:vgprValuC+100+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+102:vgprValuC+102+1], v[26:27], v[vgprValuC+102:vgprValuC+102+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v96                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v96, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+101], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v97                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+102], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v97, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+103], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+100:vgprValuC+100+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+102:vgprValuC+102+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_mov_b32 v101, v5
v_mov_b32 v102, v6
v_mov_b32 v103, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+100], 16, v[vgprValuC+100] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+101], v[vgprValuC+101] // check Nan
v_bfe_u32 v9, v[vgprValuC+101], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+101], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+101], v9, v11, s[60:61]
v_and_or_b32 v100, v[vgprValuC+101], v10, v[vgprValuC+100] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+102], v[vgprValuC+102] // check Nan
v_bfe_u32 v9, v[vgprValuC+102], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+102], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+102], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+102], 16, v[vgprValuC+102] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+103], v[vgprValuC+103] // check Nan
v_bfe_u32 v9, v[vgprValuC+103], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+103], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+103], v9, v11, s[60:61]
v_and_or_b32 v101, v[vgprValuC+103], v10, v[vgprValuC+102] // pack two bf16 to dword
buffer_store_dwordx2 v[100:101], v87, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+108:vgprValuC+108+1], v[24:25], v[vgprValuC+108:vgprValuC+108+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+110:vgprValuC+110+1], v[26:27], v[vgprValuC+110:vgprValuC+110+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v106                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+108], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v106, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+109], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v107                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+110], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v107, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+111], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+108:vgprValuC+108+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+110:vgprValuC+110+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v108, v4
v_mov_b32 v109, v5
v_mov_b32 v110, v6
v_mov_b32 v111, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+108], v[vgprValuC+108] // check Nan
v_bfe_u32 v9, v[vgprValuC+108], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+108], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+108], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+108], 16, v[vgprValuC+108] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+109], v[vgprValuC+109] // check Nan
v_bfe_u32 v9, v[vgprValuC+109], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+109], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+109], v9, v11, s[60:61]
v_and_or_b32 v108, v[vgprValuC+109], v10, v[vgprValuC+108] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+110], v[vgprValuC+110] // check Nan
v_bfe_u32 v9, v[vgprValuC+110], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+110], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+110], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+110], 16, v[vgprValuC+110] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+111], v[vgprValuC+111] // check Nan
v_bfe_u32 v9, v[vgprValuC+111], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+111], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+111], v9, v11, s[60:61]
v_and_or_b32 v109, v[vgprValuC+111], v10, v[vgprValuC+110] // pack two bf16 to dword
buffer_store_dwordx2 v[108:109], v98, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+116:vgprValuC+116+1], v[24:25], v[vgprValuC+116:vgprValuC+116+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+118:vgprValuC+118+1], v[26:27], v[vgprValuC+118:vgprValuC+118+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v114                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+116], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v114, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+117], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v115                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+118], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v115, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+119], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+116:vgprValuC+116+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+118:vgprValuC+118+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v116, v4
v_mov_b32 v117, v5
v_mov_b32 v118, v6
v_mov_b32 v119, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+116], v[vgprValuC+116] // check Nan
v_bfe_u32 v9, v[vgprValuC+116], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+116], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+116], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+116], 16, v[vgprValuC+116] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+117], v[vgprValuC+117] // check Nan
v_bfe_u32 v9, v[vgprValuC+117], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+117], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+117], v9, v11, s[60:61]
v_and_or_b32 v116, v[vgprValuC+117], v10, v[vgprValuC+116] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+118], v[vgprValuC+118] // check Nan
v_bfe_u32 v9, v[vgprValuC+118], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+118], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+118], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+118], 16, v[vgprValuC+118] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+119], v[vgprValuC+119] // check Nan
v_bfe_u32 v9, v[vgprValuC+119], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+119], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+119], v9, v11, s[60:61]
v_and_or_b32 v117, v[vgprValuC+119], v10, v[vgprValuC+118] // pack two bf16 to dword
buffer_store_dwordx2 v[116:117], v105, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+128:vgprValuC+128+1], v[24:25], v[vgprValuC+128:vgprValuC+128+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+130:vgprValuC+130+1], v[26:27], v[vgprValuC+130:vgprValuC+130+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v124                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+128], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v124, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+129], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v125                         // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+130], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_and_b32 v4, v125, v10                            // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+131], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+128:vgprValuC+128+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+130:vgprValuC+130+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v128, v4
v_mov_b32 v129, v5
v_mov_b32 v130, v6
v_mov_b32 v131, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+128], v[vgprValuC+128] // check Nan
v_bfe_u32 v9, v[vgprValuC+128], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+128], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+128], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+128], 16, v[vgprValuC+128] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+129], v[vgprValuC+129] // check Nan
v_bfe_u32 v9, v[vgprValuC+129], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+129], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+129], v9, v11, s[60:61]
v_and_or_b32 v128, v[vgprValuC+129], v10, v[vgprValuC+128] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+130], v[vgprValuC+130] // check Nan
v_bfe_u32 v9, v[vgprValuC+130], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+130], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+130], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+130], 16, v[vgprValuC+130] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+131], v[vgprValuC+131] // check Nan
v_bfe_u32 v9, v[vgprValuC+131], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+131], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+131], v9, v11, s[60:61]
v_and_or_b32 v129, v[vgprValuC+131], v10, v[vgprValuC+130] // pack two bf16 to dword
buffer_store_dwordx2 v[128:129], v120, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #4 (d1,d0,vc1,vc0) = */
/*    (12,0,0,0:vw4); (12,0,1,0:vw4); (12,0,2,0:vw4); (12,0,3,0:vw4); (13,0,0,0:vw4); (13,0,1,0:vw4); (13,0,2,0:vw4); (13,0,3,0:vw4) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v87, BufferOOB
/* (d1,vc1,d0,vc0)=(12,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v87, v13, s[64:65]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[16:17], v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v87, v14, s[64:65]              // LDBias clip if OOB. offset
ds_read_b128 v[20:23], v14 offset:0                // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dwordx4 v[24:27], v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v87, v13, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v18, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v87, v18, s[64:65]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[34:35], v18, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v19, v0, s60
v_lshlrev_b32 v19, 0x2, v19                        // Bias address scaled by BPE
v_cndmask_b32 v19, v87, v19, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v32, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v18, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v18, v87, v18, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v33, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v33, v87, v33, s[64:65]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[42:43], v33, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v40, v0, s60
v_lshlrev_b32 v40, 0x2, v40                        // Bias address scaled by BPE
v_cndmask_b32 v40, v87, v40, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v41, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v33, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v33, v87, v33, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v48, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v87, v48, s[64:65]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[52:53], v48, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v49, v0, s60
v_lshlrev_b32 v49, 0x2, v49                        // Bias address scaled by BPE
v_cndmask_b32 v49, v87, v49, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v50, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v48, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v48, v87, v48, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v87, v51, s[64:65]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[60:61], v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v54, v0, s60
v_lshlrev_b32 v54, 0x2, v54                        // Bias address scaled by BPE
v_cndmask_b32 v54, v87, v54, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v55, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v87, v51, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v62, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v87, v62, s[64:65]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[70:71], v62, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v63, v0, s60
v_lshlrev_b32 v63, 0x2, v63                        // Bias address scaled by BPE
v_cndmask_b32 v63, v87, v63, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v62, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v62, v87, v62, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v69, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v87, v69, s[64:65]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[78:79], v69, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v76, v0, s60
v_lshlrev_b32 v76, 0x2, v76                        // Bias address scaled by BPE
v_cndmask_b32 v76, v87, v76, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v77, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v69, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v69, v87, v69, s[64:65]              // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v84, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v87, v84, s[64:65]              // LDC clip if OOB. offset
buffer_load_dwordx2 v[88:89], v84, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v85, v0, s60
v_lshlrev_b32 v85, 0x2, v85                        // Bias address scaled by BPE
v_cndmask_b32 v85, v87, v85, s[64:65]              // LDBias clip if OOB. offset
v_lshlrev_b32 v86, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v84, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v84, v87, v84, s[64:65]              // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+28], acc192         // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+29], acc196         // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+30], acc200         // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+31], acc204         // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+36], acc193         // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+37], acc197         // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+38], acc201         // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+39], acc205         // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+44], acc194         // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+45], acc198         // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+46], acc202         // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+47], acc206         // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+56], acc195         // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+57], acc199         // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+58], acc203         // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+59], acc207         // copy acc to vreg[207]
v_accvgpr_read_b32 v[vgprValuC+64], acc208         // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+65], acc212         // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+66], acc216         // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+67], acc220         // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+72], acc209         // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+73], acc213         // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+74], acc217         // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+75], acc221         // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+80], acc210         // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+81], acc214         // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+82], acc218         // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+83], acc222         // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+92], acc211         // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+93], acc215         // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+94], acc219         // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+95], acc223         // copy acc to vreg[223]

/* rC *= alpha batchElements=[(12, 0, 0, 0), (12, 0, 1, 0), (12, 0, 2, 0), (12, 0, 3, 0), (13, 0, 0, 0), (13, 0, 1, 0), (13, 0, 2, 0), (13, 0, 3, 0)] */
v_mul_f32 v[vgprValuC+28], s[sgprAlpha], v[vgprValuC+28] // *= alpha
v_mul_f32 v[vgprValuC+29], s[sgprAlpha], v[vgprValuC+29] // *= alpha
v_mul_f32 v[vgprValuC+30], s[sgprAlpha], v[vgprValuC+30] // *= alpha
v_mul_f32 v[vgprValuC+31], s[sgprAlpha], v[vgprValuC+31] // *= alpha
v_mul_f32 v[vgprValuC+36], s[sgprAlpha], v[vgprValuC+36] // *= alpha
v_mul_f32 v[vgprValuC+37], s[sgprAlpha], v[vgprValuC+37] // *= alpha
v_mul_f32 v[vgprValuC+38], s[sgprAlpha], v[vgprValuC+38] // *= alpha
v_mul_f32 v[vgprValuC+39], s[sgprAlpha], v[vgprValuC+39] // *= alpha
v_mul_f32 v[vgprValuC+44], s[sgprAlpha], v[vgprValuC+44] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+46], s[sgprAlpha], v[vgprValuC+46] // *= alpha
v_mul_f32 v[vgprValuC+47], s[sgprAlpha], v[vgprValuC+47] // *= alpha
v_mul_f32 v[vgprValuC+56], s[sgprAlpha], v[vgprValuC+56] // *= alpha
v_mul_f32 v[vgprValuC+57], s[sgprAlpha], v[vgprValuC+57] // *= alpha
v_mul_f32 v[vgprValuC+58], s[sgprAlpha], v[vgprValuC+58] // *= alpha
v_mul_f32 v[vgprValuC+59], s[sgprAlpha], v[vgprValuC+59] // *= alpha
v_mul_f32 v[vgprValuC+64], s[sgprAlpha], v[vgprValuC+64] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+66], s[sgprAlpha], v[vgprValuC+66] // *= alpha
v_mul_f32 v[vgprValuC+67], s[sgprAlpha], v[vgprValuC+67] // *= alpha
v_mul_f32 v[vgprValuC+72], s[sgprAlpha], v[vgprValuC+72] // *= alpha
v_mul_f32 v[vgprValuC+73], s[sgprAlpha], v[vgprValuC+73] // *= alpha
v_mul_f32 v[vgprValuC+74], s[sgprAlpha], v[vgprValuC+74] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+81], s[sgprAlpha], v[vgprValuC+81] // *= alpha
v_mul_f32 v[vgprValuC+82], s[sgprAlpha], v[vgprValuC+82] // *= alpha
v_mul_f32 v[vgprValuC+83], s[sgprAlpha], v[vgprValuC+83] // *= alpha
v_mul_f32 v[vgprValuC+92], s[sgprAlpha], v[vgprValuC+92] // *= alpha
v_mul_f32 v[vgprValuC+93], s[sgprAlpha], v[vgprValuC+93] // *= alpha
v_mul_f32 v[vgprValuC+94], s[sgprAlpha], v[vgprValuC+94] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+28:vgprValuC+28+1], v[24:25], v[vgprValuC+28:vgprValuC+28+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+30:vgprValuC+30+1], v[26:27], v[vgprValuC+30:vgprValuC+30+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+28], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v16, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+29], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v17                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+30], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v17, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+31], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+28:vgprValuC+28+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+30:vgprValuC+30+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v28, v4
v_mov_b32 v29, v5
v_mov_b32 v30, v6
v_mov_b32 v31, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+28], v[vgprValuC+28] // check Nan
v_bfe_u32 v9, v[vgprValuC+28], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+28], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+28], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+28], 16, v[vgprValuC+28] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+29], v[vgprValuC+29] // check Nan
v_bfe_u32 v9, v[vgprValuC+29], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+29], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+29], v9, v11, s[60:61]
v_and_or_b32 v28, v[vgprValuC+29], v10, v[vgprValuC+28] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+30], v[vgprValuC+30] // check Nan
v_bfe_u32 v9, v[vgprValuC+30], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+30], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+30], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+30], 16, v[vgprValuC+30] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+31], v[vgprValuC+31] // check Nan
v_bfe_u32 v9, v[vgprValuC+31], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+31], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+31], v9, v11, s[60:61]
v_and_or_b32 v29, v[vgprValuC+31], v10, v[vgprValuC+30] // pack two bf16 to dword
buffer_store_dwordx2 v[28:29], v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+36:vgprValuC+36+1], v[24:25], v[vgprValuC+36:vgprValuC+36+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+38:vgprValuC+38+1], v[26:27], v[vgprValuC+38:vgprValuC+38+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v34                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+36], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v34, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+37], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v35                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+38], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v35, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+39], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+36:vgprValuC+36+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+38:vgprValuC+38+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v36, v4
v_mov_b32 v37, v5
v_mov_b32 v38, v6
v_mov_b32 v39, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+36], v[vgprValuC+36] // check Nan
v_bfe_u32 v9, v[vgprValuC+36], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+36], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+36], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+36], 16, v[vgprValuC+36] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+37], v[vgprValuC+37] // check Nan
v_bfe_u32 v9, v[vgprValuC+37], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+37], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+37], v9, v11, s[60:61]
v_and_or_b32 v36, v[vgprValuC+37], v10, v[vgprValuC+36] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+38], v[vgprValuC+38] // check Nan
v_bfe_u32 v9, v[vgprValuC+38], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+38], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+38], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+38], 16, v[vgprValuC+38] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+39], v[vgprValuC+39] // check Nan
v_bfe_u32 v9, v[vgprValuC+39], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+39], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+39], v9, v11, s[60:61]
v_and_or_b32 v37, v[vgprValuC+39], v10, v[vgprValuC+38] // pack two bf16 to dword
buffer_store_dwordx2 v[36:37], v18, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+44:vgprValuC+44+1], v[24:25], v[vgprValuC+44:vgprValuC+44+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+46:vgprValuC+46+1], v[26:27], v[vgprValuC+46:vgprValuC+46+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v42                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+44], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v42, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v43                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+46], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v43, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+47], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+44:vgprValuC+44+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+46:vgprValuC+46+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v44, v4
v_mov_b32 v45, v5
v_mov_b32 v46, v6
v_mov_b32 v47, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+44], v[vgprValuC+44] // check Nan
v_bfe_u32 v9, v[vgprValuC+44], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+44], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+44], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+44], 16, v[vgprValuC+44] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_and_or_b32 v44, v[vgprValuC+45], v10, v[vgprValuC+44] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+46], v[vgprValuC+46] // check Nan
v_bfe_u32 v9, v[vgprValuC+46], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+46], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+46], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+46], 16, v[vgprValuC+46] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+47], v[vgprValuC+47] // check Nan
v_bfe_u32 v9, v[vgprValuC+47], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+47], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+47], v9, v11, s[60:61]
v_and_or_b32 v45, v[vgprValuC+47], v10, v[vgprValuC+46] // pack two bf16 to dword
buffer_store_dwordx2 v[44:45], v33, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+56:vgprValuC+56+1], v[24:25], v[vgprValuC+56:vgprValuC+56+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+58:vgprValuC+58+1], v[26:27], v[vgprValuC+58:vgprValuC+58+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v52                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+56], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v52, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+57], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v53                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+58], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v53, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+59], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+56:vgprValuC+56+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+58:vgprValuC+58+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v56, v4
v_mov_b32 v57, v5
v_mov_b32 v58, v6
v_mov_b32 v59, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+56], v[vgprValuC+56] // check Nan
v_bfe_u32 v9, v[vgprValuC+56], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+56], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+56], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+56], 16, v[vgprValuC+56] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+57], v[vgprValuC+57] // check Nan
v_bfe_u32 v9, v[vgprValuC+57], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+57], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+57], v9, v11, s[60:61]
v_and_or_b32 v56, v[vgprValuC+57], v10, v[vgprValuC+56] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+58], v[vgprValuC+58] // check Nan
v_bfe_u32 v9, v[vgprValuC+58], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+58], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+58], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+58], 16, v[vgprValuC+58] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+59], v[vgprValuC+59] // check Nan
v_bfe_u32 v9, v[vgprValuC+59], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+59], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+59], v9, v11, s[60:61]
v_and_or_b32 v57, v[vgprValuC+59], v10, v[vgprValuC+58] // pack two bf16 to dword
buffer_store_dwordx2 v[56:57], v48, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+64:vgprValuC+64+1], v[24:25], v[vgprValuC+64:vgprValuC+64+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+66:vgprValuC+66+1], v[26:27], v[vgprValuC+66:vgprValuC+66+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v60                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+64], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v60, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v61                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+66], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v61, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+67], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+64:vgprValuC+64+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+66:vgprValuC+66+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v64, v4
v_mov_b32 v65, v5
v_mov_b32 v66, v6
v_mov_b32 v67, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+64], v[vgprValuC+64] // check Nan
v_bfe_u32 v9, v[vgprValuC+64], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+64], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+64], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+64], 16, v[vgprValuC+64] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_and_or_b32 v64, v[vgprValuC+65], v10, v[vgprValuC+64] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+66], v[vgprValuC+66] // check Nan
v_bfe_u32 v9, v[vgprValuC+66], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+66], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+66], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+66], 16, v[vgprValuC+66] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+67], v[vgprValuC+67] // check Nan
v_bfe_u32 v9, v[vgprValuC+67], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+67], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+67], v9, v11, s[60:61]
v_and_or_b32 v65, v[vgprValuC+67], v10, v[vgprValuC+66] // pack two bf16 to dword
buffer_store_dwordx2 v[64:65], v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+72:vgprValuC+72+1], v[24:25], v[vgprValuC+72:vgprValuC+72+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+74:vgprValuC+74+1], v[26:27], v[vgprValuC+74:vgprValuC+74+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v70                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+72], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v70, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+73], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v71                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+74], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v71, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+72:vgprValuC+72+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+74:vgprValuC+74+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v72, v4
v_mov_b32 v73, v5
v_mov_b32 v74, v6
v_mov_b32 v75, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+72], v[vgprValuC+72] // check Nan
v_bfe_u32 v9, v[vgprValuC+72], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+72], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+72], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+72], 16, v[vgprValuC+72] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+73], v[vgprValuC+73] // check Nan
v_bfe_u32 v9, v[vgprValuC+73], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+73], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+73], v9, v11, s[60:61]
v_and_or_b32 v72, v[vgprValuC+73], v10, v[vgprValuC+72] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+74], v[vgprValuC+74] // check Nan
v_bfe_u32 v9, v[vgprValuC+74], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+74], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+74], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+74], 16, v[vgprValuC+74] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_and_or_b32 v73, v[vgprValuC+75], v10, v[vgprValuC+74] // pack two bf16 to dword
buffer_store_dwordx2 v[72:73], v62, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+80:vgprValuC+80+1], v[24:25], v[vgprValuC+80:vgprValuC+80+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+82:vgprValuC+82+1], v[26:27], v[vgprValuC+82:vgprValuC+82+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v78                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v78, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+81], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+82], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v79, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+83], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+80:vgprValuC+80+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+82:vgprValuC+82+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_mov_b32 v81, v5
v_mov_b32 v82, v6
v_mov_b32 v83, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+80], 16, v[vgprValuC+80] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+81], v[vgprValuC+81] // check Nan
v_bfe_u32 v9, v[vgprValuC+81], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+81], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+81], v9, v11, s[60:61]
v_and_or_b32 v80, v[vgprValuC+81], v10, v[vgprValuC+80] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+82], v[vgprValuC+82] // check Nan
v_bfe_u32 v9, v[vgprValuC+82], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+82], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+82], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+82], 16, v[vgprValuC+82] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+83], v[vgprValuC+83] // check Nan
v_bfe_u32 v9, v[vgprValuC+83], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+83], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+83], v9, v11, s[60:61]
v_and_or_b32 v81, v[vgprValuC+83], v10, v[vgprValuC+82] // pack two bf16 to dword
buffer_store_dwordx2 v[80:81], v69, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v24, 1.0, v24, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+92:vgprValuC+92+1], v[24:25], v[vgprValuC+92:vgprValuC+92+1] // *= scaleAlphaVecVMulPK(24)(0)
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v26, 1.0, v26, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_cndmask_b32 v27, 1.0, v27, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_pk_mul_f32 v[vgprValuC+94:vgprValuC+94+1], v[26:27], v[vgprValuC+94:vgprValuC+94+1] // *= scaleAlphaVecVMulPK(24)(2)
v_lshlrev_b32 v4, 16, v88                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+92], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v88, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+93], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+94], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_and_b32 v4, v89, v10                             // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_pk_add_f32 v[4:5], v[20:21], v[vgprValuC+92:vgprValuC+92+1] // C += bias
v_pk_add_f32 v[6:7], v[22:23], v[vgprValuC+94:vgprValuC+94+1] // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v92, v4
v_mov_b32 v93, v5
v_mov_b32 v94, v6
v_mov_b32 v95, v7
v_cmp_u_f32 s[60:61], v[vgprValuC+92], v[vgprValuC+92] // check Nan
v_bfe_u32 v9, v[vgprValuC+92], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+92], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+92], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+92], 16, v[vgprValuC+92] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+93], v[vgprValuC+93] // check Nan
v_bfe_u32 v9, v[vgprValuC+93], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+93], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+93], v9, v11, s[60:61]
v_and_or_b32 v92, v[vgprValuC+93], v10, v[vgprValuC+92] // pack two bf16 to dword
v_cmp_u_f32 s[60:61], v[vgprValuC+94], v[vgprValuC+94] // check Nan
v_bfe_u32 v9, v[vgprValuC+94], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+94], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+94], v9, v11, s[60:61]
v_lshrrev_b32 v[vgprValuC+94], 16, v[vgprValuC+94] // convert C to bf16
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_and_or_b32 v93, v[vgprValuC+95], v10, v[vgprValuC+94] // pack two bf16 to dword
buffer_store_dwordx2 v[92:93], v84, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_2                            // jump to end
label_GW_B1_E1_M:
s_cmpk_eq_u32 s[sgprActivationType], 1             // activationType == 1
s_cbranch_scc1 label_To_Activation_Abs_VW1_beta_1_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 2             // activationType == 2
s_cbranch_scc1 label_To_Activation_Clippedrelu_VW1_beta_1_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 3             // activationType == 3
s_cbranch_scc1 label_To_Activation_Gelu_VW1_beta_1_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 4             // activationType == 4
s_cbranch_scc1 label_To_Activation_Leakyrelu_VW1_beta_1_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 5             // activationType == 5
s_cbranch_scc1 label_To_Activation_Relu_VW1_beta_1_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 6             // activationType == 6
s_cbranch_scc1 label_To_Activation_Sigmoid_VW1_beta_1_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 7             // activationType == 7
s_cbranch_scc1 label_To_Activation_Tanh_VW1_beta_1_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 9             // activationType == 9
s_cbranch_scc1 label_To_Activation_Geluscaling_VW1_beta_1_edge_1 // Branch if true
s_cmpk_eq_u32 s[sgprActivationType], 10            // activationType == 10
s_cbranch_scc1 label_To_Activation_Silu_VW1_beta_1_edge_1 // Branch if true
label_To_Activation_None_VW1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_None_VW1, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_1
label_To_Activation_Abs_VW1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Abs_VW1, 0x4        // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_1
label_To_Activation_Clippedrelu_VW1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Clippedrelu_VW1, 0x4 // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_1
label_To_Activation_Gelu_VW1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Gelu_VW1, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_1
label_To_Activation_Leakyrelu_VW1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Leakyrelu_VW1, 0x4  // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_1
label_To_Activation_Relu_VW1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Relu_VW1, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_1
label_To_Activation_Sigmoid_VW1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Sigmoid_VW1, 0x4    // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_1
label_To_Activation_Tanh_VW1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Tanh_VW1, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_1
label_To_Activation_Geluscaling_VW1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Geluscaling_VW1, 0x4 // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_1
label_To_Activation_Silu_VW1_beta_1_edge_1:
s_getpc_b64 s[12:13]                               // addr of next instr
s_add_i32 s8, label_Activation_Silu_VW1, 0x4       // target branch offset
s_add_u32 s12, s12, s8                             // add target branch offset
s_addc_u32 s13, s13, 0                             // add high and carry
s_branch label_ActivationSetPCAddrEnd_1
label_ActivationSetPCAddrEnd_1:

/* edge=1, allocate 6 sgpr. perBatchTmpS=4 perBatchMaskS=2 perElementMaskS=0 elementsPerBatch=16 */
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #0 (d1,d0,vc1,vc0) = */
/*    (0,0,0,0:vw1); (0,0,0,1:vw1); (0,0,0,2:vw1); (0,0,0,3:vw1); (0,0,1,0:vw1); (0,0,1,1:vw1); (0,0,1,2:vw1); (0,0,1,3:vw1); (0,0,2,0:vw1); (0,0,2,1:vw1); (0,0,2,2:vw1); (0,0,2,3:vw1); (0,0,3,0:vw1); (0,0,3,1:vw1); (0,0,3,2:vw1); (0,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v101, BufferOOB
/* (d1,vc1,d0,vc0)=(0,0,0,0) */
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v101, v14, s[64:65]             // LDBias clip if OOB. offset
s_waitcnt lgkmcnt(0)                               // Wait for Bias LDS write
s_barrier                                          // Bias LDS write barrier
ds_read_b32 v17, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v18, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s60
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
v_cndmask_b32 v21, v101, v21, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v24, v21 offset:0                      // load bias
v_lshlrev_b32 v22, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v25, v22, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s60
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v101, v28, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v31, v28 offset:0                      // load bias
v_lshlrev_b32 v29, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v32, v29, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s60
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v101, v35, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v38, v35 offset:0                      // load bias
v_lshlrev_b32 v36, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v39, v36, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v41, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v0, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v101, v42, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v49, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v47, v4, s60
v_lshlrev_b32 v47, 0x2, v47                        // Bias address scaled by BPE
v_cndmask_b32 v47, v101, v47, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v48, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v46, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v54, v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s60
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v101, v52, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v56, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v59, v56, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v4, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v101, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v61, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v64, v61, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v0, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v101, v62, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v66, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v69, v66, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v101, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v71, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v74, v71, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s60
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_cndmask_b32 v72, v101, v72, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v73, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v71, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s60
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
v_cndmask_b32 v77, v101, v77, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v78, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v81, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v84, v81, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v0, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v101, v82, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v86, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v89, v86, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v87, v4, s60
v_lshlrev_b32 v87, 0x2, v87                        // Bias address scaled by BPE
v_cndmask_b32 v87, v101, v87, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v88, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v86, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v91, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v94, v91, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v4, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v101, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v91, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(0,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v96, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v99, v96, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s60
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
v_cndmask_b32 v97, v101, v97, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v98, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+19], acc0           // copy acc to vreg[0]
v_accvgpr_read_b32 v[vgprValuC+26], acc4           // copy acc to vreg[1]
v_accvgpr_read_b32 v[vgprValuC+33], acc8           // copy acc to vreg[2]
v_accvgpr_read_b32 v[vgprValuC+40], acc12          // copy acc to vreg[3]
v_accvgpr_read_b32 v[vgprValuC+45], acc1           // copy acc to vreg[4]
v_accvgpr_read_b32 v[vgprValuC+50], acc5           // copy acc to vreg[5]
v_accvgpr_read_b32 v[vgprValuC+55], acc9           // copy acc to vreg[6]
v_accvgpr_read_b32 v[vgprValuC+60], acc13          // copy acc to vreg[7]
v_accvgpr_read_b32 v[vgprValuC+65], acc2           // copy acc to vreg[8]
v_accvgpr_read_b32 v[vgprValuC+70], acc6           // copy acc to vreg[9]
v_accvgpr_read_b32 v[vgprValuC+75], acc10          // copy acc to vreg[10]
v_accvgpr_read_b32 v[vgprValuC+80], acc14          // copy acc to vreg[11]
v_accvgpr_read_b32 v[vgprValuC+85], acc3           // copy acc to vreg[12]
v_accvgpr_read_b32 v[vgprValuC+90], acc7           // copy acc to vreg[13]
v_accvgpr_read_b32 v[vgprValuC+95], acc11          // copy acc to vreg[14]
v_accvgpr_read_b32 v[vgprValuC+100], acc15         // copy acc to vreg[15]

/* rC *= alpha batchElements=[(0, 0, 0, 0), (0, 0, 0, 1), (0, 0, 0, 2), (0, 0, 0, 3), (0, 0, 1, 0), (0, 0, 1, 1), (0, 0, 1, 2), (0, 0, 1, 3), (0, 0, 2, 0), (0, 0, 2, 1), (0, 0, 2, 2), (0, 0, 2, 3), (0, 0, 3, 0), (0, 0, 3, 1), (0, 0, 3, 2), (0, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+19], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+19]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v19, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+19], v[vgprValuC+19] // check Nan
v_bfe_u32 v9, v[vgprValuC+19], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+19], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+19], v9, v11, s[60:61]
v_lshrrev_b32 v19, 16, v[vgprValuC+19]             // convert C to bf16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v23                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+26], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+26]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v26, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v26, 16, v[vgprValuC+26]             // convert C to bf16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v30                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+33], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+33]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v33, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_lshrrev_b32 v33, 16, v[vgprValuC+33]             // convert C to bf16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v37                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+45], v18, v[vgprValuC+45]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+45]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v45, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_lshrrev_b32 v45, 16, v[vgprValuC+45]             // convert C to bf16
buffer_store_short v45, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+50], v25, v[vgprValuC+50]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v49                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+50]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v50, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v50, 16, v[vgprValuC+50]             // convert C to bf16
buffer_store_short v50, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+55], v32, v[vgprValuC+55]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v54                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+55]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v55, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_lshrrev_b32 v55, 16, v[vgprValuC+55]             // convert C to bf16
buffer_store_short v55, v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v39, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+65], v18, v[vgprValuC+65]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v64                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+65]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v65, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_lshrrev_b32 v65, 16, v[vgprValuC+65]             // convert C to bf16
buffer_store_short v65, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+70], v25, v[vgprValuC+70]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+70], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+70]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v70, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v70, 16, v[vgprValuC+70]             // convert C to bf16
buffer_store_short v70, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+75], v32, v[vgprValuC+75]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v74                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+75]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v75, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_lshrrev_b32 v75, 16, v[vgprValuC+75]             // convert C to bf16
buffer_store_short v75, v71, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v39, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+85], v18, v[vgprValuC+85]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v84                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+85]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v85, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_lshrrev_b32 v85, 16, v[vgprValuC+85]             // convert C to bf16
buffer_store_short v85, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+90], v25, v[vgprValuC+90]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+90]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v90, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v90, 16, v[vgprValuC+90]             // convert C to bf16
buffer_store_short v90, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+95], v32, v[vgprValuC+95]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+95]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v95, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_lshrrev_b32 v95, 16, v[vgprValuC+95]             // convert C to bf16
buffer_store_short v95, v91, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+100], v39, v[vgprValuC+100]  // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v99                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+100]                // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v100, 16, v[vgprValuC+100]           // convert C to bf16
buffer_store_short v100, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #1 (d1,d0,vc1,vc0) = */
/*    (1,0,0,0:vw1); (1,0,0,1:vw1); (1,0,0,2:vw1); (1,0,0,3:vw1); (1,0,1,0:vw1); (1,0,1,1:vw1); (1,0,1,2:vw1); (1,0,1,3:vw1); (1,0,2,0:vw1); (1,0,2,1:vw1); (1,0,2,2:vw1); (1,0,2,3:vw1); (1,0,3,0:vw1); (1,0,3,1:vw1); (1,0,3,2:vw1); (1,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v101, BufferOOB
/* (d1,vc1,d0,vc0)=(1,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v101, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v17, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v18, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s60
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
v_cndmask_b32 v21, v101, v21, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v24, v21 offset:0                      // load bias
v_lshlrev_b32 v22, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v25, v22, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s60
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v101, v28, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v31, v28 offset:0                      // load bias
v_lshlrev_b32 v29, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v32, v29, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s60
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v101, v35, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v38, v35 offset:0                      // load bias
v_lshlrev_b32 v36, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v39, v36, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v41, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v0, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v101, v42, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v49, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v47, v4, s60
v_lshlrev_b32 v47, 0x2, v47                        // Bias address scaled by BPE
v_cndmask_b32 v47, v101, v47, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v48, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v46, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v54, v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s60
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v101, v52, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v56, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v59, v56, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v4, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v101, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v61, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v64, v61, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v0, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v101, v62, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v66, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v69, v66, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v101, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v71, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v74, v71, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s60
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_cndmask_b32 v72, v101, v72, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v73, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v71, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s60
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
v_cndmask_b32 v77, v101, v77, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v78, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v81, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v84, v81, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v0, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v101, v82, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v86, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v89, v86, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v87, v4, s60
v_lshlrev_b32 v87, 0x2, v87                        // Bias address scaled by BPE
v_cndmask_b32 v87, v101, v87, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v88, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v86, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v91, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v94, v91, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v4, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v101, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v91, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(1,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v96, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v99, v96, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s60
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
v_cndmask_b32 v97, v101, v97, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v98, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+19], acc16          // copy acc to vreg[16]
v_accvgpr_read_b32 v[vgprValuC+26], acc20          // copy acc to vreg[17]
v_accvgpr_read_b32 v[vgprValuC+33], acc24          // copy acc to vreg[18]
v_accvgpr_read_b32 v[vgprValuC+40], acc28          // copy acc to vreg[19]
v_accvgpr_read_b32 v[vgprValuC+45], acc17          // copy acc to vreg[20]
v_accvgpr_read_b32 v[vgprValuC+50], acc21          // copy acc to vreg[21]
v_accvgpr_read_b32 v[vgprValuC+55], acc25          // copy acc to vreg[22]
v_accvgpr_read_b32 v[vgprValuC+60], acc29          // copy acc to vreg[23]
v_accvgpr_read_b32 v[vgprValuC+65], acc18          // copy acc to vreg[24]
v_accvgpr_read_b32 v[vgprValuC+70], acc22          // copy acc to vreg[25]
v_accvgpr_read_b32 v[vgprValuC+75], acc26          // copy acc to vreg[26]
v_accvgpr_read_b32 v[vgprValuC+80], acc30          // copy acc to vreg[27]
v_accvgpr_read_b32 v[vgprValuC+85], acc19          // copy acc to vreg[28]
v_accvgpr_read_b32 v[vgprValuC+90], acc23          // copy acc to vreg[29]
v_accvgpr_read_b32 v[vgprValuC+95], acc27          // copy acc to vreg[30]
v_accvgpr_read_b32 v[vgprValuC+100], acc31         // copy acc to vreg[31]

/* rC *= alpha batchElements=[(1, 0, 0, 0), (1, 0, 0, 1), (1, 0, 0, 2), (1, 0, 0, 3), (1, 0, 1, 0), (1, 0, 1, 1), (1, 0, 1, 2), (1, 0, 1, 3), (1, 0, 2, 0), (1, 0, 2, 1), (1, 0, 2, 2), (1, 0, 2, 3), (1, 0, 3, 0), (1, 0, 3, 1), (1, 0, 3, 2), (1, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+19], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+19]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v19, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+19], v[vgprValuC+19] // check Nan
v_bfe_u32 v9, v[vgprValuC+19], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+19], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+19], v9, v11, s[60:61]
v_lshrrev_b32 v19, 16, v[vgprValuC+19]             // convert C to bf16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v23                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+26], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+26]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v26, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v26, 16, v[vgprValuC+26]             // convert C to bf16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v30                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+33], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+33]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v33, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_lshrrev_b32 v33, 16, v[vgprValuC+33]             // convert C to bf16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v37                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+45], v18, v[vgprValuC+45]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+45]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v45, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_lshrrev_b32 v45, 16, v[vgprValuC+45]             // convert C to bf16
buffer_store_short v45, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+50], v25, v[vgprValuC+50]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v49                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+50]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v50, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v50, 16, v[vgprValuC+50]             // convert C to bf16
buffer_store_short v50, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+55], v32, v[vgprValuC+55]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v54                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+55]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v55, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_lshrrev_b32 v55, 16, v[vgprValuC+55]             // convert C to bf16
buffer_store_short v55, v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v39, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+65], v18, v[vgprValuC+65]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v64                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+65]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v65, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_lshrrev_b32 v65, 16, v[vgprValuC+65]             // convert C to bf16
buffer_store_short v65, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+70], v25, v[vgprValuC+70]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+70], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+70]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v70, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v70, 16, v[vgprValuC+70]             // convert C to bf16
buffer_store_short v70, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+75], v32, v[vgprValuC+75]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v74                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+75]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v75, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_lshrrev_b32 v75, 16, v[vgprValuC+75]             // convert C to bf16
buffer_store_short v75, v71, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v39, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+85], v18, v[vgprValuC+85]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v84                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+85]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v85, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_lshrrev_b32 v85, 16, v[vgprValuC+85]             // convert C to bf16
buffer_store_short v85, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+90], v25, v[vgprValuC+90]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+90]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v90, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v90, 16, v[vgprValuC+90]             // convert C to bf16
buffer_store_short v90, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+95], v32, v[vgprValuC+95]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+95]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v95, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_lshrrev_b32 v95, 16, v[vgprValuC+95]             // convert C to bf16
buffer_store_short v95, v91, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+100], v39, v[vgprValuC+100]  // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v99                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+100]                // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v100, 16, v[vgprValuC+100]           // convert C to bf16
buffer_store_short v100, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #2 (d1,d0,vc1,vc0) = */
/*    (2,0,0,0:vw1); (2,0,0,1:vw1); (2,0,0,2:vw1); (2,0,0,3:vw1); (2,0,1,0:vw1); (2,0,1,1:vw1); (2,0,1,2:vw1); (2,0,1,3:vw1); (2,0,2,0:vw1); (2,0,2,1:vw1); (2,0,2,2:vw1); (2,0,2,3:vw1); (2,0,3,0:vw1); (2,0,3,1:vw1); (2,0,3,2:vw1); (2,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v101, BufferOOB
/* (d1,vc1,d0,vc0)=(2,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v101, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v17, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v18, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s60
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
v_cndmask_b32 v21, v101, v21, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v24, v21 offset:0                      // load bias
v_lshlrev_b32 v22, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v25, v22, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s60
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v101, v28, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v31, v28 offset:0                      // load bias
v_lshlrev_b32 v29, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v32, v29, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s60
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v101, v35, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v38, v35 offset:0                      // load bias
v_lshlrev_b32 v36, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v39, v36, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v41, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v0, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v101, v42, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v49, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v47, v4, s60
v_lshlrev_b32 v47, 0x2, v47                        // Bias address scaled by BPE
v_cndmask_b32 v47, v101, v47, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v48, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v46, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v54, v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s60
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v101, v52, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v56, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v59, v56, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v4, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v101, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v61, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v64, v61, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v0, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v101, v62, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v66, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v69, v66, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v101, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v71, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v74, v71, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s60
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_cndmask_b32 v72, v101, v72, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v73, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v71, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s60
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
v_cndmask_b32 v77, v101, v77, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v78, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v81, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v84, v81, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v0, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v101, v82, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v86, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v89, v86, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v87, v4, s60
v_lshlrev_b32 v87, 0x2, v87                        // Bias address scaled by BPE
v_cndmask_b32 v87, v101, v87, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v88, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v86, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v91, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v94, v91, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v4, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v101, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v91, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(2,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v96, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v99, v96, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s60
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
v_cndmask_b32 v97, v101, v97, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v98, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+19], acc32          // copy acc to vreg[32]
v_accvgpr_read_b32 v[vgprValuC+26], acc36          // copy acc to vreg[33]
v_accvgpr_read_b32 v[vgprValuC+33], acc40          // copy acc to vreg[34]
v_accvgpr_read_b32 v[vgprValuC+40], acc44          // copy acc to vreg[35]
v_accvgpr_read_b32 v[vgprValuC+45], acc33          // copy acc to vreg[36]
v_accvgpr_read_b32 v[vgprValuC+50], acc37          // copy acc to vreg[37]
v_accvgpr_read_b32 v[vgprValuC+55], acc41          // copy acc to vreg[38]
v_accvgpr_read_b32 v[vgprValuC+60], acc45          // copy acc to vreg[39]
v_accvgpr_read_b32 v[vgprValuC+65], acc34          // copy acc to vreg[40]
v_accvgpr_read_b32 v[vgprValuC+70], acc38          // copy acc to vreg[41]
v_accvgpr_read_b32 v[vgprValuC+75], acc42          // copy acc to vreg[42]
v_accvgpr_read_b32 v[vgprValuC+80], acc46          // copy acc to vreg[43]
v_accvgpr_read_b32 v[vgprValuC+85], acc35          // copy acc to vreg[44]
v_accvgpr_read_b32 v[vgprValuC+90], acc39          // copy acc to vreg[45]
v_accvgpr_read_b32 v[vgprValuC+95], acc43          // copy acc to vreg[46]
v_accvgpr_read_b32 v[vgprValuC+100], acc47         // copy acc to vreg[47]

/* rC *= alpha batchElements=[(2, 0, 0, 0), (2, 0, 0, 1), (2, 0, 0, 2), (2, 0, 0, 3), (2, 0, 1, 0), (2, 0, 1, 1), (2, 0, 1, 2), (2, 0, 1, 3), (2, 0, 2, 0), (2, 0, 2, 1), (2, 0, 2, 2), (2, 0, 2, 3), (2, 0, 3, 0), (2, 0, 3, 1), (2, 0, 3, 2), (2, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+19], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+19]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v19, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+19], v[vgprValuC+19] // check Nan
v_bfe_u32 v9, v[vgprValuC+19], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+19], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+19], v9, v11, s[60:61]
v_lshrrev_b32 v19, 16, v[vgprValuC+19]             // convert C to bf16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v23                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+26], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+26]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v26, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v26, 16, v[vgprValuC+26]             // convert C to bf16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v30                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+33], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+33]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v33, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_lshrrev_b32 v33, 16, v[vgprValuC+33]             // convert C to bf16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v37                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+45], v18, v[vgprValuC+45]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+45]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v45, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_lshrrev_b32 v45, 16, v[vgprValuC+45]             // convert C to bf16
buffer_store_short v45, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+50], v25, v[vgprValuC+50]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v49                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+50]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v50, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v50, 16, v[vgprValuC+50]             // convert C to bf16
buffer_store_short v50, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+55], v32, v[vgprValuC+55]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v54                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+55]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v55, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_lshrrev_b32 v55, 16, v[vgprValuC+55]             // convert C to bf16
buffer_store_short v55, v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v39, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+65], v18, v[vgprValuC+65]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v64                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+65]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v65, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_lshrrev_b32 v65, 16, v[vgprValuC+65]             // convert C to bf16
buffer_store_short v65, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+70], v25, v[vgprValuC+70]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+70], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+70]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v70, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v70, 16, v[vgprValuC+70]             // convert C to bf16
buffer_store_short v70, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+75], v32, v[vgprValuC+75]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v74                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+75]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v75, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_lshrrev_b32 v75, 16, v[vgprValuC+75]             // convert C to bf16
buffer_store_short v75, v71, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v39, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+85], v18, v[vgprValuC+85]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v84                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+85]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v85, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_lshrrev_b32 v85, 16, v[vgprValuC+85]             // convert C to bf16
buffer_store_short v85, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+90], v25, v[vgprValuC+90]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+90]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v90, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v90, 16, v[vgprValuC+90]             // convert C to bf16
buffer_store_short v90, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+95], v32, v[vgprValuC+95]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+95]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v95, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_lshrrev_b32 v95, 16, v[vgprValuC+95]             // convert C to bf16
buffer_store_short v95, v91, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+100], v39, v[vgprValuC+100]  // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v99                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+100]                // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v100, 16, v[vgprValuC+100]           // convert C to bf16
buffer_store_short v100, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #3 (d1,d0,vc1,vc0) = */
/*    (3,0,0,0:vw1); (3,0,0,1:vw1); (3,0,0,2:vw1); (3,0,0,3:vw1); (3,0,1,0:vw1); (3,0,1,1:vw1); (3,0,1,2:vw1); (3,0,1,3:vw1); (3,0,2,0:vw1); (3,0,2,1:vw1); (3,0,2,2:vw1); (3,0,2,3:vw1); (3,0,3,0:vw1); (3,0,3,1:vw1); (3,0,3,2:vw1); (3,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v101, BufferOOB
/* (d1,vc1,d0,vc0)=(3,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v101, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v17, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v18, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s60
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
v_cndmask_b32 v21, v101, v21, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v24, v21 offset:0                      // load bias
v_lshlrev_b32 v22, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v25, v22, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s60
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v101, v28, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v31, v28 offset:0                      // load bias
v_lshlrev_b32 v29, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v32, v29, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s60
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v101, v35, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v38, v35 offset:0                      // load bias
v_lshlrev_b32 v36, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v39, v36, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v41, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v0, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v101, v42, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v49, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v47, v4, s60
v_lshlrev_b32 v47, 0x2, v47                        // Bias address scaled by BPE
v_cndmask_b32 v47, v101, v47, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v48, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v46, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v54, v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s60
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v101, v52, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v56, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v59, v56, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v4, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v101, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v61, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v64, v61, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v0, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v101, v62, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v66, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v69, v66, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v101, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v71, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v74, v71, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s60
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_cndmask_b32 v72, v101, v72, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v73, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v71, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s60
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
v_cndmask_b32 v77, v101, v77, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v78, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v81, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v84, v81, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v0, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v101, v82, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v86, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v89, v86, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v87, v4, s60
v_lshlrev_b32 v87, 0x2, v87                        // Bias address scaled by BPE
v_cndmask_b32 v87, v101, v87, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v88, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v86, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v91, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v94, v91, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v4, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v101, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v91, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(3,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v96, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v99, v96, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s60
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
v_cndmask_b32 v97, v101, v97, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v98, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+19], acc48          // copy acc to vreg[48]
v_accvgpr_read_b32 v[vgprValuC+26], acc52          // copy acc to vreg[49]
v_accvgpr_read_b32 v[vgprValuC+33], acc56          // copy acc to vreg[50]
v_accvgpr_read_b32 v[vgprValuC+40], acc60          // copy acc to vreg[51]
v_accvgpr_read_b32 v[vgprValuC+45], acc49          // copy acc to vreg[52]
v_accvgpr_read_b32 v[vgprValuC+50], acc53          // copy acc to vreg[53]
v_accvgpr_read_b32 v[vgprValuC+55], acc57          // copy acc to vreg[54]
v_accvgpr_read_b32 v[vgprValuC+60], acc61          // copy acc to vreg[55]
v_accvgpr_read_b32 v[vgprValuC+65], acc50          // copy acc to vreg[56]
v_accvgpr_read_b32 v[vgprValuC+70], acc54          // copy acc to vreg[57]
v_accvgpr_read_b32 v[vgprValuC+75], acc58          // copy acc to vreg[58]
v_accvgpr_read_b32 v[vgprValuC+80], acc62          // copy acc to vreg[59]
v_accvgpr_read_b32 v[vgprValuC+85], acc51          // copy acc to vreg[60]
v_accvgpr_read_b32 v[vgprValuC+90], acc55          // copy acc to vreg[61]
v_accvgpr_read_b32 v[vgprValuC+95], acc59          // copy acc to vreg[62]
v_accvgpr_read_b32 v[vgprValuC+100], acc63         // copy acc to vreg[63]

/* rC *= alpha batchElements=[(3, 0, 0, 0), (3, 0, 0, 1), (3, 0, 0, 2), (3, 0, 0, 3), (3, 0, 1, 0), (3, 0, 1, 1), (3, 0, 1, 2), (3, 0, 1, 3), (3, 0, 2, 0), (3, 0, 2, 1), (3, 0, 2, 2), (3, 0, 2, 3), (3, 0, 3, 0), (3, 0, 3, 1), (3, 0, 3, 2), (3, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+19], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+19]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v19, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+19], v[vgprValuC+19] // check Nan
v_bfe_u32 v9, v[vgprValuC+19], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+19], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+19], v9, v11, s[60:61]
v_lshrrev_b32 v19, 16, v[vgprValuC+19]             // convert C to bf16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v23                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+26], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+26]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v26, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v26, 16, v[vgprValuC+26]             // convert C to bf16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v30                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+33], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+33]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v33, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_lshrrev_b32 v33, 16, v[vgprValuC+33]             // convert C to bf16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v37                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+45], v18, v[vgprValuC+45]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+45]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v45, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_lshrrev_b32 v45, 16, v[vgprValuC+45]             // convert C to bf16
buffer_store_short v45, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+50], v25, v[vgprValuC+50]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v49                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+50]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v50, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v50, 16, v[vgprValuC+50]             // convert C to bf16
buffer_store_short v50, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+55], v32, v[vgprValuC+55]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v54                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+55]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v55, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_lshrrev_b32 v55, 16, v[vgprValuC+55]             // convert C to bf16
buffer_store_short v55, v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v39, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+65], v18, v[vgprValuC+65]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v64                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+65]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v65, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_lshrrev_b32 v65, 16, v[vgprValuC+65]             // convert C to bf16
buffer_store_short v65, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+70], v25, v[vgprValuC+70]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+70], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+70]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v70, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v70, 16, v[vgprValuC+70]             // convert C to bf16
buffer_store_short v70, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+75], v32, v[vgprValuC+75]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v74                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+75]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v75, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_lshrrev_b32 v75, 16, v[vgprValuC+75]             // convert C to bf16
buffer_store_short v75, v71, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v39, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+85], v18, v[vgprValuC+85]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v84                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+85]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v85, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_lshrrev_b32 v85, 16, v[vgprValuC+85]             // convert C to bf16
buffer_store_short v85, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+90], v25, v[vgprValuC+90]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+90]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v90, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v90, 16, v[vgprValuC+90]             // convert C to bf16
buffer_store_short v90, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+95], v32, v[vgprValuC+95]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+95]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v95, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_lshrrev_b32 v95, 16, v[vgprValuC+95]             // convert C to bf16
buffer_store_short v95, v91, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+100], v39, v[vgprValuC+100]  // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v99                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+100]                // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v100, 16, v[vgprValuC+100]           // convert C to bf16
buffer_store_short v100, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #4 (d1,d0,vc1,vc0) = */
/*    (4,0,0,0:vw1); (4,0,0,1:vw1); (4,0,0,2:vw1); (4,0,0,3:vw1); (4,0,1,0:vw1); (4,0,1,1:vw1); (4,0,1,2:vw1); (4,0,1,3:vw1); (4,0,2,0:vw1); (4,0,2,1:vw1); (4,0,2,2:vw1); (4,0,2,3:vw1); (4,0,3,0:vw1); (4,0,3,1:vw1); (4,0,3,2:vw1); (4,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v101, BufferOOB
/* (d1,vc1,d0,vc0)=(4,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v101, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v17, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v18, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s60
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
v_cndmask_b32 v21, v101, v21, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v24, v21 offset:0                      // load bias
v_lshlrev_b32 v22, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v25, v22, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s60
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v101, v28, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v31, v28 offset:0                      // load bias
v_lshlrev_b32 v29, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v32, v29, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s60
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v101, v35, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v38, v35 offset:0                      // load bias
v_lshlrev_b32 v36, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v39, v36, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v41, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v0, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v101, v42, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v49, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v47, v4, s60
v_lshlrev_b32 v47, 0x2, v47                        // Bias address scaled by BPE
v_cndmask_b32 v47, v101, v47, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v48, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v46, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v54, v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s60
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v101, v52, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v56, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v59, v56, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v4, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v101, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v61, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v64, v61, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v0, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v101, v62, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v66, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v69, v66, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v101, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v71, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v74, v71, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s60
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_cndmask_b32 v72, v101, v72, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v73, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v71, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s60
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
v_cndmask_b32 v77, v101, v77, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v78, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v81, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v84, v81, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v0, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v101, v82, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v86, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v89, v86, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v87, v4, s60
v_lshlrev_b32 v87, 0x2, v87                        // Bias address scaled by BPE
v_cndmask_b32 v87, v101, v87, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v88, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v86, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v91, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v94, v91, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v4, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v101, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v91, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(4,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v96, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v99, v96, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s60
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
v_cndmask_b32 v97, v101, v97, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v98, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+19], acc64          // copy acc to vreg[64]
v_accvgpr_read_b32 v[vgprValuC+26], acc68          // copy acc to vreg[65]
v_accvgpr_read_b32 v[vgprValuC+33], acc72          // copy acc to vreg[66]
v_accvgpr_read_b32 v[vgprValuC+40], acc76          // copy acc to vreg[67]
v_accvgpr_read_b32 v[vgprValuC+45], acc65          // copy acc to vreg[68]
v_accvgpr_read_b32 v[vgprValuC+50], acc69          // copy acc to vreg[69]
v_accvgpr_read_b32 v[vgprValuC+55], acc73          // copy acc to vreg[70]
v_accvgpr_read_b32 v[vgprValuC+60], acc77          // copy acc to vreg[71]
v_accvgpr_read_b32 v[vgprValuC+65], acc66          // copy acc to vreg[72]
v_accvgpr_read_b32 v[vgprValuC+70], acc70          // copy acc to vreg[73]
v_accvgpr_read_b32 v[vgprValuC+75], acc74          // copy acc to vreg[74]
v_accvgpr_read_b32 v[vgprValuC+80], acc78          // copy acc to vreg[75]
v_accvgpr_read_b32 v[vgprValuC+85], acc67          // copy acc to vreg[76]
v_accvgpr_read_b32 v[vgprValuC+90], acc71          // copy acc to vreg[77]
v_accvgpr_read_b32 v[vgprValuC+95], acc75          // copy acc to vreg[78]
v_accvgpr_read_b32 v[vgprValuC+100], acc79         // copy acc to vreg[79]

/* rC *= alpha batchElements=[(4, 0, 0, 0), (4, 0, 0, 1), (4, 0, 0, 2), (4, 0, 0, 3), (4, 0, 1, 0), (4, 0, 1, 1), (4, 0, 1, 2), (4, 0, 1, 3), (4, 0, 2, 0), (4, 0, 2, 1), (4, 0, 2, 2), (4, 0, 2, 3), (4, 0, 3, 0), (4, 0, 3, 1), (4, 0, 3, 2), (4, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+19], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+19]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v19, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+19], v[vgprValuC+19] // check Nan
v_bfe_u32 v9, v[vgprValuC+19], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+19], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+19], v9, v11, s[60:61]
v_lshrrev_b32 v19, 16, v[vgprValuC+19]             // convert C to bf16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v23                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+26], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+26]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v26, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v26, 16, v[vgprValuC+26]             // convert C to bf16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v30                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+33], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+33]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v33, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_lshrrev_b32 v33, 16, v[vgprValuC+33]             // convert C to bf16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v37                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+45], v18, v[vgprValuC+45]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+45]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v45, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_lshrrev_b32 v45, 16, v[vgprValuC+45]             // convert C to bf16
buffer_store_short v45, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+50], v25, v[vgprValuC+50]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v49                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+50]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v50, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v50, 16, v[vgprValuC+50]             // convert C to bf16
buffer_store_short v50, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+55], v32, v[vgprValuC+55]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v54                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+55]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v55, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_lshrrev_b32 v55, 16, v[vgprValuC+55]             // convert C to bf16
buffer_store_short v55, v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v39, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+65], v18, v[vgprValuC+65]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v64                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+65]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v65, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_lshrrev_b32 v65, 16, v[vgprValuC+65]             // convert C to bf16
buffer_store_short v65, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+70], v25, v[vgprValuC+70]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+70], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+70]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v70, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v70, 16, v[vgprValuC+70]             // convert C to bf16
buffer_store_short v70, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+75], v32, v[vgprValuC+75]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v74                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+75]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v75, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_lshrrev_b32 v75, 16, v[vgprValuC+75]             // convert C to bf16
buffer_store_short v75, v71, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v39, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+85], v18, v[vgprValuC+85]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v84                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+85]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v85, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_lshrrev_b32 v85, 16, v[vgprValuC+85]             // convert C to bf16
buffer_store_short v85, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+90], v25, v[vgprValuC+90]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+90]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v90, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v90, 16, v[vgprValuC+90]             // convert C to bf16
buffer_store_short v90, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+95], v32, v[vgprValuC+95]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+95]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v95, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_lshrrev_b32 v95, 16, v[vgprValuC+95]             // convert C to bf16
buffer_store_short v95, v91, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+100], v39, v[vgprValuC+100]  // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v99                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+100]                // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v100, 16, v[vgprValuC+100]           // convert C to bf16
buffer_store_short v100, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #5 (d1,d0,vc1,vc0) = */
/*    (5,0,0,0:vw1); (5,0,0,1:vw1); (5,0,0,2:vw1); (5,0,0,3:vw1); (5,0,1,0:vw1); (5,0,1,1:vw1); (5,0,1,2:vw1); (5,0,1,3:vw1); (5,0,2,0:vw1); (5,0,2,1:vw1); (5,0,2,2:vw1); (5,0,2,3:vw1); (5,0,3,0:vw1); (5,0,3,1:vw1); (5,0,3,2:vw1); (5,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v101, BufferOOB
/* (d1,vc1,d0,vc0)=(5,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v101, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v17, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v18, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s60
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
v_cndmask_b32 v21, v101, v21, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v24, v21 offset:0                      // load bias
v_lshlrev_b32 v22, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v25, v22, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s60
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v101, v28, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v31, v28 offset:0                      // load bias
v_lshlrev_b32 v29, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v32, v29, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s60
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v101, v35, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v38, v35 offset:0                      // load bias
v_lshlrev_b32 v36, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v39, v36, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v41, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v0, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v101, v42, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v49, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v47, v4, s60
v_lshlrev_b32 v47, 0x2, v47                        // Bias address scaled by BPE
v_cndmask_b32 v47, v101, v47, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v48, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v46, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v54, v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s60
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v101, v52, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v56, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v59, v56, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v4, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v101, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v61, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v64, v61, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v0, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v101, v62, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v66, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v69, v66, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v101, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v71, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v74, v71, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s60
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_cndmask_b32 v72, v101, v72, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v73, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v71, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s60
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
v_cndmask_b32 v77, v101, v77, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v78, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v81, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v84, v81, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v0, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v101, v82, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v86, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v89, v86, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v87, v4, s60
v_lshlrev_b32 v87, 0x2, v87                        // Bias address scaled by BPE
v_cndmask_b32 v87, v101, v87, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v88, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v86, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v91, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v94, v91, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v4, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v101, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v91, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(5,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v96, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v99, v96, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s60
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
v_cndmask_b32 v97, v101, v97, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v98, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+19], acc80          // copy acc to vreg[80]
v_accvgpr_read_b32 v[vgprValuC+26], acc84          // copy acc to vreg[81]
v_accvgpr_read_b32 v[vgprValuC+33], acc88          // copy acc to vreg[82]
v_accvgpr_read_b32 v[vgprValuC+40], acc92          // copy acc to vreg[83]
v_accvgpr_read_b32 v[vgprValuC+45], acc81          // copy acc to vreg[84]
v_accvgpr_read_b32 v[vgprValuC+50], acc85          // copy acc to vreg[85]
v_accvgpr_read_b32 v[vgprValuC+55], acc89          // copy acc to vreg[86]
v_accvgpr_read_b32 v[vgprValuC+60], acc93          // copy acc to vreg[87]
v_accvgpr_read_b32 v[vgprValuC+65], acc82          // copy acc to vreg[88]
v_accvgpr_read_b32 v[vgprValuC+70], acc86          // copy acc to vreg[89]
v_accvgpr_read_b32 v[vgprValuC+75], acc90          // copy acc to vreg[90]
v_accvgpr_read_b32 v[vgprValuC+80], acc94          // copy acc to vreg[91]
v_accvgpr_read_b32 v[vgprValuC+85], acc83          // copy acc to vreg[92]
v_accvgpr_read_b32 v[vgprValuC+90], acc87          // copy acc to vreg[93]
v_accvgpr_read_b32 v[vgprValuC+95], acc91          // copy acc to vreg[94]
v_accvgpr_read_b32 v[vgprValuC+100], acc95         // copy acc to vreg[95]

/* rC *= alpha batchElements=[(5, 0, 0, 0), (5, 0, 0, 1), (5, 0, 0, 2), (5, 0, 0, 3), (5, 0, 1, 0), (5, 0, 1, 1), (5, 0, 1, 2), (5, 0, 1, 3), (5, 0, 2, 0), (5, 0, 2, 1), (5, 0, 2, 2), (5, 0, 2, 3), (5, 0, 3, 0), (5, 0, 3, 1), (5, 0, 3, 2), (5, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+19], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+19]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v19, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+19], v[vgprValuC+19] // check Nan
v_bfe_u32 v9, v[vgprValuC+19], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+19], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+19], v9, v11, s[60:61]
v_lshrrev_b32 v19, 16, v[vgprValuC+19]             // convert C to bf16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v23                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+26], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+26]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v26, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v26, 16, v[vgprValuC+26]             // convert C to bf16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v30                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+33], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+33]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v33, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_lshrrev_b32 v33, 16, v[vgprValuC+33]             // convert C to bf16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v37                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+45], v18, v[vgprValuC+45]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+45]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v45, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_lshrrev_b32 v45, 16, v[vgprValuC+45]             // convert C to bf16
buffer_store_short v45, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+50], v25, v[vgprValuC+50]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v49                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+50]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v50, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v50, 16, v[vgprValuC+50]             // convert C to bf16
buffer_store_short v50, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+55], v32, v[vgprValuC+55]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v54                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+55]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v55, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_lshrrev_b32 v55, 16, v[vgprValuC+55]             // convert C to bf16
buffer_store_short v55, v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v39, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+65], v18, v[vgprValuC+65]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v64                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+65]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v65, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_lshrrev_b32 v65, 16, v[vgprValuC+65]             // convert C to bf16
buffer_store_short v65, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+70], v25, v[vgprValuC+70]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+70], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+70]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v70, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v70, 16, v[vgprValuC+70]             // convert C to bf16
buffer_store_short v70, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+75], v32, v[vgprValuC+75]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v74                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+75]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v75, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_lshrrev_b32 v75, 16, v[vgprValuC+75]             // convert C to bf16
buffer_store_short v75, v71, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v39, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+85], v18, v[vgprValuC+85]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v84                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+85]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v85, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_lshrrev_b32 v85, 16, v[vgprValuC+85]             // convert C to bf16
buffer_store_short v85, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+90], v25, v[vgprValuC+90]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+90]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v90, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v90, 16, v[vgprValuC+90]             // convert C to bf16
buffer_store_short v90, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+95], v32, v[vgprValuC+95]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+95]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v95, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_lshrrev_b32 v95, 16, v[vgprValuC+95]             // convert C to bf16
buffer_store_short v95, v91, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+100], v39, v[vgprValuC+100]  // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v99                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+100]                // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v100, 16, v[vgprValuC+100]           // convert C to bf16
buffer_store_short v100, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #6 (d1,d0,vc1,vc0) = */
/*    (6,0,0,0:vw1); (6,0,0,1:vw1); (6,0,0,2:vw1); (6,0,0,3:vw1); (6,0,1,0:vw1); (6,0,1,1:vw1); (6,0,1,2:vw1); (6,0,1,3:vw1); (6,0,2,0:vw1); (6,0,2,1:vw1); (6,0,2,2:vw1); (6,0,2,3:vw1); (6,0,3,0:vw1); (6,0,3,1:vw1); (6,0,3,2:vw1); (6,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v101, BufferOOB
/* (d1,vc1,d0,vc0)=(6,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v101, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v17, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v18, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s60
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
v_cndmask_b32 v21, v101, v21, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v24, v21 offset:0                      // load bias
v_lshlrev_b32 v22, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v25, v22, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s60
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v101, v28, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v31, v28 offset:0                      // load bias
v_lshlrev_b32 v29, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v32, v29, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s60
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v101, v35, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v38, v35 offset:0                      // load bias
v_lshlrev_b32 v36, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v39, v36, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v41, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v0, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v101, v42, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v49, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v47, v4, s60
v_lshlrev_b32 v47, 0x2, v47                        // Bias address scaled by BPE
v_cndmask_b32 v47, v101, v47, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v48, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v46, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v54, v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s60
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v101, v52, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v56, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v59, v56, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v4, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v101, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v61, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v64, v61, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v0, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v101, v62, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v66, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v69, v66, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v101, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v71, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v74, v71, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s60
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_cndmask_b32 v72, v101, v72, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v73, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v71, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s60
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
v_cndmask_b32 v77, v101, v77, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v78, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v81, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v84, v81, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v0, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v101, v82, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v86, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v89, v86, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v87, v4, s60
v_lshlrev_b32 v87, 0x2, v87                        // Bias address scaled by BPE
v_cndmask_b32 v87, v101, v87, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v88, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v86, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v91, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v94, v91, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v4, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v101, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v91, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(6,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v96, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v99, v96, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s60
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
v_cndmask_b32 v97, v101, v97, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v98, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+19], acc96          // copy acc to vreg[96]
v_accvgpr_read_b32 v[vgprValuC+26], acc100         // copy acc to vreg[97]
v_accvgpr_read_b32 v[vgprValuC+33], acc104         // copy acc to vreg[98]
v_accvgpr_read_b32 v[vgprValuC+40], acc108         // copy acc to vreg[99]
v_accvgpr_read_b32 v[vgprValuC+45], acc97          // copy acc to vreg[100]
v_accvgpr_read_b32 v[vgprValuC+50], acc101         // copy acc to vreg[101]
v_accvgpr_read_b32 v[vgprValuC+55], acc105         // copy acc to vreg[102]
v_accvgpr_read_b32 v[vgprValuC+60], acc109         // copy acc to vreg[103]
v_accvgpr_read_b32 v[vgprValuC+65], acc98          // copy acc to vreg[104]
v_accvgpr_read_b32 v[vgprValuC+70], acc102         // copy acc to vreg[105]
v_accvgpr_read_b32 v[vgprValuC+75], acc106         // copy acc to vreg[106]
v_accvgpr_read_b32 v[vgprValuC+80], acc110         // copy acc to vreg[107]
v_accvgpr_read_b32 v[vgprValuC+85], acc99          // copy acc to vreg[108]
v_accvgpr_read_b32 v[vgprValuC+90], acc103         // copy acc to vreg[109]
v_accvgpr_read_b32 v[vgprValuC+95], acc107         // copy acc to vreg[110]
v_accvgpr_read_b32 v[vgprValuC+100], acc111        // copy acc to vreg[111]

/* rC *= alpha batchElements=[(6, 0, 0, 0), (6, 0, 0, 1), (6, 0, 0, 2), (6, 0, 0, 3), (6, 0, 1, 0), (6, 0, 1, 1), (6, 0, 1, 2), (6, 0, 1, 3), (6, 0, 2, 0), (6, 0, 2, 1), (6, 0, 2, 2), (6, 0, 2, 3), (6, 0, 3, 0), (6, 0, 3, 1), (6, 0, 3, 2), (6, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+19], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+19]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v19, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+19], v[vgprValuC+19] // check Nan
v_bfe_u32 v9, v[vgprValuC+19], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+19], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+19], v9, v11, s[60:61]
v_lshrrev_b32 v19, 16, v[vgprValuC+19]             // convert C to bf16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v23                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+26], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+26]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v26, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v26, 16, v[vgprValuC+26]             // convert C to bf16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v30                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+33], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+33]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v33, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_lshrrev_b32 v33, 16, v[vgprValuC+33]             // convert C to bf16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v37                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+45], v18, v[vgprValuC+45]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+45]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v45, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_lshrrev_b32 v45, 16, v[vgprValuC+45]             // convert C to bf16
buffer_store_short v45, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+50], v25, v[vgprValuC+50]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v49                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+50]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v50, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v50, 16, v[vgprValuC+50]             // convert C to bf16
buffer_store_short v50, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+55], v32, v[vgprValuC+55]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v54                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+55]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v55, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_lshrrev_b32 v55, 16, v[vgprValuC+55]             // convert C to bf16
buffer_store_short v55, v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v39, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+65], v18, v[vgprValuC+65]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v64                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+65]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v65, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_lshrrev_b32 v65, 16, v[vgprValuC+65]             // convert C to bf16
buffer_store_short v65, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+70], v25, v[vgprValuC+70]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+70], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+70]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v70, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v70, 16, v[vgprValuC+70]             // convert C to bf16
buffer_store_short v70, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+75], v32, v[vgprValuC+75]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v74                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+75]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v75, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_lshrrev_b32 v75, 16, v[vgprValuC+75]             // convert C to bf16
buffer_store_short v75, v71, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v39, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+85], v18, v[vgprValuC+85]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v84                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+85]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v85, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_lshrrev_b32 v85, 16, v[vgprValuC+85]             // convert C to bf16
buffer_store_short v85, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+90], v25, v[vgprValuC+90]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+90]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v90, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v90, 16, v[vgprValuC+90]             // convert C to bf16
buffer_store_short v90, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+95], v32, v[vgprValuC+95]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+95]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v95, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_lshrrev_b32 v95, 16, v[vgprValuC+95]             // convert C to bf16
buffer_store_short v95, v91, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+100], v39, v[vgprValuC+100]  // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v99                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+100]                // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v100, 16, v[vgprValuC+100]           // convert C to bf16
buffer_store_short v100, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #7 (d1,d0,vc1,vc0) = */
/*    (7,0,0,0:vw1); (7,0,0,1:vw1); (7,0,0,2:vw1); (7,0,0,3:vw1); (7,0,1,0:vw1); (7,0,1,1:vw1); (7,0,1,2:vw1); (7,0,1,3:vw1); (7,0,2,0:vw1); (7,0,2,1:vw1); (7,0,2,2:vw1); (7,0,2,3:vw1); (7,0,3,0:vw1); (7,0,3,1:vw1); (7,0,3,2:vw1); (7,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v101, BufferOOB
/* (d1,vc1,d0,vc0)=(7,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v101, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v17, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v18, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s60
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
v_cndmask_b32 v21, v101, v21, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v24, v21 offset:0                      // load bias
v_lshlrev_b32 v22, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v25, v22, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s60
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v101, v28, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v31, v28 offset:0                      // load bias
v_lshlrev_b32 v29, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v32, v29, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s60
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v101, v35, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v38, v35 offset:0                      // load bias
v_lshlrev_b32 v36, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v39, v36, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v41, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v0, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v101, v42, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v49, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v47, v4, s60
v_lshlrev_b32 v47, 0x2, v47                        // Bias address scaled by BPE
v_cndmask_b32 v47, v101, v47, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v48, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v46, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v54, v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s60
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v101, v52, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v56, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v59, v56, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v4, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v101, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v61, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v64, v61, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v0, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v101, v62, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v66, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v69, v66, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v101, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v71, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v74, v71, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s60
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_cndmask_b32 v72, v101, v72, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v73, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v71, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s60
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
v_cndmask_b32 v77, v101, v77, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v78, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v81, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v84, v81, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v0, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v101, v82, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v86, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v89, v86, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v87, v4, s60
v_lshlrev_b32 v87, 0x2, v87                        // Bias address scaled by BPE
v_cndmask_b32 v87, v101, v87, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v88, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v86, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v91, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v94, v91, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v4, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v101, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v91, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(7,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v96, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v99, v96, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s60
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
v_cndmask_b32 v97, v101, v97, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v98, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+19], acc112         // copy acc to vreg[112]
v_accvgpr_read_b32 v[vgprValuC+26], acc116         // copy acc to vreg[113]
v_accvgpr_read_b32 v[vgprValuC+33], acc120         // copy acc to vreg[114]
v_accvgpr_read_b32 v[vgprValuC+40], acc124         // copy acc to vreg[115]
v_accvgpr_read_b32 v[vgprValuC+45], acc113         // copy acc to vreg[116]
v_accvgpr_read_b32 v[vgprValuC+50], acc117         // copy acc to vreg[117]
v_accvgpr_read_b32 v[vgprValuC+55], acc121         // copy acc to vreg[118]
v_accvgpr_read_b32 v[vgprValuC+60], acc125         // copy acc to vreg[119]
v_accvgpr_read_b32 v[vgprValuC+65], acc114         // copy acc to vreg[120]
v_accvgpr_read_b32 v[vgprValuC+70], acc118         // copy acc to vreg[121]
v_accvgpr_read_b32 v[vgprValuC+75], acc122         // copy acc to vreg[122]
v_accvgpr_read_b32 v[vgprValuC+80], acc126         // copy acc to vreg[123]
v_accvgpr_read_b32 v[vgprValuC+85], acc115         // copy acc to vreg[124]
v_accvgpr_read_b32 v[vgprValuC+90], acc119         // copy acc to vreg[125]
v_accvgpr_read_b32 v[vgprValuC+95], acc123         // copy acc to vreg[126]
v_accvgpr_read_b32 v[vgprValuC+100], acc127        // copy acc to vreg[127]

/* rC *= alpha batchElements=[(7, 0, 0, 0), (7, 0, 0, 1), (7, 0, 0, 2), (7, 0, 0, 3), (7, 0, 1, 0), (7, 0, 1, 1), (7, 0, 1, 2), (7, 0, 1, 3), (7, 0, 2, 0), (7, 0, 2, 1), (7, 0, 2, 2), (7, 0, 2, 3), (7, 0, 3, 0), (7, 0, 3, 1), (7, 0, 3, 2), (7, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+19], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+19]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v19, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+19], v[vgprValuC+19] // check Nan
v_bfe_u32 v9, v[vgprValuC+19], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+19], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+19], v9, v11, s[60:61]
v_lshrrev_b32 v19, 16, v[vgprValuC+19]             // convert C to bf16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v23                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+26], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+26]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v26, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v26, 16, v[vgprValuC+26]             // convert C to bf16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v30                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+33], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+33]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v33, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_lshrrev_b32 v33, 16, v[vgprValuC+33]             // convert C to bf16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v37                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+45], v18, v[vgprValuC+45]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+45]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v45, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_lshrrev_b32 v45, 16, v[vgprValuC+45]             // convert C to bf16
buffer_store_short v45, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+50], v25, v[vgprValuC+50]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v49                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+50]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v50, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v50, 16, v[vgprValuC+50]             // convert C to bf16
buffer_store_short v50, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+55], v32, v[vgprValuC+55]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v54                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+55]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v55, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_lshrrev_b32 v55, 16, v[vgprValuC+55]             // convert C to bf16
buffer_store_short v55, v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v39, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+65], v18, v[vgprValuC+65]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v64                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+65]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v65, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_lshrrev_b32 v65, 16, v[vgprValuC+65]             // convert C to bf16
buffer_store_short v65, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+70], v25, v[vgprValuC+70]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+70], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+70]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v70, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v70, 16, v[vgprValuC+70]             // convert C to bf16
buffer_store_short v70, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+75], v32, v[vgprValuC+75]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v74                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+75]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v75, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_lshrrev_b32 v75, 16, v[vgprValuC+75]             // convert C to bf16
buffer_store_short v75, v71, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v39, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+85], v18, v[vgprValuC+85]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v84                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+85]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v85, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_lshrrev_b32 v85, 16, v[vgprValuC+85]             // convert C to bf16
buffer_store_short v85, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+90], v25, v[vgprValuC+90]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+90]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v90, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v90, 16, v[vgprValuC+90]             // convert C to bf16
buffer_store_short v90, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+95], v32, v[vgprValuC+95]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+95]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v95, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_lshrrev_b32 v95, 16, v[vgprValuC+95]             // convert C to bf16
buffer_store_short v95, v91, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+100], v39, v[vgprValuC+100]  // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v99                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+100]                // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v100, 16, v[vgprValuC+100]           // convert C to bf16
buffer_store_short v100, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #8 (d1,d0,vc1,vc0) = */
/*    (8,0,0,0:vw1); (8,0,0,1:vw1); (8,0,0,2:vw1); (8,0,0,3:vw1); (8,0,1,0:vw1); (8,0,1,1:vw1); (8,0,1,2:vw1); (8,0,1,3:vw1); (8,0,2,0:vw1); (8,0,2,1:vw1); (8,0,2,2:vw1); (8,0,2,3:vw1); (8,0,3,0:vw1); (8,0,3,1:vw1); (8,0,3,2:vw1); (8,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v101, BufferOOB
/* (d1,vc1,d0,vc0)=(8,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v101, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v17, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v18, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s60
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
v_cndmask_b32 v21, v101, v21, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v24, v21 offset:0                      // load bias
v_lshlrev_b32 v22, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v25, v22, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s60
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v101, v28, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v31, v28 offset:0                      // load bias
v_lshlrev_b32 v29, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v32, v29, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s60
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v101, v35, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v38, v35 offset:0                      // load bias
v_lshlrev_b32 v36, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v39, v36, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v41, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v0, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v101, v42, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v49, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v47, v4, s60
v_lshlrev_b32 v47, 0x2, v47                        // Bias address scaled by BPE
v_cndmask_b32 v47, v101, v47, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v48, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v46, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v54, v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s60
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v101, v52, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v56, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v59, v56, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v4, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v101, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v61, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v64, v61, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v0, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v101, v62, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v66, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v69, v66, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v101, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v71, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v74, v71, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s60
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_cndmask_b32 v72, v101, v72, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v73, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v71, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s60
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
v_cndmask_b32 v77, v101, v77, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v78, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v81, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v84, v81, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v0, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v101, v82, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v86, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v89, v86, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v87, v4, s60
v_lshlrev_b32 v87, 0x2, v87                        // Bias address scaled by BPE
v_cndmask_b32 v87, v101, v87, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v88, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v86, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v91, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v94, v91, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v4, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v101, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v91, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(8,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v96, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v99, v96, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s60
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
v_cndmask_b32 v97, v101, v97, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v98, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+19], acc128         // copy acc to vreg[128]
v_accvgpr_read_b32 v[vgprValuC+26], acc132         // copy acc to vreg[129]
v_accvgpr_read_b32 v[vgprValuC+33], acc136         // copy acc to vreg[130]
v_accvgpr_read_b32 v[vgprValuC+40], acc140         // copy acc to vreg[131]
v_accvgpr_read_b32 v[vgprValuC+45], acc129         // copy acc to vreg[132]
v_accvgpr_read_b32 v[vgprValuC+50], acc133         // copy acc to vreg[133]
v_accvgpr_read_b32 v[vgprValuC+55], acc137         // copy acc to vreg[134]
v_accvgpr_read_b32 v[vgprValuC+60], acc141         // copy acc to vreg[135]
v_accvgpr_read_b32 v[vgprValuC+65], acc130         // copy acc to vreg[136]
v_accvgpr_read_b32 v[vgprValuC+70], acc134         // copy acc to vreg[137]
v_accvgpr_read_b32 v[vgprValuC+75], acc138         // copy acc to vreg[138]
v_accvgpr_read_b32 v[vgprValuC+80], acc142         // copy acc to vreg[139]
v_accvgpr_read_b32 v[vgprValuC+85], acc131         // copy acc to vreg[140]
v_accvgpr_read_b32 v[vgprValuC+90], acc135         // copy acc to vreg[141]
v_accvgpr_read_b32 v[vgprValuC+95], acc139         // copy acc to vreg[142]
v_accvgpr_read_b32 v[vgprValuC+100], acc143        // copy acc to vreg[143]

/* rC *= alpha batchElements=[(8, 0, 0, 0), (8, 0, 0, 1), (8, 0, 0, 2), (8, 0, 0, 3), (8, 0, 1, 0), (8, 0, 1, 1), (8, 0, 1, 2), (8, 0, 1, 3), (8, 0, 2, 0), (8, 0, 2, 1), (8, 0, 2, 2), (8, 0, 2, 3), (8, 0, 3, 0), (8, 0, 3, 1), (8, 0, 3, 2), (8, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+19], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+19]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v19, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+19], v[vgprValuC+19] // check Nan
v_bfe_u32 v9, v[vgprValuC+19], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+19], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+19], v9, v11, s[60:61]
v_lshrrev_b32 v19, 16, v[vgprValuC+19]             // convert C to bf16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v23                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+26], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+26]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v26, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v26, 16, v[vgprValuC+26]             // convert C to bf16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v30                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+33], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+33]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v33, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_lshrrev_b32 v33, 16, v[vgprValuC+33]             // convert C to bf16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v37                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+45], v18, v[vgprValuC+45]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+45]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v45, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_lshrrev_b32 v45, 16, v[vgprValuC+45]             // convert C to bf16
buffer_store_short v45, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+50], v25, v[vgprValuC+50]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v49                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+50]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v50, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v50, 16, v[vgprValuC+50]             // convert C to bf16
buffer_store_short v50, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+55], v32, v[vgprValuC+55]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v54                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+55]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v55, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_lshrrev_b32 v55, 16, v[vgprValuC+55]             // convert C to bf16
buffer_store_short v55, v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v39, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+65], v18, v[vgprValuC+65]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v64                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+65]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v65, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_lshrrev_b32 v65, 16, v[vgprValuC+65]             // convert C to bf16
buffer_store_short v65, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+70], v25, v[vgprValuC+70]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+70], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+70]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v70, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v70, 16, v[vgprValuC+70]             // convert C to bf16
buffer_store_short v70, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+75], v32, v[vgprValuC+75]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v74                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+75]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v75, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_lshrrev_b32 v75, 16, v[vgprValuC+75]             // convert C to bf16
buffer_store_short v75, v71, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v39, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+85], v18, v[vgprValuC+85]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v84                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+85]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v85, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_lshrrev_b32 v85, 16, v[vgprValuC+85]             // convert C to bf16
buffer_store_short v85, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+90], v25, v[vgprValuC+90]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+90]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v90, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v90, 16, v[vgprValuC+90]             // convert C to bf16
buffer_store_short v90, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+95], v32, v[vgprValuC+95]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+95]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v95, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_lshrrev_b32 v95, 16, v[vgprValuC+95]             // convert C to bf16
buffer_store_short v95, v91, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+100], v39, v[vgprValuC+100]  // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v99                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+100]                // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v100, 16, v[vgprValuC+100]           // convert C to bf16
buffer_store_short v100, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #9 (d1,d0,vc1,vc0) = */
/*    (9,0,0,0:vw1); (9,0,0,1:vw1); (9,0,0,2:vw1); (9,0,0,3:vw1); (9,0,1,0:vw1); (9,0,1,1:vw1); (9,0,1,2:vw1); (9,0,1,3:vw1); (9,0,2,0:vw1); (9,0,2,1:vw1); (9,0,2,2:vw1); (9,0,2,3:vw1); (9,0,3,0:vw1); (9,0,3,1:vw1); (9,0,3,2:vw1); (9,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v101, BufferOOB
/* (d1,vc1,d0,vc0)=(9,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v101, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v17, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v18, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s60
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
v_cndmask_b32 v21, v101, v21, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v24, v21 offset:0                      // load bias
v_lshlrev_b32 v22, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v25, v22, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s60
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v101, v28, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v31, v28 offset:0                      // load bias
v_lshlrev_b32 v29, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v32, v29, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s60
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v101, v35, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v38, v35 offset:0                      // load bias
v_lshlrev_b32 v36, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v39, v36, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v41, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v0, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v101, v42, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v49, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v47, v4, s60
v_lshlrev_b32 v47, 0x2, v47                        // Bias address scaled by BPE
v_cndmask_b32 v47, v101, v47, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v48, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v46, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v54, v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s60
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v101, v52, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v56, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v59, v56, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v4, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v101, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v61, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v64, v61, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v0, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v101, v62, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v66, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v69, v66, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v101, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v71, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v74, v71, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s60
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_cndmask_b32 v72, v101, v72, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v73, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v71, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s60
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
v_cndmask_b32 v77, v101, v77, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v78, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v81, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v84, v81, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v0, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v101, v82, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v86, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v89, v86, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v87, v4, s60
v_lshlrev_b32 v87, 0x2, v87                        // Bias address scaled by BPE
v_cndmask_b32 v87, v101, v87, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v88, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v86, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v91, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v94, v91, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v4, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v101, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v91, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(9,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v96, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v99, v96, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s60
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
v_cndmask_b32 v97, v101, v97, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v98, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+19], acc144         // copy acc to vreg[144]
v_accvgpr_read_b32 v[vgprValuC+26], acc148         // copy acc to vreg[145]
v_accvgpr_read_b32 v[vgprValuC+33], acc152         // copy acc to vreg[146]
v_accvgpr_read_b32 v[vgprValuC+40], acc156         // copy acc to vreg[147]
v_accvgpr_read_b32 v[vgprValuC+45], acc145         // copy acc to vreg[148]
v_accvgpr_read_b32 v[vgprValuC+50], acc149         // copy acc to vreg[149]
v_accvgpr_read_b32 v[vgprValuC+55], acc153         // copy acc to vreg[150]
v_accvgpr_read_b32 v[vgprValuC+60], acc157         // copy acc to vreg[151]
v_accvgpr_read_b32 v[vgprValuC+65], acc146         // copy acc to vreg[152]
v_accvgpr_read_b32 v[vgprValuC+70], acc150         // copy acc to vreg[153]
v_accvgpr_read_b32 v[vgprValuC+75], acc154         // copy acc to vreg[154]
v_accvgpr_read_b32 v[vgprValuC+80], acc158         // copy acc to vreg[155]
v_accvgpr_read_b32 v[vgprValuC+85], acc147         // copy acc to vreg[156]
v_accvgpr_read_b32 v[vgprValuC+90], acc151         // copy acc to vreg[157]
v_accvgpr_read_b32 v[vgprValuC+95], acc155         // copy acc to vreg[158]
v_accvgpr_read_b32 v[vgprValuC+100], acc159        // copy acc to vreg[159]

/* rC *= alpha batchElements=[(9, 0, 0, 0), (9, 0, 0, 1), (9, 0, 0, 2), (9, 0, 0, 3), (9, 0, 1, 0), (9, 0, 1, 1), (9, 0, 1, 2), (9, 0, 1, 3), (9, 0, 2, 0), (9, 0, 2, 1), (9, 0, 2, 2), (9, 0, 2, 3), (9, 0, 3, 0), (9, 0, 3, 1), (9, 0, 3, 2), (9, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+19], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+19]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v19, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+19], v[vgprValuC+19] // check Nan
v_bfe_u32 v9, v[vgprValuC+19], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+19], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+19], v9, v11, s[60:61]
v_lshrrev_b32 v19, 16, v[vgprValuC+19]             // convert C to bf16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v23                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+26], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+26]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v26, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v26, 16, v[vgprValuC+26]             // convert C to bf16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v30                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+33], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+33]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v33, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_lshrrev_b32 v33, 16, v[vgprValuC+33]             // convert C to bf16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v37                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+45], v18, v[vgprValuC+45]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+45]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v45, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_lshrrev_b32 v45, 16, v[vgprValuC+45]             // convert C to bf16
buffer_store_short v45, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+50], v25, v[vgprValuC+50]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v49                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+50]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v50, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v50, 16, v[vgprValuC+50]             // convert C to bf16
buffer_store_short v50, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+55], v32, v[vgprValuC+55]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v54                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+55]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v55, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_lshrrev_b32 v55, 16, v[vgprValuC+55]             // convert C to bf16
buffer_store_short v55, v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v39, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+65], v18, v[vgprValuC+65]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v64                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+65]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v65, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_lshrrev_b32 v65, 16, v[vgprValuC+65]             // convert C to bf16
buffer_store_short v65, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+70], v25, v[vgprValuC+70]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+70], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+70]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v70, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v70, 16, v[vgprValuC+70]             // convert C to bf16
buffer_store_short v70, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+75], v32, v[vgprValuC+75]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v74                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+75]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v75, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_lshrrev_b32 v75, 16, v[vgprValuC+75]             // convert C to bf16
buffer_store_short v75, v71, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v39, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+85], v18, v[vgprValuC+85]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v84                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+85]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v85, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_lshrrev_b32 v85, 16, v[vgprValuC+85]             // convert C to bf16
buffer_store_short v85, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+90], v25, v[vgprValuC+90]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+90]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v90, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v90, 16, v[vgprValuC+90]             // convert C to bf16
buffer_store_short v90, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+95], v32, v[vgprValuC+95]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+95]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v95, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_lshrrev_b32 v95, 16, v[vgprValuC+95]             // convert C to bf16
buffer_store_short v95, v91, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+100], v39, v[vgprValuC+100]  // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v99                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+100]                // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v100, 16, v[vgprValuC+100]           // convert C to bf16
buffer_store_short v100, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #10 (d1,d0,vc1,vc0) = */
/*    (10,0,0,0:vw1); (10,0,0,1:vw1); (10,0,0,2:vw1); (10,0,0,3:vw1); (10,0,1,0:vw1); (10,0,1,1:vw1); (10,0,1,2:vw1); (10,0,1,3:vw1); (10,0,2,0:vw1); (10,0,2,1:vw1); (10,0,2,2:vw1); (10,0,2,3:vw1); (10,0,3,0:vw1); (10,0,3,1:vw1); (10,0,3,2:vw1); (10,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v101, BufferOOB
/* (d1,vc1,d0,vc0)=(10,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v101, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v17, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v18, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s60
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
v_cndmask_b32 v21, v101, v21, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v24, v21 offset:0                      // load bias
v_lshlrev_b32 v22, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v25, v22, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s60
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v101, v28, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v31, v28 offset:0                      // load bias
v_lshlrev_b32 v29, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v32, v29, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s60
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v101, v35, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v38, v35 offset:0                      // load bias
v_lshlrev_b32 v36, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v39, v36, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v41, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v0, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v101, v42, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v49, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v47, v4, s60
v_lshlrev_b32 v47, 0x2, v47                        // Bias address scaled by BPE
v_cndmask_b32 v47, v101, v47, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v48, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v46, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v54, v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s60
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v101, v52, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v56, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v59, v56, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v4, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v101, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v61, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v64, v61, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v0, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v101, v62, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v66, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v69, v66, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v101, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v71, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v74, v71, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s60
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_cndmask_b32 v72, v101, v72, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v73, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v71, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s60
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
v_cndmask_b32 v77, v101, v77, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v78, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v81, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v84, v81, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v0, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v101, v82, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v86, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v89, v86, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v87, v4, s60
v_lshlrev_b32 v87, 0x2, v87                        // Bias address scaled by BPE
v_cndmask_b32 v87, v101, v87, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v88, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v86, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v91, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v94, v91, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v4, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v101, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v91, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(10,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v96, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v99, v96, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s60
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
v_cndmask_b32 v97, v101, v97, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v98, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+19], acc160         // copy acc to vreg[160]
v_accvgpr_read_b32 v[vgprValuC+26], acc164         // copy acc to vreg[161]
v_accvgpr_read_b32 v[vgprValuC+33], acc168         // copy acc to vreg[162]
v_accvgpr_read_b32 v[vgprValuC+40], acc172         // copy acc to vreg[163]
v_accvgpr_read_b32 v[vgprValuC+45], acc161         // copy acc to vreg[164]
v_accvgpr_read_b32 v[vgprValuC+50], acc165         // copy acc to vreg[165]
v_accvgpr_read_b32 v[vgprValuC+55], acc169         // copy acc to vreg[166]
v_accvgpr_read_b32 v[vgprValuC+60], acc173         // copy acc to vreg[167]
v_accvgpr_read_b32 v[vgprValuC+65], acc162         // copy acc to vreg[168]
v_accvgpr_read_b32 v[vgprValuC+70], acc166         // copy acc to vreg[169]
v_accvgpr_read_b32 v[vgprValuC+75], acc170         // copy acc to vreg[170]
v_accvgpr_read_b32 v[vgprValuC+80], acc174         // copy acc to vreg[171]
v_accvgpr_read_b32 v[vgprValuC+85], acc163         // copy acc to vreg[172]
v_accvgpr_read_b32 v[vgprValuC+90], acc167         // copy acc to vreg[173]
v_accvgpr_read_b32 v[vgprValuC+95], acc171         // copy acc to vreg[174]
v_accvgpr_read_b32 v[vgprValuC+100], acc175        // copy acc to vreg[175]

/* rC *= alpha batchElements=[(10, 0, 0, 0), (10, 0, 0, 1), (10, 0, 0, 2), (10, 0, 0, 3), (10, 0, 1, 0), (10, 0, 1, 1), (10, 0, 1, 2), (10, 0, 1, 3), (10, 0, 2, 0), (10, 0, 2, 1), (10, 0, 2, 2), (10, 0, 2, 3), (10, 0, 3, 0), (10, 0, 3, 1), (10, 0, 3, 2), (10, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+19], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+19]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v19, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+19], v[vgprValuC+19] // check Nan
v_bfe_u32 v9, v[vgprValuC+19], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+19], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+19], v9, v11, s[60:61]
v_lshrrev_b32 v19, 16, v[vgprValuC+19]             // convert C to bf16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v23                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+26], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+26]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v26, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v26, 16, v[vgprValuC+26]             // convert C to bf16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v30                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+33], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+33]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v33, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_lshrrev_b32 v33, 16, v[vgprValuC+33]             // convert C to bf16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v37                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+45], v18, v[vgprValuC+45]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+45]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v45, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_lshrrev_b32 v45, 16, v[vgprValuC+45]             // convert C to bf16
buffer_store_short v45, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+50], v25, v[vgprValuC+50]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v49                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+50]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v50, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v50, 16, v[vgprValuC+50]             // convert C to bf16
buffer_store_short v50, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+55], v32, v[vgprValuC+55]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v54                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+55]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v55, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_lshrrev_b32 v55, 16, v[vgprValuC+55]             // convert C to bf16
buffer_store_short v55, v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v39, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+65], v18, v[vgprValuC+65]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v64                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+65]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v65, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_lshrrev_b32 v65, 16, v[vgprValuC+65]             // convert C to bf16
buffer_store_short v65, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+70], v25, v[vgprValuC+70]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+70], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+70]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v70, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v70, 16, v[vgprValuC+70]             // convert C to bf16
buffer_store_short v70, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+75], v32, v[vgprValuC+75]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v74                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+75]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v75, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_lshrrev_b32 v75, 16, v[vgprValuC+75]             // convert C to bf16
buffer_store_short v75, v71, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v39, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+85], v18, v[vgprValuC+85]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v84                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+85]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v85, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_lshrrev_b32 v85, 16, v[vgprValuC+85]             // convert C to bf16
buffer_store_short v85, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+90], v25, v[vgprValuC+90]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+90]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v90, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v90, 16, v[vgprValuC+90]             // convert C to bf16
buffer_store_short v90, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+95], v32, v[vgprValuC+95]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+95]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v95, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_lshrrev_b32 v95, 16, v[vgprValuC+95]             // convert C to bf16
buffer_store_short v95, v91, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+100], v39, v[vgprValuC+100]  // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v99                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+100]                // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v100, 16, v[vgprValuC+100]           // convert C to bf16
buffer_store_short v100, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #11 (d1,d0,vc1,vc0) = */
/*    (11,0,0,0:vw1); (11,0,0,1:vw1); (11,0,0,2:vw1); (11,0,0,3:vw1); (11,0,1,0:vw1); (11,0,1,1:vw1); (11,0,1,2:vw1); (11,0,1,3:vw1); (11,0,2,0:vw1); (11,0,2,1:vw1); (11,0,2,2:vw1); (11,0,2,3:vw1); (11,0,3,0:vw1); (11,0,3,1:vw1); (11,0,3,2:vw1); (11,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v101, BufferOOB
/* (d1,vc1,d0,vc0)=(11,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v101, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v17, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v18, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s60
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
v_cndmask_b32 v21, v101, v21, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v24, v21 offset:0                      // load bias
v_lshlrev_b32 v22, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v25, v22, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s60
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v101, v28, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v31, v28 offset:0                      // load bias
v_lshlrev_b32 v29, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v32, v29, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s60
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v101, v35, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v38, v35 offset:0                      // load bias
v_lshlrev_b32 v36, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v39, v36, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v41, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v0, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v101, v42, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v49, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v47, v4, s60
v_lshlrev_b32 v47, 0x2, v47                        // Bias address scaled by BPE
v_cndmask_b32 v47, v101, v47, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v48, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v46, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v54, v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s60
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v101, v52, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v56, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v59, v56, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v4, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v101, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v61, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v64, v61, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v0, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v101, v62, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v66, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v69, v66, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v101, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v71, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v74, v71, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s60
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_cndmask_b32 v72, v101, v72, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v73, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v71, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s60
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
v_cndmask_b32 v77, v101, v77, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v78, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v81, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v84, v81, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v0, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v101, v82, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v86, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v89, v86, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v87, v4, s60
v_lshlrev_b32 v87, 0x2, v87                        // Bias address scaled by BPE
v_cndmask_b32 v87, v101, v87, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v88, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v86, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v91, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v94, v91, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v4, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v101, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v91, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(11,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v96, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v99, v96, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s60
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
v_cndmask_b32 v97, v101, v97, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v98, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+19], acc176         // copy acc to vreg[176]
v_accvgpr_read_b32 v[vgprValuC+26], acc180         // copy acc to vreg[177]
v_accvgpr_read_b32 v[vgprValuC+33], acc184         // copy acc to vreg[178]
v_accvgpr_read_b32 v[vgprValuC+40], acc188         // copy acc to vreg[179]
v_accvgpr_read_b32 v[vgprValuC+45], acc177         // copy acc to vreg[180]
v_accvgpr_read_b32 v[vgprValuC+50], acc181         // copy acc to vreg[181]
v_accvgpr_read_b32 v[vgprValuC+55], acc185         // copy acc to vreg[182]
v_accvgpr_read_b32 v[vgprValuC+60], acc189         // copy acc to vreg[183]
v_accvgpr_read_b32 v[vgprValuC+65], acc178         // copy acc to vreg[184]
v_accvgpr_read_b32 v[vgprValuC+70], acc182         // copy acc to vreg[185]
v_accvgpr_read_b32 v[vgprValuC+75], acc186         // copy acc to vreg[186]
v_accvgpr_read_b32 v[vgprValuC+80], acc190         // copy acc to vreg[187]
v_accvgpr_read_b32 v[vgprValuC+85], acc179         // copy acc to vreg[188]
v_accvgpr_read_b32 v[vgprValuC+90], acc183         // copy acc to vreg[189]
v_accvgpr_read_b32 v[vgprValuC+95], acc187         // copy acc to vreg[190]
v_accvgpr_read_b32 v[vgprValuC+100], acc191        // copy acc to vreg[191]

/* rC *= alpha batchElements=[(11, 0, 0, 0), (11, 0, 0, 1), (11, 0, 0, 2), (11, 0, 0, 3), (11, 0, 1, 0), (11, 0, 1, 1), (11, 0, 1, 2), (11, 0, 1, 3), (11, 0, 2, 0), (11, 0, 2, 1), (11, 0, 2, 2), (11, 0, 2, 3), (11, 0, 3, 0), (11, 0, 3, 1), (11, 0, 3, 2), (11, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+19], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+19]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v19, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+19], v[vgprValuC+19] // check Nan
v_bfe_u32 v9, v[vgprValuC+19], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+19], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+19], v9, v11, s[60:61]
v_lshrrev_b32 v19, 16, v[vgprValuC+19]             // convert C to bf16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v23                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+26], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+26]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v26, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v26, 16, v[vgprValuC+26]             // convert C to bf16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v30                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+33], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+33]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v33, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_lshrrev_b32 v33, 16, v[vgprValuC+33]             // convert C to bf16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v37                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+45], v18, v[vgprValuC+45]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+45]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v45, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_lshrrev_b32 v45, 16, v[vgprValuC+45]             // convert C to bf16
buffer_store_short v45, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+50], v25, v[vgprValuC+50]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v49                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+50]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v50, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v50, 16, v[vgprValuC+50]             // convert C to bf16
buffer_store_short v50, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+55], v32, v[vgprValuC+55]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v54                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+55]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v55, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_lshrrev_b32 v55, 16, v[vgprValuC+55]             // convert C to bf16
buffer_store_short v55, v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v39, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+65], v18, v[vgprValuC+65]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v64                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+65]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v65, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_lshrrev_b32 v65, 16, v[vgprValuC+65]             // convert C to bf16
buffer_store_short v65, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+70], v25, v[vgprValuC+70]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+70], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+70]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v70, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v70, 16, v[vgprValuC+70]             // convert C to bf16
buffer_store_short v70, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+75], v32, v[vgprValuC+75]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v74                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+75]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v75, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_lshrrev_b32 v75, 16, v[vgprValuC+75]             // convert C to bf16
buffer_store_short v75, v71, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v39, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+85], v18, v[vgprValuC+85]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v84                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+85]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v85, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_lshrrev_b32 v85, 16, v[vgprValuC+85]             // convert C to bf16
buffer_store_short v85, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+90], v25, v[vgprValuC+90]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+90]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v90, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v90, 16, v[vgprValuC+90]             // convert C to bf16
buffer_store_short v90, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+95], v32, v[vgprValuC+95]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+95]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v95, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_lshrrev_b32 v95, 16, v[vgprValuC+95]             // convert C to bf16
buffer_store_short v95, v91, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+100], v39, v[vgprValuC+100]  // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v99                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+100]                // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v100, 16, v[vgprValuC+100]           // convert C to bf16
buffer_store_short v100, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #12 (d1,d0,vc1,vc0) = */
/*    (12,0,0,0:vw1); (12,0,0,1:vw1); (12,0,0,2:vw1); (12,0,0,3:vw1); (12,0,1,0:vw1); (12,0,1,1:vw1); (12,0,1,2:vw1); (12,0,1,3:vw1); (12,0,2,0:vw1); (12,0,2,1:vw1); (12,0,2,2:vw1); (12,0,2,3:vw1); (12,0,3,0:vw1); (12,0,3,1:vw1); (12,0,3,2:vw1); (12,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v101, BufferOOB
/* (d1,vc1,d0,vc0)=(12,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v101, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v17, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v18, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s60
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
v_cndmask_b32 v21, v101, v21, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v24, v21 offset:0                      // load bias
v_lshlrev_b32 v22, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v25, v22, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s60
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v101, v28, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v31, v28 offset:0                      // load bias
v_lshlrev_b32 v29, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v32, v29, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s60
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v101, v35, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v38, v35 offset:0                      // load bias
v_lshlrev_b32 v36, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v39, v36, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v41, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v0, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v101, v42, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v49, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v47, v4, s60
v_lshlrev_b32 v47, 0x2, v47                        // Bias address scaled by BPE
v_cndmask_b32 v47, v101, v47, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v48, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v46, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v54, v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s60
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v101, v52, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v56, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v59, v56, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v4, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v101, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v61, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v64, v61, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v0, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v101, v62, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v66, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v69, v66, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v101, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v71, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v74, v71, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s60
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_cndmask_b32 v72, v101, v72, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v73, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v71, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s60
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
v_cndmask_b32 v77, v101, v77, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v78, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v81, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v84, v81, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v0, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v101, v82, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v86, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v89, v86, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v87, v4, s60
v_lshlrev_b32 v87, 0x2, v87                        // Bias address scaled by BPE
v_cndmask_b32 v87, v101, v87, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v88, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v86, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v91, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v94, v91, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v4, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v101, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v91, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(12,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v96, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v99, v96, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s60
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
v_cndmask_b32 v97, v101, v97, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v98, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+19], acc192         // copy acc to vreg[192]
v_accvgpr_read_b32 v[vgprValuC+26], acc196         // copy acc to vreg[193]
v_accvgpr_read_b32 v[vgprValuC+33], acc200         // copy acc to vreg[194]
v_accvgpr_read_b32 v[vgprValuC+40], acc204         // copy acc to vreg[195]
v_accvgpr_read_b32 v[vgprValuC+45], acc193         // copy acc to vreg[196]
v_accvgpr_read_b32 v[vgprValuC+50], acc197         // copy acc to vreg[197]
v_accvgpr_read_b32 v[vgprValuC+55], acc201         // copy acc to vreg[198]
v_accvgpr_read_b32 v[vgprValuC+60], acc205         // copy acc to vreg[199]
v_accvgpr_read_b32 v[vgprValuC+65], acc194         // copy acc to vreg[200]
v_accvgpr_read_b32 v[vgprValuC+70], acc198         // copy acc to vreg[201]
v_accvgpr_read_b32 v[vgprValuC+75], acc202         // copy acc to vreg[202]
v_accvgpr_read_b32 v[vgprValuC+80], acc206         // copy acc to vreg[203]
v_accvgpr_read_b32 v[vgprValuC+85], acc195         // copy acc to vreg[204]
v_accvgpr_read_b32 v[vgprValuC+90], acc199         // copy acc to vreg[205]
v_accvgpr_read_b32 v[vgprValuC+95], acc203         // copy acc to vreg[206]
v_accvgpr_read_b32 v[vgprValuC+100], acc207        // copy acc to vreg[207]

/* rC *= alpha batchElements=[(12, 0, 0, 0), (12, 0, 0, 1), (12, 0, 0, 2), (12, 0, 0, 3), (12, 0, 1, 0), (12, 0, 1, 1), (12, 0, 1, 2), (12, 0, 1, 3), (12, 0, 2, 0), (12, 0, 2, 1), (12, 0, 2, 2), (12, 0, 2, 3), (12, 0, 3, 0), (12, 0, 3, 1), (12, 0, 3, 2), (12, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+19], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+19]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v19, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+19], v[vgprValuC+19] // check Nan
v_bfe_u32 v9, v[vgprValuC+19], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+19], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+19], v9, v11, s[60:61]
v_lshrrev_b32 v19, 16, v[vgprValuC+19]             // convert C to bf16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v23                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+26], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+26]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v26, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v26, 16, v[vgprValuC+26]             // convert C to bf16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v30                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+33], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+33]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v33, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_lshrrev_b32 v33, 16, v[vgprValuC+33]             // convert C to bf16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v37                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+45], v18, v[vgprValuC+45]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+45]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v45, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_lshrrev_b32 v45, 16, v[vgprValuC+45]             // convert C to bf16
buffer_store_short v45, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+50], v25, v[vgprValuC+50]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v49                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+50]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v50, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v50, 16, v[vgprValuC+50]             // convert C to bf16
buffer_store_short v50, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+55], v32, v[vgprValuC+55]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v54                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+55]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v55, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_lshrrev_b32 v55, 16, v[vgprValuC+55]             // convert C to bf16
buffer_store_short v55, v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v39, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+65], v18, v[vgprValuC+65]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v64                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+65]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v65, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_lshrrev_b32 v65, 16, v[vgprValuC+65]             // convert C to bf16
buffer_store_short v65, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+70], v25, v[vgprValuC+70]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+70], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+70]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v70, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v70, 16, v[vgprValuC+70]             // convert C to bf16
buffer_store_short v70, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+75], v32, v[vgprValuC+75]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v74                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+75]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v75, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_lshrrev_b32 v75, 16, v[vgprValuC+75]             // convert C to bf16
buffer_store_short v75, v71, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v39, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+85], v18, v[vgprValuC+85]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v84                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+85]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v85, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_lshrrev_b32 v85, 16, v[vgprValuC+85]             // convert C to bf16
buffer_store_short v85, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+90], v25, v[vgprValuC+90]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+90]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v90, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v90, 16, v[vgprValuC+90]             // convert C to bf16
buffer_store_short v90, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+95], v32, v[vgprValuC+95]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+95]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v95, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_lshrrev_b32 v95, 16, v[vgprValuC+95]             // convert C to bf16
buffer_store_short v95, v91, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+100], v39, v[vgprValuC+100]  // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v99                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+100]                // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v100, 16, v[vgprValuC+100]           // convert C to bf16
buffer_store_short v100, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
/* optSingleColVgpr=0 optSharedColVgpr=0 optSGPRUsage=BufferLoad_Edge_Mask optSrdIncForRow=0 biasDim=0 */

/******************************************/
/* Global Write Beta Edge Batch #13 (d1,d0,vc1,vc0) = */
/*    (13,0,0,0:vw1); (13,0,0,1:vw1); (13,0,0,2:vw1); (13,0,0,3:vw1); (13,0,1,0:vw1); (13,0,1,1:vw1); (13,0,1,2:vw1); (13,0,1,3:vw1); (13,0,2,0:vw1); (13,0,2,1:vw1); (13,0,2,2:vw1); (13,0,2,3:vw1); (13,0,3,0:vw1); (13,0,3,1:vw1); (13,0,3,2:vw1); (13,0,3,3:vw1) */
/******************************************/

/* calc coords, apply mask, and issue loads (if necessary) */
v_mov_b32 v101, BufferOOB
/* (d1,vc1,d0,vc0)=(13,0,0,0) */
v_add_co_u32 v1, vcc, v1, 13                       // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
s_mul_i32 s60, s[sgprStrideC1J], 13                // scale stride
v_add_i32 v2, v2, s60                              // ROWINC- Move cinRowPtr to next row
s_mul_i32 s60, s[sgprStrideD1J], 13                // scale stride
v_add_i32 v3, v3, s60                              // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v13, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v16, v13, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v14, v0, s60
v_lshlrev_b32 v14, 0x2, v14                        // Bias address scaled by BPE
v_cndmask_b32 v14, v101, v14, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v17, v14 offset:0                      // load bias
v_lshlrev_b32 v15, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v18, v15, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v13, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v13, v101, v13, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,0,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v20, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v23, v20, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v21, v4, s60
v_lshlrev_b32 v21, 0x2, v21                        // Bias address scaled by BPE
v_cndmask_b32 v21, v101, v21, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v24, v21 offset:0                      // load bias
v_lshlrev_b32 v22, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v25, v22, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v20, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v20, v101, v20, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,0,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v27, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v30, v27, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v28, v4, s60
v_lshlrev_b32 v28, 0x2, v28                        // Bias address scaled by BPE
v_cndmask_b32 v28, v101, v28, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v31, v28 offset:0                      // load bias
v_lshlrev_b32 v29, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v32, v29, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v27, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v27, v101, v27, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,0,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v34, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v37, v34, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v35, v4, s60
v_lshlrev_b32 v35, 0x2, v35                        // Bias address scaled by BPE
v_cndmask_b32 v35, v101, v35, s[64:65]             // LDBias clip if OOB. offset
ds_read_b32 v38, v35 offset:0                      // load bias
v_lshlrev_b32 v36, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
buffer_load_dword v39, v36, s[sgprSrdScaleAlphaVec:sgprSrdScaleAlphaVec+3], 0 offen offset:0 // load scaleAlphaVecI
v_add_lshl_u32 v34, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v34, v101, v34, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,1,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v41, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v44, v41, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v42, v0, s60
v_lshlrev_b32 v42, 0x2, v42                        // Bias address scaled by BPE
v_cndmask_b32 v42, v101, v42, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v43, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v41, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v41, v101, v41, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,1,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v46, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v49, v46, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v47, v4, s60
v_lshlrev_b32 v47, 0x2, v47                        // Bias address scaled by BPE
v_cndmask_b32 v47, v101, v47, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v48, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v46, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v46, v101, v46, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,1,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v51, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v54, v51, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v52, v4, s60
v_lshlrev_b32 v52, 0x2, v52                        // Bias address scaled by BPE
v_cndmask_b32 v52, v101, v52, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v53, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v51, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v51, v101, v51, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,1,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v56, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v59, v56, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v57, v4, s60
v_lshlrev_b32 v57, 0x2, v57                        // Bias address scaled by BPE
v_cndmask_b32 v57, v101, v57, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v58, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v56, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v56, v101, v56, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,2,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v61, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v64, v61, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v62, v0, s60
v_lshlrev_b32 v62, 0x2, v62                        // Bias address scaled by BPE
v_cndmask_b32 v62, v101, v62, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v63, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v61, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v61, v101, v61, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,2,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v66, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v69, v66, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v67, v4, s60
v_lshlrev_b32 v67, 0x2, v67                        // Bias address scaled by BPE
v_cndmask_b32 v67, v101, v67, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v68, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v66, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v66, v101, v66, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,2,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v71, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v74, v71, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v72, v4, s60
v_lshlrev_b32 v72, 0x2, v72                        // Bias address scaled by BPE
v_cndmask_b32 v72, v101, v72, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v73, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v71, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v71, v101, v71, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,2,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v76, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v79, v76, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v77, v4, s60
v_lshlrev_b32 v77, 0x2, v77                        // Bias address scaled by BPE
v_cndmask_b32 v77, v101, v77, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v78, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v76, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v76, v101, v76, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,3,0,0) */
v_add_co_u32 v1, vcc, v1, 1                        // coord1.1: coord1Vgpr += d1*sg1*VW + vc1

/* Fix for UseInitialStridesCD, emitAddressSetupCode */
v_add_u32 v2, v2, s[sgprStrideC1J]                 // ROWINC- Move cinRowPtr to next row
v_add_u32 v3, v3, s[sgprStrideD1J]                 // Move coutRowPtrD to next row
v_cmp_lt_u32 s[60:61], v0, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v81, v2, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v84, v81, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v82, v0, s60
v_lshlrev_b32 v82, 0x2, v82                        // Bias address scaled by BPE
v_cndmask_b32 v82, v101, v82, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v83, 0x2, v0                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v81, v3, v0, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v81, v101, v81, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,3,0,1) */
v_add_co_u32 v4, vcc, v0, 1                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v86, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v89, v86, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v87, v4, s60
v_lshlrev_b32 v87, 0x2, v87                        // Bias address scaled by BPE
v_cndmask_b32 v87, v101, v87, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v88, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v86, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v86, v101, v86, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,3,0,2) */
v_add_co_u32 v4, vcc, v0, 2                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v91, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v94, v91, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v92, v4, s60
v_lshlrev_b32 v92, 0x2, v92                        // Bias address scaled by BPE
v_cndmask_b32 v92, v101, v92, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v93, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v91, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v91, v101, v91, s[64:65]             // LDD clip if OOB. offset
/* (d1,vc1,d0,vc0)=(13,3,0,3) */
v_add_co_u32 v4, vcc, v0, 3                        // coord0.1: coord0 += d0*sg0*VW + vc0
v_cmp_lt_u32 s[60:61], v4, s[sgprSizeI]            // coord0 < size0
v_cmp_lt_u32 s[64:65], v1, s[sgprSizeJ]            // coord1 < size1
s_and_b64 s[64:65], s[60:61], s[64:65]             // in0 && in1
v_add_lshl_u32 v96, v2, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDC clip if OOB. offset
buffer_load_short_d16 v99, v96, s[sgprSrdC:sgprSrdC+3], 0 offen offset:0 // load C
s_mul_i32 s60, 256, s[sgprWorkGroup0]              // wgp0 * MT0
v_sub_u32 v97, v4, s60
v_lshlrev_b32 v97, 0x2, v97                        // Bias address scaled by BPE
v_cndmask_b32 v97, v101, v97, s[64:65]             // LDBias clip if OOB. offset
v_lshlrev_b32 v98, 0x2, v4                         // ScaleAlphaVec address scaled by BPE
v_add_lshl_u32 v96, v3, v4, 0x1                    // scaleToBpe: accumulate d0 lower and *= bpe into Cin addr
v_cndmask_b32 v96, v101, v96, s[64:65]             // LDD clip if OOB. offset
v_accvgpr_read_b32 v[vgprValuC+19], acc208         // copy acc to vreg[208]
v_accvgpr_read_b32 v[vgprValuC+26], acc212         // copy acc to vreg[209]
v_accvgpr_read_b32 v[vgprValuC+33], acc216         // copy acc to vreg[210]
v_accvgpr_read_b32 v[vgprValuC+40], acc220         // copy acc to vreg[211]
v_accvgpr_read_b32 v[vgprValuC+45], acc209         // copy acc to vreg[212]
v_accvgpr_read_b32 v[vgprValuC+50], acc213         // copy acc to vreg[213]
v_accvgpr_read_b32 v[vgprValuC+55], acc217         // copy acc to vreg[214]
v_accvgpr_read_b32 v[vgprValuC+60], acc221         // copy acc to vreg[215]
v_accvgpr_read_b32 v[vgprValuC+65], acc210         // copy acc to vreg[216]
v_accvgpr_read_b32 v[vgprValuC+70], acc214         // copy acc to vreg[217]
v_accvgpr_read_b32 v[vgprValuC+75], acc218         // copy acc to vreg[218]
v_accvgpr_read_b32 v[vgprValuC+80], acc222         // copy acc to vreg[219]
v_accvgpr_read_b32 v[vgprValuC+85], acc211         // copy acc to vreg[220]
v_accvgpr_read_b32 v[vgprValuC+90], acc215         // copy acc to vreg[221]
v_accvgpr_read_b32 v[vgprValuC+95], acc219         // copy acc to vreg[222]
v_accvgpr_read_b32 v[vgprValuC+100], acc223        // copy acc to vreg[223]

/* rC *= alpha batchElements=[(13, 0, 0, 0), (13, 0, 0, 1), (13, 0, 0, 2), (13, 0, 0, 3), (13, 0, 1, 0), (13, 0, 1, 1), (13, 0, 1, 2), (13, 0, 1, 3), (13, 0, 2, 0), (13, 0, 2, 1), (13, 0, 2, 2), (13, 0, 2, 3), (13, 0, 3, 0), (13, 0, 3, 1), (13, 0, 3, 2), (13, 0, 3, 3)] */
v_mul_f32 v[vgprValuC+19], s[sgprAlpha], v[vgprValuC+19] // *= alpha
v_mul_f32 v[vgprValuC+26], s[sgprAlpha], v[vgprValuC+26] // *= alpha
v_mul_f32 v[vgprValuC+33], s[sgprAlpha], v[vgprValuC+33] // *= alpha
v_mul_f32 v[vgprValuC+40], s[sgprAlpha], v[vgprValuC+40] // *= alpha
v_mul_f32 v[vgprValuC+45], s[sgprAlpha], v[vgprValuC+45] // *= alpha
v_mul_f32 v[vgprValuC+50], s[sgprAlpha], v[vgprValuC+50] // *= alpha
v_mul_f32 v[vgprValuC+55], s[sgprAlpha], v[vgprValuC+55] // *= alpha
v_mul_f32 v[vgprValuC+60], s[sgprAlpha], v[vgprValuC+60] // *= alpha
v_mul_f32 v[vgprValuC+65], s[sgprAlpha], v[vgprValuC+65] // *= alpha
v_mul_f32 v[vgprValuC+70], s[sgprAlpha], v[vgprValuC+70] // *= alpha
v_mul_f32 v[vgprValuC+75], s[sgprAlpha], v[vgprValuC+75] // *= alpha
v_mul_f32 v[vgprValuC+80], s[sgprAlpha], v[vgprValuC+80] // *= alpha
v_mul_f32 v[vgprValuC+85], s[sgprAlpha], v[vgprValuC+85] // *= alpha
v_mul_f32 v[vgprValuC+90], s[sgprAlpha], v[vgprValuC+90] // *= alpha
v_mul_f32 v[vgprValuC+95], s[sgprAlpha], v[vgprValuC+95] // *= alpha
v_mul_f32 v[vgprValuC+100], s[sgprAlpha], v[vgprValuC+100] // *= alpha
s_waitcnt 0                                        // wait for Beta, ScaleAlphaVec, Bias LDS

/* apply mask, calc new C and issue writes */
v_mov_b32 v10, 0xffff0000                          // mask for pack two bfloat16 element to 32bit
v_mov_b32 v11, 0x7fff0000                          // fp32 Nan
v_mov_b32 v12, 0x7fff                              // rounding bias for bfloat16
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+19], v18, v[vgprValuC+19]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v16                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+19], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+19]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v19, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+19], v[vgprValuC+19] // check Nan
v_bfe_u32 v9, v[vgprValuC+19], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+19], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+19], v9, v11, s[60:61]
v_lshrrev_b32 v19, 16, v[vgprValuC+19]             // convert C to bf16
buffer_store_short v19, v13, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+26], v25, v[vgprValuC+26]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v23                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+26], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+26]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v26, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+26], v[vgprValuC+26] // check Nan
v_bfe_u32 v9, v[vgprValuC+26], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+26], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+26], v9, v11, s[60:61]
v_lshrrev_b32 v26, 16, v[vgprValuC+26]             // convert C to bf16
buffer_store_short v26, v20, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+33], v32, v[vgprValuC+33]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v30                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+33], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+33]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v33, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+33], v[vgprValuC+33] // check Nan
v_bfe_u32 v9, v[vgprValuC+33], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+33], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+33], v9, v11, s[60:61]
v_lshrrev_b32 v33, 16, v[vgprValuC+33]             // convert C to bf16
buffer_store_short v33, v27, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+40], v39, v[vgprValuC+40]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v37                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+40], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+40]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v40, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+40], v[vgprValuC+40] // check Nan
v_bfe_u32 v9, v[vgprValuC+40], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+40], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+40], v9, v11, s[60:61]
v_lshrrev_b32 v40, 16, v[vgprValuC+40]             // convert C to bf16
buffer_store_short v40, v34, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+45], v18, v[vgprValuC+45]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v44                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+45], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+45]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v45, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+45], v[vgprValuC+45] // check Nan
v_bfe_u32 v9, v[vgprValuC+45], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+45], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+45], v9, v11, s[60:61]
v_lshrrev_b32 v45, 16, v[vgprValuC+45]             // convert C to bf16
buffer_store_short v45, v41, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+50], v25, v[vgprValuC+50]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v49                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+50], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+50]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v50, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+50], v[vgprValuC+50] // check Nan
v_bfe_u32 v9, v[vgprValuC+50], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+50], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+50], v9, v11, s[60:61]
v_lshrrev_b32 v50, 16, v[vgprValuC+50]             // convert C to bf16
buffer_store_short v50, v46, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+55], v32, v[vgprValuC+55]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v54                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+55], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+55]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v55, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+55], v[vgprValuC+55] // check Nan
v_bfe_u32 v9, v[vgprValuC+55], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+55], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+55], v9, v11, s[60:61]
v_lshrrev_b32 v55, 16, v[vgprValuC+55]             // convert C to bf16
buffer_store_short v55, v51, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+60], v39, v[vgprValuC+60]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v59                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+60], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+60]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v60, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+60], v[vgprValuC+60] // check Nan
v_bfe_u32 v9, v[vgprValuC+60], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+60], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+60], v9, v11, s[60:61]
v_lshrrev_b32 v60, 16, v[vgprValuC+60]             // convert C to bf16
buffer_store_short v60, v56, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+65], v18, v[vgprValuC+65]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v64                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+65], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+65]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v65, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+65], v[vgprValuC+65] // check Nan
v_bfe_u32 v9, v[vgprValuC+65], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+65], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+65], v9, v11, s[60:61]
v_lshrrev_b32 v65, 16, v[vgprValuC+65]             // convert C to bf16
buffer_store_short v65, v61, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+70], v25, v[vgprValuC+70]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v69                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+70], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+70]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v70, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+70], v[vgprValuC+70] // check Nan
v_bfe_u32 v9, v[vgprValuC+70], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+70], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+70], v9, v11, s[60:61]
v_lshrrev_b32 v70, 16, v[vgprValuC+70]             // convert C to bf16
buffer_store_short v70, v66, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+75], v32, v[vgprValuC+75]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v74                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+75], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+75]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v75, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+75], v[vgprValuC+75] // check Nan
v_bfe_u32 v9, v[vgprValuC+75], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+75], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+75], v9, v11, s[60:61]
v_lshrrev_b32 v75, 16, v[vgprValuC+75]             // convert C to bf16
buffer_store_short v75, v71, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+80], v39, v[vgprValuC+80]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v79                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+80], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+80]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v80, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+80], v[vgprValuC+80] // check Nan
v_bfe_u32 v9, v[vgprValuC+80], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+80], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+80], v9, v11, s[60:61]
v_lshrrev_b32 v80, 16, v[vgprValuC+80]             // convert C to bf16
buffer_store_short v80, v76, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v18, 1.0, v18, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+85], v18, v[vgprValuC+85]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v84                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+85], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v17, v[vgprValuC+85]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v85, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+85], v[vgprValuC+85] // check Nan
v_bfe_u32 v9, v[vgprValuC+85], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+85], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+85], v9, v11, s[60:61]
v_lshrrev_b32 v85, 16, v[vgprValuC+85]             // convert C to bf16
buffer_store_short v85, v81, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v25, 1.0, v25, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+90], v25, v[vgprValuC+90]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v89                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+90], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v24, v[vgprValuC+90]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v90, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+90], v[vgprValuC+90] // check Nan
v_bfe_u32 v9, v[vgprValuC+90], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+90], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+90], v9, v11, s[60:61]
v_lshrrev_b32 v90, 16, v[vgprValuC+90]             // convert C to bf16
buffer_store_short v90, v86, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v32, 1.0, v32, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+95], v32, v[vgprValuC+95]    // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v94                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+95], v4, s[sgprBeta]        // finalSum = sum*alpha + C*beta
v_add_f32 v4, v31, v[vgprValuC+95]                 // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v95, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+95], v[vgprValuC+95] // check Nan
v_bfe_u32 v9, v[vgprValuC+95], 16, 1               // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+95], v9, v12            // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+95], v9, v11, s[60:61]
v_lshrrev_b32 v95, 16, v[vgprValuC+95]             // convert C to bf16
buffer_store_short v95, v91, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
v_cmp_gt_u32 s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1], s[sgprSrdScaleAlphaVec+2], 0 //  == 0 ?
v_cndmask_b32 v39, 1.0, v39, s[sgprAddressScaleAlphaVec:sgprAddressScaleAlphaVec+1] // 1. mul 1 if 0
v_mul_f32 v[vgprValuC+100], v39, v[vgprValuC+100]  // *= scaleAlphaVecVMul
v_lshlrev_b32 v4, 16, v99                          // cvt bf16 to fp32. 
v_fmac_f32 v[vgprValuC+100], v4, s[sgprBeta]       // finalSum = sum*alpha + C*beta
v_add_f32 v4, v38, v[vgprValuC+100]                // C += bias
s_swappc_b64 s[58:59], s[12:13]
v_mov_b32 v100, v4
v_cmp_u_f32 s[60:61], v[vgprValuC+100], v[vgprValuC+100] // check Nan
v_bfe_u32 v9, v[vgprValuC+100], 16, 1              // Non-Nan case: store lsb of bf16
v_add3_u32 v9, v[vgprValuC+100], v9, v12           // Non-Nan case: add lsb and the increment for rounding
v_cndmask_b32 v[vgprValuC+100], v9, v11, s[60:61]
v_lshrrev_b32 v100, 16, v[vgprValuC+100]           // convert C to bf16
buffer_store_short v100, v96, s[sgprSrdD:sgprSrdD+3], 0 offen offset:0 // store D
s_nop 0                                            // 1 wait state required when next inst writes vgprs held by previous dwordx4 store inst
s_branch label_GW_End_2                            // jump to end
label_Activation_None_VW1:
s_setpc_b64 s[58:59]
label_Activation_Abs_VW1:
v_and_b32 v4, 0x7fffffff, v4                       // Remove sign bit
s_setpc_b64 s[58:59]
label_Activation_Clippedrelu_VW1:
v_cmp_gt_f32 vcc, v4, s[sgpractivationAlpha]       // x > alpha ?
v_min_f32 v4, s[sgpractivationBeta], v4            // min(x, beta)
v_cndmask_b32 v4, 0.0, v4, vcc                     // set x to 0 if <= alpha
s_setpc_b64 s[58:59]
label_Activation_Gelu_VW1:
v_mul_f32 v8, 0x3d372713, v4                       // k1 * x
v_fma_f32 v8, v4, v8, 1.0                          // 1 + (k1 * x * x)
v_mul_f32 v8, v4, v8                               // x * (1 + k1 * x * x)
v_mul_f32 v8, 0x40135761, v8                       //  (fused 2.302208)
v_exp_f32 v8, v8                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v8, 1.0, v8                              // e^2x + 1
v_rcp_f32 v8, v8                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v8, -2.0, v8, 2.0                        //  ( + 1 (fused))
v_mul_f32 v8, v4, v8                               // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v8                              // 0.5 * x * (1 + tanh(...))
s_setpc_b64 s[58:59]
label_Activation_Leakyrelu_VW1:
v_mul_f32 v8, s[sgpractivationAlpha], v4           // tmp = x * alpha
v_cmp_ge_f32 vcc, v4, 0.0                          // x >= 0 ?
v_cndmask_b32 v4, v8, v4, vcc                      // set x to tmp if < 0
s_setpc_b64 s[58:59]
label_Activation_Relu_VW1:
v_max_f32 v4, v4, 0                                // x = max(0, x)
s_setpc_b64 s[58:59]
label_Activation_Sigmoid_VW1:
v_mul_f32 v4, 0xbfb8aa3b, v4                       //  (fused -1.442695)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // 1 + exp(-x)
v_rcp_f32 v4, v4                                   // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
s_setpc_b64 s[58:59]
label_Activation_Tanh_VW1:
v_mul_f32 v4, s[sgpractivationAlpha], v4           // x * alpha
v_mul_f32 v4, 0x4038aa3b, v4                       //  (fused 2)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 1.0                        // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v4, s[sgpractivationBeta], v4            // beta * tanh(x)
s_setpc_b64 s[58:59]
label_Activation_Geluscaling_VW1:
v_mul_f32 v8, 0x3d372713, v4                       // k1 * x
v_fma_f32 v8, v4, v8, 1.0                          // 1 + (k1 * x * x)
v_mul_f32 v8, v4, v8                               // x * (1 + k1 * x * x)
v_mul_f32 v8, 0x40135761, v8                       //  (fused 2.302208)
v_exp_f32 v8, v8                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v8, 1.0, v8                              // e^2x + 1
v_rcp_f32 v8, v8                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v8, -2.0, v8, 2.0                        //  ( + 1 (fused))
v_mul_f32 v8, v4, v8                               // x * (1 + tanh(...))
v_mul_f32 v8, 0.5, v8                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, s[sgpractivationAlpha], v8           // 0.5 * x * (1 + tanh(...)) * scale
s_setpc_b64 s[58:59]
label_Activation_Silu_VW1:
v_mul_f32 v8, -1.4426950408889634, v4              //  (fused -1.442695)
v_exp_f32 v8, v8                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v8, 1.0, v8                              // 1 + exp(-x)
v_rcp_f32 v8, v8                                   // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v4, v4, v8                               // x / (1 + exp(-x))
s_setpc_b64 s[58:59]
label_GW_End_2:
label_KernelEnd:
s_endpgm                                           // Kernel End
label_Activation_None_VW4:
s_setpc_b64 s[58:59]
label_Activation_Abs_VW4:
v_and_b32 v4, 0x7fffffff, v4                       // Remove sign bit
v_and_b32 v5, 0x7fffffff, v5                       // Remove sign bit
v_and_b32 v6, 0x7fffffff, v6                       // Remove sign bit
v_and_b32 v7, 0x7fffffff, v7                       // Remove sign bit
s_setpc_b64 s[58:59]
label_Activation_Clippedrelu_VW4:
v_cmp_gt_f32 vcc, v4, s[sgpractivationAlpha]       // x > alpha ?
v_min_f32 v4, s[sgpractivationBeta], v4            // min(x, beta)
v_cndmask_b32 v4, 0.0, v4, vcc                     // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v5, s[sgpractivationAlpha]       // x > alpha ?
v_min_f32 v5, s[sgpractivationBeta], v5            // min(x, beta)
v_cndmask_b32 v5, 0.0, v5, vcc                     // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v6, s[sgpractivationAlpha]       // x > alpha ?
v_min_f32 v6, s[sgpractivationBeta], v6            // min(x, beta)
v_cndmask_b32 v6, 0.0, v6, vcc                     // set x to 0 if <= alpha
v_cmp_gt_f32 vcc, v7, s[sgpractivationAlpha]       // x > alpha ?
v_min_f32 v7, s[sgpractivationBeta], v7            // min(x, beta)
v_cndmask_b32 v7, 0.0, v7, vcc                     // set x to 0 if <= alpha
s_setpc_b64 s[58:59]
label_Activation_Gelu_VW4:
v_mul_f32 v8, 0x3d372713, v4                       // k1 * x
v_fma_f32 v8, v4, v8, 1.0                          // 1 + (k1 * x * x)
v_mul_f32 v8, v4, v8                               // x * (1 + k1 * x * x)
v_mul_f32 v8, 0x40135761, v8                       //  (fused 2.302208)
v_exp_f32 v8, v8                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v8, 1.0, v8                              // e^2x + 1
v_rcp_f32 v8, v8                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v8, -2.0, v8, 2.0                        //  ( + 1 (fused))
v_mul_f32 v8, v4, v8                               // x * (1 + tanh(...))
v_mul_f32 v4, 0.5, v8                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v8, 0x3d372713, v5                       // k1 * x
v_fma_f32 v8, v5, v8, 1.0                          // 1 + (k1 * x * x)
v_mul_f32 v8, v5, v8                               // x * (1 + k1 * x * x)
v_mul_f32 v8, 0x40135761, v8                       //  (fused 2.302208)
v_exp_f32 v8, v8                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v8, 1.0, v8                              // e^2x + 1
v_rcp_f32 v8, v8                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v8, -2.0, v8, 2.0                        //  ( + 1 (fused))
v_mul_f32 v8, v5, v8                               // x * (1 + tanh(...))
v_mul_f32 v5, 0.5, v8                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v8, 0x3d372713, v6                       // k1 * x
v_fma_f32 v8, v6, v8, 1.0                          // 1 + (k1 * x * x)
v_mul_f32 v8, v6, v8                               // x * (1 + k1 * x * x)
v_mul_f32 v8, 0x40135761, v8                       //  (fused 2.302208)
v_exp_f32 v8, v8                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v8, 1.0, v8                              // e^2x + 1
v_rcp_f32 v8, v8                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v8, -2.0, v8, 2.0                        //  ( + 1 (fused))
v_mul_f32 v8, v6, v8                               // x * (1 + tanh(...))
v_mul_f32 v6, 0.5, v8                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v8, 0x3d372713, v7                       // k1 * x
v_fma_f32 v8, v7, v8, 1.0                          // 1 + (k1 * x * x)
v_mul_f32 v8, v7, v8                               // x * (1 + k1 * x * x)
v_mul_f32 v8, 0x40135761, v8                       //  (fused 2.302208)
v_exp_f32 v8, v8                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v8, 1.0, v8                              // e^2x + 1
v_rcp_f32 v8, v8                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v8, -2.0, v8, 2.0                        //  ( + 1 (fused))
v_mul_f32 v8, v7, v8                               // x * (1 + tanh(...))
v_mul_f32 v7, 0.5, v8                              // 0.5 * x * (1 + tanh(...))
s_setpc_b64 s[58:59]
label_Activation_Leakyrelu_VW4:
v_mul_f32 v8, s[sgpractivationAlpha], v4           // tmp = x * alpha
v_cmp_ge_f32 vcc, v4, 0.0                          // x >= 0 ?
v_cndmask_b32 v4, v8, v4, vcc                      // set x to tmp if < 0
v_mul_f32 v8, s[sgpractivationAlpha], v5           // tmp = x * alpha
v_cmp_ge_f32 vcc, v5, 0.0                          // x >= 0 ?
v_cndmask_b32 v5, v8, v5, vcc                      // set x to tmp if < 0
v_mul_f32 v8, s[sgpractivationAlpha], v6           // tmp = x * alpha
v_cmp_ge_f32 vcc, v6, 0.0                          // x >= 0 ?
v_cndmask_b32 v6, v8, v6, vcc                      // set x to tmp if < 0
v_mul_f32 v8, s[sgpractivationAlpha], v7           // tmp = x * alpha
v_cmp_ge_f32 vcc, v7, 0.0                          // x >= 0 ?
v_cndmask_b32 v7, v8, v7, vcc                      // set x to tmp if < 0
s_setpc_b64 s[58:59]
label_Activation_Relu_VW4:
v_max_f32 v4, v4, 0                                // x = max(0, x)
v_max_f32 v5, v5, 0                                // x = max(0, x)
v_max_f32 v6, v6, 0                                // x = max(0, x)
v_max_f32 v7, v7, 0                                // x = max(0, x)
s_setpc_b64 s[58:59]
label_Activation_Sigmoid_VW4:
v_mul_f32 v4, 0xbfb8aa3b, v4                       //  (fused -1.442695)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // 1 + exp(-x)
v_rcp_f32 v4, v4                                   // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v5, 0xbfb8aa3b, v5                       //  (fused -1.442695)
v_exp_f32 v5, v5                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v5, 1.0, v5                              // 1 + exp(-x)
v_rcp_f32 v5, v5                                   // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v6, 0xbfb8aa3b, v6                       //  (fused -1.442695)
v_exp_f32 v6, v6                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v6, 1.0, v6                              // 1 + exp(-x)
v_rcp_f32 v6, v6                                   // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v7, 0xbfb8aa3b, v7                       //  (fused -1.442695)
v_exp_f32 v7, v7                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v7, 1.0, v7                              // 1 + exp(-x)
v_rcp_f32 v7, v7                                   // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
s_setpc_b64 s[58:59]
label_Activation_Tanh_VW4:
v_mul_f32 v4, s[sgpractivationAlpha], v4           // x * alpha
v_mul_f32 v4, 0x4038aa3b, v4                       //  (fused 2)
v_exp_f32 v4, v4                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v4, 1.0, v4                              // e^2x + 1
v_rcp_f32 v4, v4                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v4, -2.0, v4, 1.0                        // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v4, s[sgpractivationBeta], v4            // beta * tanh(x)
v_mul_f32 v5, s[sgpractivationAlpha], v5           // x * alpha
v_mul_f32 v5, 0x4038aa3b, v5                       //  (fused 2)
v_exp_f32 v5, v5                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v5, 1.0, v5                              // e^2x + 1
v_rcp_f32 v5, v5                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v5, -2.0, v5, 1.0                        // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v5, s[sgpractivationBeta], v5            // beta * tanh(x)
v_mul_f32 v6, s[sgpractivationAlpha], v6           // x * alpha
v_mul_f32 v6, 0x4038aa3b, v6                       //  (fused 2)
v_exp_f32 v6, v6                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v6, 1.0, v6                              // e^2x + 1
v_rcp_f32 v6, v6                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v6, -2.0, v6, 1.0                        // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v6, s[sgpractivationBeta], v6            // beta * tanh(x)
v_mul_f32 v7, s[sgpractivationAlpha], v7           // x * alpha
v_mul_f32 v7, 0x4038aa3b, v7                       //  (fused 2)
v_exp_f32 v7, v7                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v7, 1.0, v7                              // e^2x + 1
v_rcp_f32 v7, v7                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v7, -2.0, v7, 1.0                        // (-2) * (1 / (e^2x + 1)) + 1
v_mul_f32 v7, s[sgpractivationBeta], v7            // beta * tanh(x)
s_setpc_b64 s[58:59]
label_Activation_Geluscaling_VW4:
v_mul_f32 v8, 0x3d372713, v4                       // k1 * x
v_fma_f32 v8, v4, v8, 1.0                          // 1 + (k1 * x * x)
v_mul_f32 v8, v4, v8                               // x * (1 + k1 * x * x)
v_mul_f32 v8, 0x40135761, v8                       //  (fused 2.302208)
v_exp_f32 v8, v8                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v8, 1.0, v8                              // e^2x + 1
v_rcp_f32 v8, v8                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v8, -2.0, v8, 2.0                        //  ( + 1 (fused))
v_mul_f32 v8, v4, v8                               // x * (1 + tanh(...))
v_mul_f32 v8, 0.5, v8                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v4, s[sgpractivationAlpha], v8           // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v8, 0x3d372713, v5                       // k1 * x
v_fma_f32 v8, v5, v8, 1.0                          // 1 + (k1 * x * x)
v_mul_f32 v8, v5, v8                               // x * (1 + k1 * x * x)
v_mul_f32 v8, 0x40135761, v8                       //  (fused 2.302208)
v_exp_f32 v8, v8                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v8, 1.0, v8                              // e^2x + 1
v_rcp_f32 v8, v8                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v8, -2.0, v8, 2.0                        //  ( + 1 (fused))
v_mul_f32 v8, v5, v8                               // x * (1 + tanh(...))
v_mul_f32 v8, 0.5, v8                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v5, s[sgpractivationAlpha], v8           // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v8, 0x3d372713, v6                       // k1 * x
v_fma_f32 v8, v6, v8, 1.0                          // 1 + (k1 * x * x)
v_mul_f32 v8, v6, v8                               // x * (1 + k1 * x * x)
v_mul_f32 v8, 0x40135761, v8                       //  (fused 2.302208)
v_exp_f32 v8, v8                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v8, 1.0, v8                              // e^2x + 1
v_rcp_f32 v8, v8                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v8, -2.0, v8, 2.0                        //  ( + 1 (fused))
v_mul_f32 v8, v6, v8                               // x * (1 + tanh(...))
v_mul_f32 v8, 0.5, v8                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v6, s[sgpractivationAlpha], v8           // 0.5 * x * (1 + tanh(...)) * scale
v_mul_f32 v8, 0x3d372713, v7                       // k1 * x
v_fma_f32 v8, v7, v8, 1.0                          // 1 + (k1 * x * x)
v_mul_f32 v8, v7, v8                               // x * (1 + k1 * x * x)
v_mul_f32 v8, 0x40135761, v8                       //  (fused 2.302208)
v_exp_f32 v8, v8                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v8, 1.0, v8                              // e^2x + 1
v_rcp_f32 v8, v8                                   // 1 / (e^2x + 1)
s_nop 0                                            // 1 wait states
v_fma_f32 v8, -2.0, v8, 2.0                        //  ( + 1 (fused))
v_mul_f32 v8, v7, v8                               // x * (1 + tanh(...))
v_mul_f32 v8, 0.5, v8                              // 0.5 * x * (1 + tanh(...))
v_mul_f32 v7, s[sgpractivationAlpha], v8           // 0.5 * x * (1 + tanh(...)) * scale
s_setpc_b64 s[58:59]
label_Activation_Silu_VW4:
v_mul_f32 v8, -1.4426950408889634, v4              //  (fused -1.442695)
v_exp_f32 v8, v8                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v8, 1.0, v8                              // 1 + exp(-x)
v_rcp_f32 v8, v8                                   // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v4, v4, v8                               // x / (1 + exp(-x))
v_mul_f32 v8, -1.4426950408889634, v5              //  (fused -1.442695)
v_exp_f32 v8, v8                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v8, 1.0, v8                              // 1 + exp(-x)
v_rcp_f32 v8, v8                                   // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v5, v5, v8                               // x / (1 + exp(-x))
v_mul_f32 v8, -1.4426950408889634, v6              //  (fused -1.442695)
v_exp_f32 v8, v8                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v8, 1.0, v8                              // 1 + exp(-x)
v_rcp_f32 v8, v8                                   // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v6, v6, v8                               // x / (1 + exp(-x))
v_mul_f32 v8, -1.4426950408889634, v7              //  (fused -1.442695)
v_exp_f32 v8, v8                                   // exp step 2
s_nop 0                                            // 1 wait states
v_add_f32 v8, 1.0, v8                              // 1 + exp(-x)
v_rcp_f32 v8, v8                                   // 1 / (1 + exp(-x))
s_nop 0                                            // 1 wait states
v_mul_f32 v7, v7, v8                               // x / (1 + exp(-x))
s_setpc_b64 s[58:59]
s_endpgm
label_ASM_End:  /// The end of the kernel
